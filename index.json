[{"uri":"https://veljg.github.io/AWS-Worklog/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"AWS Partner Network (APN) Blog ƒê·∫°t ƒê∆∞·ª£c S·ª± Xu·∫•t S·∫Øc trong D·ªãch V·ª• H·∫≠u M√£i v·ªõi Syncron v√† AWS b·ªüi Ankit Gupta, Angelo Malatacca, Taj Abdulahi, v√† Marc Cervera Castro v√†o ng√†y 10 th√°ng 7 nƒÉm 2025 trong Amazon Athena, Amazon EMR, Analytics, AWS Glue, Industries, Manufacturing, Partner solutions | Permalink | B√¨nh lu·∫≠n | Chia s·∫ª\nB·ªüi: Taj Abdulahi, Sr. Manager Product ‚Äì Syncron\nB·ªüi: Ankit Gupta, Sr. Solutions Architect ‚Äì AWS\nB·ªüi: Marc Cervera Castro, Sr. Account Manager ‚Äì AWS\nB·ªüi: Angelo Malatacca, Partner Solutions Architect ‚Äì AWS\nC√°c nh√† s·∫£n xu·∫•t thi·∫øt b·ªã khi qu·∫£n l√Ω d·ªãch v·ª• h·∫≠u m√£i c·ªßa h·ªç m·ªôt c√°ch hi·ªáu qu·∫£ c√≥ th·ªÉ ki·∫øm ƒë∆∞·ª£c nhi·ªÅu doanh thu h∆°n t·ª´ vi·ªác b√°n ph·ª• t√πng, tƒÉng l·ª£i nhu·∫≠n, c·∫£i thi·ªán k·∫øt qu·∫£ d·ªãch v·ª• v√† c·ªßng c·ªë l√≤ng trung th√†nh t·ª´ c·∫£ kh√°ch h√†ng v√† c√°c ƒë·∫°i l√Ω. C√°c nh√† s·∫£n xu·∫•t ch∆∞a ph√°t tri·ªÉn d·ªãch v·ª• h·∫≠u m√£i c·ªßa m√¨nh c√≥ nguy c∆° g·∫∑p ph·∫£i c√°c k·∫øt qu·∫£ kh√°ch h√†ng k√©m, c·∫°nh tranh v·ªõi c√°c ƒë·∫°i l√Ω v·ªÅ th·ªã ph·∫ßn d·ªãch v·ª• v√† tƒÉng nguy c∆° b·ªã c√°c nh√† cung c·∫•p ch·ª£ ƒëen thay th·∫ø.\nC√°c nh√† s·∫£n xu·∫•t ph·∫£i ƒë·ªëi m·∫∑t v·ªõi c√°c r√†o c·∫£n v·∫≠n h√†nh trong vi·ªác ƒëi·ªÅu ph·ªëi d·ªãch v·ª• h·∫≠u m√£i c·ªßa h·ªç. Sau khi b√°n h√†ng, h·ªç ph·∫£i huy ƒë·ªông nhi·ªÅu ƒë·ªôi ng≈© ƒë·ªÉ h·ªó tr·ª£ ch·ª©c nƒÉng c·ªßa thi·∫øt b·ªã trong su·ªët v√≤ng ƒë·ªùi c·ªßa n√≥, v√† ƒë·∫£m b·∫£o c√≥ s·∫µn ph·ª• t√πng thay th·∫ø ƒë·ªÉ ho√†n th√†nh b·∫•t k·ª≥ c√¥ng vi·ªác s·ª≠a ch·ªØa n√†o. S·ª± ph·ª©c t·∫°p c·ªßa c√°c ho·∫°t ƒë·ªông n√†y b·ªôc l·ªô nh·ªØng th√°ch th·ª©c trong m√¥i tr∆∞·ªùng s·∫£n xu·∫•t ng√†y nay. Ho·∫°t ƒë·ªông c√¥ l·∫≠p xu·∫•t hi·ªán khi c√°c ph√≤ng ban kh√°c nhau, ƒë·ªôi ng≈© ph·ª• t√πng, d·ªãch v·ª• v√† b·∫£o h√†nh, l√†m vi·ªác ri√™ng l·∫ª. S·ª± t√°ch bi·ªát n√†y t·∫°o ra r√†o c·∫£n cho s·ª± c·ªông t√°c hi·ªáu qu·∫£ v√† d·∫´n ƒë·∫øn d·ªØ li·ªáu b·ªã ph√¢n m·∫£nh tr√™n nhi·ªÅu h·ªá th·ªëng, g√¢y kh√≥ khƒÉn trong vi·ªác duy tr√¨ c√°i nh√¨n to√†n di·ªán v·ªÅ ho·∫°t ƒë·ªông d·ªãch v·ª•. C√°c ho·∫°t ƒë·ªông c√¥ l·∫≠p n√†y tr·ª±c ti·∫øp g√≥p ph·∫ßn l√†m tƒÉng chi ph√≠ v·∫≠n h√†nh. Khi c√°c ƒë·ªôi ng≈© ho·∫°t ƒë·ªông ƒë·ªôc l·∫≠p, c√°c quy tr√¨nh tr·ªü n√™n tr√πng l·∫∑p v√† vi·ªác ph·ªëi h·ª£p tr·ªü n√™n t·ªën th·ªùi gian. S·ª± k√©m hi·ªáu qu·∫£ n√†y d·∫´n ƒë·∫øn vi·ªác ph√¢n b·ªï ngu·ªìn l·ª±c kh√¥ng t·ªëi ∆∞u v√† h·∫°n ch·∫ø kh·∫£ nƒÉng hi·ªÉn th·ªã tr√™n chu·ªói d·ªãch v·ª•, cu·ªëi c√πng l√†m tƒÉng chi ph√≠ v·∫≠n h√†nh v√† ·∫£nh h∆∞·ªüng ƒë·∫øn l·ª£i nhu·∫≠n c·ªßa nh√† s·∫£n xu·∫•t.\nƒê·ªÉ gi·∫£i quy·∫øt nh·ªØng th√°ch th·ª©c n√†y, Syncron cung c·∫•p m·ªôt n·ªÅn t·∫£ng Service Lifecycle Management (SLM) gi√∫p chuy·ªÉn ƒë·ªïi c√°ch c√°c nh√† s·∫£n xu·∫•t qu·∫£n l√Ω c√°c ho·∫°t ƒë·ªông h·∫≠u m√£i v√† t·ªï ch·ª©c d·ªãch v·ª• c·ªßa h·ªç.\nTrong b√†i blog n√†y, b·∫°n s·∫Ω t√¨m hi·ªÉu c√°ch Service Lifecycle Management (SLM) c·ªßa Syncron c√≥ th·ªÉ hi·ªán ƒë·∫°i h√≥a c√°c d·ªãch v·ª• h·∫≠u m√£i c·ªßa b·∫°n tr√™n AWS.\nN·ªÅn t·∫£ng SLM c·ªßa Syncron c·ªßng c·ªë c√°c ho·∫°t ƒë·ªông h·∫≠u m√£i Syncron ƒë∆∞·ª£c th√†nh l·∫≠p 35 nƒÉm tr∆∞·ªõc v·ªõi m·ª•c ti√™u gi√∫p vi·ªác l∆∞u kho v√† b√°n ph·ª• t√πng tr·ªü n√™n d·ªÖ d√†ng h∆°n. Hi·ªán t·∫°i, h·ªç c√≥ h∆°n 200+ kh√°ch h√†ng doanh nghi·ªáp h√†ng ƒë·∫ßu tr√™n to√†n c·∫ßu. C√°c gi·∫£i ph√°p h·∫≠u m√£i c·ªßa Syncron bao g·ªìm Gi√° ph·ª• t√πng, Gi√° h·ª£p ƒë·ªìng, L·∫≠p k·∫ø ho·∫°ch kho h√†ng v√† B·∫£o h√†nh.\nG·∫ßn ƒë√¢y, h·ªç ƒë√£ th√™m SLM Data Platform v√†o d·ªãch v·ª• n√†y, ƒë∆∞·ª£c l∆∞u tr·ªØ tr√™n AWS. C√¥ng c·ª• t·∫≠p trung m·ªõi n√†y t√≠ch h·ª£p d·ªØ li·ªáu tr√™n c√°c ho·∫°t ƒë·ªông d·ªãch v·ª•, ph·ª• t√πng v√† b·∫£o h√†nh (t·ª´ c·∫£ c√°c gi·∫£i ph√°p c·ªßa Syncron v√† c√°c ngu·ªìn d·ªØ li·ªáu b√™n ngo√†i), t·∫°o ra m·ªôt h·ªá sinh th√°i kinh doanh ƒë∆∞·ª£c k·∫øt n·ªëi ho√†n to√†n.\nC√°ch ti·∫øp c·∫≠n to√†n di·ªán n√†y ƒëi·ªÅu ch·ªânh vi·ªác ra quy·∫øt ƒë·ªãnh gi·ªØa c√°c ph√≤ng ban, thay th·∫ø c√°c h·ªá th·ªëng b·ªã ph√¢n m·∫£nh b·∫±ng m·ªôt gi·∫£i ph√°p th·ªëng nh·∫•t, d·ª±a tr√™n d·ªØ li·ªáu v√† tƒÉng c∆∞·ªùng s·ª± linh ho·∫°t, cho ph√©p c√°c doanh nghi·ªáp nhanh ch√≥ng ph·∫£n ·ª©ng v·ªõi s·ª± thay ƒë·ªïi c·ªßa th·ªã tr∆∞·ªùng. H√¨nh 1 minh h·ªça c√°ch SLM Data Platform t√≠ch h·ª£p d·ªØ li·ªáu tr√™n gi·∫£i ph√°p c·ªßa Syncron v√† c√°c ngu·ªìn d·ªØ li·ªáu.\nH√¨nh 1 ‚Äì Service Lifecycle Management (SLM) c·ªßa Syncron\nC√°ch ti·∫øp c·∫≠n m·ªõi n√†y trao quy·ªÅn cho c√°c ƒë·ªôi ng≈© t·∫°i hi·ªán tr∆∞·ªùng ƒë·ªÉ truy c·∫≠p d·ªØ li·ªáu ngay l·∫≠p t·ª©c nh·∫±m ƒë∆∞a ra c√°c quy·∫øt ƒë·ªãnh t·ªët h∆°n t·∫°i ch·ªó v·ªõi th·ªùi gian ng·ª´ng ho·∫°t ƒë·ªông v√† s·ª± ph·ªëi h·ª£p th·∫•p h∆°n. ·ªû c·∫•p ƒë·ªô th·ª© hai, n√≥ h·ªó tr·ª£ c√°c nh√† khoa h·ªçc d·ªØ li·ªáu kh√°m ph√° d·ªØ li·ªáu, thu th·∫≠p th√¥ng tin chi ti·∫øt v√† x√¢y d·ª±ng c√°c s·∫£n ph·∫©m d·ªØ li·ªáu nh∆∞ b·∫£ng ƒëi·ªÅu khi·ªÉn ph√¢n t√≠ch v√† m√¥ h√¨nh AI. ƒêi·ªÅu n√†y m·ªü ra c√°c tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng tr∆∞·ªõc ƒë√¢y kh√¥ng th·ªÉ th·ª±c hi·ªán ƒë∆∞·ª£c. Cu·ªëi c√πng, c√°c nh√† qu·∫£n l√Ω v√† gi√°m ƒë·ªëc ƒëi·ªÅu h√†nh c√≥ quy·ªÅn truy c·∫≠p v√†o c√°c c√¥ng c·ª• ph√¢n t√≠ch v√† b·∫£ng ƒëi·ªÅu khi·ªÉn, ƒë·∫£m b·∫£o c√°c quy·∫øt ƒë·ªãnh ƒë∆∞·ª£c cƒÉn ch·ªânh v·ªõi c√°c m·ª•c ti√™u kinh doanh d√†i h·∫°n.\nC√°c Tr∆∞·ªùng h·ª£p S·ª≠ d·ª•ng c·ªßa Kh√°ch h√†ng Truy c·∫≠p d·ªØ li·ªáu h·∫≠u m√£i t·ª©c th√¨\nV·ªõi SLM Data Platform, kh√°ch h√†ng c√≥ th·ªÉ nh·∫≠n ƒë∆∞·ª£c g·∫•p 10 l·∫ßn d·ªØ li·ªáu t·ª´ c√°c ·ª©ng d·ª•ng Service Lifecycle c·ªßa h·ªç. C√°c nh√† s·∫£n xu·∫•t truy c·∫≠p ngay l·∫≠p t·ª©c v√†o d·ªØ li·ªáu s·∫°ch, ƒë√£ ƒë∆∞·ª£c tinh ch·ªânh v√† s·∫µn s√†ng h√†nh ƒë·ªông t·ª´ t·∫•t c·∫£ c√°c ho·∫°t ƒë·ªông h·∫≠u m√£i c·ªßa h·ªç trong m·ªôt trong m·ªôt giao di·ªán qu·∫£n l√Ω th·ªëng nh·∫•t. ƒêi·ªÅu n√†y cho ph√©p truy c·∫≠p d·ªØ li·ªáu c√≥ th·ªÉ m·ªü r·ªông v√† an to√†n, tu√¢n theo c√°c h∆∞·ªõng d·∫´n qu·∫£n tr·ªã ƒë∆∞·ª£c thi·∫øt l·∫≠p s·∫µn, ƒë·ªÉ nhi·ªÅu ƒë·ªôi ng≈© c√≥ th·ªÉ x√¢y d·ª±ng c√°c th√¥ng tin chi ti·∫øt v√† m√¥ h√¨nh h·ªçc m√°y (ML) c·ªßa ri√™ng h·ªç. Vi·ªác b·ªè qua c√°c b∆∞·ªõc tr√≠ch xu·∫•t v√† ƒëi·ªÅu ch·ªânh d·ªØ li·ªáu gi√∫p tƒÉng t·ªëc qu√° tr√¨nh v·∫≠n h√†nh d·ªØ li·ªáu, mang l·∫°i th√¥ng tin chi ti·∫øt kinh doanh nhanh h∆°n.\nTrung t√¢m Ph√¢n t√≠ch cho nh√† ph√¢n t√≠ch d·ªØ li·ªáu\nNh·ªù v√†o vi·ªác s·ª≠ d·ª•ng SLM Data Platform, kh√°ch h√†ng c√≥ th·ªÉ ƒë·ªãnh v·ªã, k·∫øt h·ª£p v√† truy v·∫•n c√°c b·ªô d·ªØ li·ªáu ƒë·ªÉ x√¢y d·ª±ng c√°c data stories c√≥ th·ªÉ chia s·∫ª. H·ªç c√≥ th·ªÉ cung c·∫•p nh·ªØng d·ªØ li·ªáu n√†y l√†m ngu·ªìn cho c√°c c√¥ng c·ª• tr·ª±c quan h√≥a d·ªØ li·ªáu hi·ªán c√≥, ƒë·ªÉ th√¥ng tin ƒë∆∞·ª£c ph√¢n ph·ªëi ƒë·∫øn c√°c ƒë·ªôi ng≈© kh√°c nhau.\nT·∫°o ra c√°c s·∫£n ph·∫©m d·ªØ li·ªáu\nCu·ªëi c√πng, c√°c nh√† s·∫£n xu·∫•t c√≥ th·ªÉ x√¢y d·ª±ng c√°c s·∫£n ph·∫©m d·ªØ li·ªáu c·ªßa ri√™ng h·ªç (v√≠ d·ª•: m√¥ h√¨nh ML t√πy ch·ªânh v·ªÅ ƒë·ªãnh gi√° ph·ª• t√πng ho·∫∑c t·ªëi ∆∞u h√≥a kho h√†ng) t·ª´ t·∫•t c·∫£ d·ªØ li·ªáu v√† cung c·∫•p ch√∫ng t·ª´ SLM Data Platform c·ªßa Syncron. T·∫≠n d·ª•ng kinh nghi·ªám c·ªßa Syncron, h·ªç ƒë√£ x√°c ƒë·ªãnh ƒë∆∞·ª£c h∆°n 30 tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng ƒë∆∞·ª£c x√°c ƒë·ªãnh tr∆∞·ªõc (m·ªôt s·ªë ƒë∆∞·ª£c kh√°ch h√†ng cu·ªëi ƒë·ªãnh gi√° h∆°n 1 tri·ªáu ƒë√¥ la M·ªπ) c√≥ th·ªÉ ƒë∆∞·ª£c k√≠ch ho·∫°t v√† t√≠ch h·ª£p v·ªõi quy tr√¨nh s·∫£n xu·∫•t hi·ªán c√≥.\nSolution Architecture C·ªët l√µi c·ªßa SLM Data Platform c·ªßa Syncron l√† m·ªôt h·ªá sinh th√°i d·ªØ li·ªáu m·∫°nh m·∫Ω, h·ª£p nh·∫•t nhi·ªÅu ngu·ªìn‚Äîd·ªØ li·ªáu ƒë·ªãnh gi√°, h·ª£p ƒë·ªìng, l·∫≠p k·∫ø ho·∫°ch ph·ª• t√πng v√† b·∫£o h√†nh‚Äîth√†nh m·ªôt khung th√¥ng minh, duy nh·∫•t. S·ª± h·ª£p nh·∫•t n√†y cho ph√©p c√°c doanh nghi·ªáp bi·∫øn d·ªØ li·ªáu th√¥ th√†nh th√¥ng tin chi ti·∫øt c√≥ √Ω nghƒ©a, th√∫c ƒë·∫©y hi·ªáu su·∫•t v√† l·ª£i nhu·∫≠n. N·ªÅn t·∫£ng ƒë∆∞·ª£c x√¢y d·ª±ng tr√™n AWS bao g·ªìm c√°c th√†nh ph·∫ßn ch√≠nh sau:\nData Landing Zone\nM·ªôt n·ªÅn t·∫£ng an to√†n, c√≥ kh·∫£ nƒÉng m·ªü r·ªông ƒë·ªÉ ti·∫øp nh·∫≠n d·ªØ li·ªáu ƒëa ƒë·ªëi t∆∞·ª£ng thu√™ t·ª´ nhi·ªÅu ngu·ªìn, ƒë∆∞·ª£c x√¢y d·ª±ng tr√™n Amazon Simple Storage Service (Amazon S3). Data Landing Zone ch·ª©a c·∫£ d·ªØ li·ªáu c√≥ c·∫•u tr√∫c v√† phi c·∫•u tr√∫c. Data Product Framework\nM·ªôt c√°ch ti·∫øp c·∫≠n c√≥ c·∫•u tr√∫c ƒë·ªÉ tinh ch·ªânh v√† tri·ªÉn khai c√°c b·ªô d·ªØ li·ªáu ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh theo m√¥ h√¨nh v·∫≠n h√†nh OEM c·ªßa Syncron, t·∫≠n d·ª•ng AWS Glue. Multi-Tenant Data.all Setup\nƒê·∫£m b·∫£o c√¥ l·∫≠p v√† qu·∫£n tr·ªã d·ªØ li·ªáu kh√°ch h√†ng trong khi duy tr√¨ hi·ªáu su·∫•t v·∫≠n h√†nh b·∫±ng c√°ch √°p d·ª•ng data.all, khung ph√°t tri·ªÉn m√£ ngu·ªìn m·ªü c·ªßa AWS gi√∫p x√¢y d·ª±ng m·ªôt th·ªã tr∆∞·ªùng d·ªØ li·ªáu tr√™n AWS. Unified Data Access\nCung c·∫•p quy·ªÅn truy c·∫≠p d·ªØ li·ªáu theo th·ªùi gian th·ª±c, cho ph√©p kh√°ch h√†ng ƒë∆∞a ra c√°c quy·∫øt ƒë·ªãnh c√≥ th√¥ng tin ƒë·∫ßy ƒë·ªß. N·ªÅn t·∫£ng n√†y c√≥ c√°c t√≠nh nƒÉng: H√¨nh ·∫£nh h√≥a d·ªØ li·ªáu tr·ª±c quan gi√∫p ƒë∆°n gi·∫£n h√≥a d·ªØ li·ªáu ph·ª©c t·∫°p, xu·∫•t d·ªØ li·ªáu li·ªÅn m·∫°ch ƒë·ªÉ t√≠ch h·ª£p v·ªõi c√°c h·ªá th·ªëng b√™n ngo√†i v√† kh·∫£ nƒÉng s·ª≠ d·ª•ng AI c√πng ph√¢n t√≠ch n√¢ng cao cho th√¥ng tin chi ti·∫øt d·ª± ƒëo√°n. H√¨nh 2 l√†m n·ªïi b·∫≠t ki·∫øn tr√∫c c·∫•p cao c·ªßa SLM Data Platform c·ªßa Syncron.\nH√¨nh 2 ‚Äì Ki·∫øn tr√∫c SLM Data Platform c·ªßa Syncron\nK·∫øt lu·∫≠n C√°c gi·∫£i ph√°p c·ªßa Syncron tr√™n AWS cung c·∫•p m·ªôt n·ªÅn t·∫£ng m·∫°nh m·∫Ω ƒë·ªÉ khai th√°c d·ªØ li·ªáu, th√∫c ƒë·∫©y vi·ªác ra quy·∫øt ƒë·ªãnh th√¥ng minh h∆°n v√† ph·ªëi h·ª£p gi·ªØa c√°c ph√≤ng ban. D√π l√† t·ªëi ∆∞u h√≥a kho h√†ng, ƒë·ªãnh gi√°, hay th·ª±c hi·ªán d·ªãch v·ª•, SLM Data Platform ƒë·ªÅu cung c·∫•p m·ªôt n·ªÅn t·∫£ng d·ªØ li·ªáu m·∫°nh m·∫Ω cho c√°c tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng ph√¢n t√≠ch v√† AI.\nƒê·ªÉ t√¨m hi·ªÉu th√™m v·ªÅ c√°ch n·ªÅn t·∫£ng SLM c·ªßa Syncron c√≥ th·ªÉ ph√π h·ª£p v·ªõi quy tr√¨nh h·∫≠u m√£i c·ªßa b·∫°n, h√£y n√≥i chuy·ªán v·ªõi ƒë·ªôi ng≈© qu·∫£n l√Ω t√†i kho·∫£n c·ªßa ch√∫ng t√¥i.\nSyncron ‚Äì AWS Partner Spotlight Syncron l√† ƒê·ªëi t√°c C√¥ng ngh·ªá N√¢ng cao c·ªßa AWS, gi√∫p c√°c nh√† s·∫£n xu·∫•t h√†ng ƒë·∫ßu th·∫ø gi·ªõi t·ªëi ƒëa h√≥a th·ªùi gian ho·∫°t ƒë·ªông c·ªßa s·∫£n ph·∫©m v√† mang l·∫°i tr·∫£i nghi·ªám d·ªãch v·ª• h·∫≠u m√£i v∆∞·ª£t tr·ªôi.\nLi√™n h·ªá v·ªõi Syncron | T·ªïng quan v·ªÅ ƒê·ªëi t√°c | AWS Marketplace\nTAGS: AI for Data Analytics, Industries, Manufacturing, Syncron\n"},{"uri":"https://veljg.github.io/AWS-Worklog/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Huynh An Khuong\nPhone Number: 0964440342\nEmail: huynhankhuong0511@gmail.com\nUniversity: FPTU Ho Chi Minh Campus\nMajor: Information Technology\nClass AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJA Cloud Intern\nInternship Duration: From 08/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect with FCJ members and mentors. Find out what working in an office is like. Install Linux, learn how to properly use Linux. Learn the basics of AWS, console and CLI. Complete first and second module. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Read internship rules - Create AWS account - Learnt what AWS is - Module 1 Lab 1 Done (Learnt how to create AWS account and manage user groups) - Module 1 Lab 7 Done (Learnt how to create budgets of using the service) - Lab 7-3 (Usage Budget) could not be completed due to an error in usage type dropdown, showing nothing - Module 1 Lab 9 Done (Learnt about AWS Support Services, its type, benefits and how to request supports) 08/09/2025 08/09/2025 Create new AWS Account MFA for AWS Accounts Create Admin Group and Admin User Account Authentication Support Explore and Configure AWS Management Console Creating Support Cases and Case Management in AWS Tue - Get started on Module 2 theory: + Learnt about VPC (Amazon Virtual Private Cloud)\n+ Learnt about Subnets and Routetable, Security Groups\n+ Learnt about ENI and EIP\n+ Learnt about VPC Peering and Transit Gateway + Learnt about Elastic Load Balancing\n+ Learnt about EC2\n- Setup site for workshop report - Installed Hugo - Successfully wrote worklog using markdown and Hugo 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ Wed - Complete Module 2\u0026rsquo;s labs - Lab 3: + Learnt about the resources necessary to create and run EC2 instances + Successfully configured and ran EC2 instances + Successfully connect and ping to EC2 instances + Created NAT Gateway to allow private EC2 connections - Lab 10: + Learnt how to create and use key pairs for security + Learnt how to configure security groups to manage connections + Successfully connect and use RDP via EC2 + Set up hybrid DNS with Route 53 Resolver (In progress, Cloud Formation template didn\u0026rsquo;t create security group to proceed with the lab) - Lab 19: + Successfully created VPC Peering Connection + Learnt how to configure Network ACLs + Enabled Cross-Peer DNS to resolve private host names - Downloaded and used MobaXTerm to connect to EC2 instances - Downloaded and used PuTTY to configure key pairs 10/09/2025 11/09/2025 Lab 3 Lab 10 Lab 19 Thu - Lab 20: + Successfully created AWS Transit Gateway to allow for connection between VPCs via a common hub ‚Ä¢ The Cloud Formation template yaml file isn\u0026rsquo;t up to date, creation failed ‚Ä¢ Fixed template file, changed EC2 instance type to t3.micro - Learnt the hard way why you need to clean up resources after a lab, got charged 12$ credits - Verified the cost and budget plans worked as intended, notified over email. 11/09/2025 11/09/2025 Lab 20 Fri - Get started on Module 3 theory + Learnt about EBS, Instance store feature and check User and Meta Data + Learnt about Amazon Lightsail + Learnt about Elastic File System(EFS) and FSx + Learnt about MGN + Learnt how to use S3 Buckets on AWS - Complete Module 3\u0026rsquo;s labs - Lab 13: Successfully created Backup Plan and Vaults for data in S3 Buckets + Successfully set up notification for Backup events + Successfully restored backup - Lab 24: + Created storage gateway + Successfully completed file sharing - Lab 57: + Successfully hosted static website using S3 Buckets + Successfully configured access modifiers + Accelerate Static Websites with Cloudfront configuration did not work, skipping this step + Successfully created bucket versions + Moved objects between buckets + Replicated bucket across regions. 12/09/2025 12/09/2025 Lab 13 Lab 24 Lab 57 Week 1 Achievements: Created and secured an AWS account, including setting up budgets and exploring support services.\nCompleted theory and practical labs for VPC, Subnets, Security Groups, and Routetables.\nSuccessfully deployed and connected to EC2 instances, configured a NAT Gateway, and managed key connections using VPC Peering and AWS Transit Gateway.\nGained hands-on experience with S3 Buckets (static website hosting, versioning, replication), AWS Backup, and Storage Gateway.\nDocumentation Setup: Successfully installed Hugo and configured the site for writing the worklog using markdown.\nTool Proficiency: Learnt to use MobaXTerm and PuTTY for connecting to and managing EC2 instances.\nSuccessfully fixed an outdated CloudFormation template during the Transit Gateway lab and learnt cost management through budget alerts.\nCompleted Module 1 and Module 2, and made a strong start on Module 3.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Completed Modules 1 \u0026amp; 2, securing the AWS account, and performing hands-on labs for VPC and EC2 networking\nWeek 2: Implemented database services (RDS) and Load Balancing, making significant progress on Module 3 \u0026amp; 4, and researching the AWS Well-Architected Framework\nWeek 3: Completed Module 5 (theory and labs), mastering advanced security practices (IAM, KMS, Security Hub) and complex storage/migration labs\nWeek 4: Reviewed core database concepts, attempted database migration (DMS/SCT), and began formulating the team workshop proposa\nWeek 5: Developed the workshop architecture diagram and gained hands-on experience with Kinesis, Glue, and Athena for data streaming and analytics\nWeek 6: Finalized the workshop proposal, integrated GuardDuty and EventBridge into the architecture, and deployed documentation to GitHub Pages\nWeek 7: Extensively tested GuardDuty and configured EventBridge automation to send alerts via SNS and Lambda, integrating AWS Detective for forensics\nWeek 8: Completed intensive review and participated in the FCJ Midterm Exam, while researching AWS Step Functions for IR workflow orchestration\nWeek 9: Finalized the Incident Response architecture using Step Functions and successfully built a custom ETL pipeline for CloudTrail logs\nWeek 10: Completed all custom ETL pipelines (CloudWatch, GuardDuty), integrated security logs with KMS, and set up various threat notification systems\nWeek 11: Refined the architecture by removing the Glue Crawler, added SQS for reliability, and began comprehensive research on AWS CDK\nWeek 12: Successfully implemented the core ETL pipeline and logging infrastructure using AWS CDK, and optimized CloudTrail logging with gzip compression\nWeek 13: Finalized the architecture by integrating Kinesis Data Firehose and overhauling the IR workflow with tagging and EBS snapshotting for project submission\n"},{"uri":"https://veljg.github.io/AWS-Worklog/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: ‚ÄúVietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders: Gen AI and Data track‚Äù Event Objectives Vietnam\u0026rsquo;s premier technology event, bringing together businesses, builders, and leaders to harness cloud and AI innovation. Explore the latest in generative AI, cloud technologies, and digital solutions. Gain valuable insights through keynote sessions, learn from customer success stories, participate in hands-on workshops, and discover cutting-edge solutions from Amazon Web Services (AWS) experts and partners. Speakers H.E Pham Duc Long ‚Äì Vice Minister of Science and Technology H.E Marc E. Knapper ‚Äì US Ambassador to Vietnam Jaime Valles ‚Äì Vice President, General Manager Asia Pacific and Japan, AWS Jeff Johnson ‚Äì Managing Director ASEAN, AWS Dr Jens Lottner ‚Äì CEO, Techcombank Dieter Botha ‚Äì CEO, TymeX Trang Phung ‚Äì CEO \u0026amp; Co-Founder, U2U Network Vu Van ‚Äì CEO \u0026amp; Co-Founder, ELSA Corp Nguyen Hoa Binh ‚Äì Chairman, Texttech Group Taiki Dang ‚Äì Solutions Architect, AWS Jun Kai Loke ‚Äì AI/ML Specialist SA, AWS Kien Nguyen ‚Äì Solutions Architect, AWS Tamelly Lim ‚Äì Storage Specialist SA, AWS Binh Tran ‚Äì Senior Solutions Architect, AWS Michael Armentano - Principal WW GTM Specialist, AWS Key Highlights AWS Customer Keynotes Building a Unified Data Foundation on AWS for AI and Analytics Workloads Constructing a unified, scalable data foundation on AWS, specifically tailored to support AI and analytics workloads Cover key components such as: Data ingestion Storage Processing Governance Ensuring that organizations can effectively manage and utilize their data for advanced analytics and AI initiatives Building the Future: Gen AI Adoption and Roadmap on AWS Showcase comprehensive vision, emerging trends, and strategic roadmap for the adoption of Generative AI (GenAI) technologies Cover key AWS services and initiatives designed to empower organizations in leveraging GenAI to drive innovation and efficiency. AI-Driven Development Lifecycle (AI-DLC) AI-centric approach paving the future of software implementation by fully embedding AI as a central collaborator in the entire software development lifecycle. Securing Generative AI Applications with AWS: Fundamentals and Best Practices Explore security challenges at each layer of the generative AI stack‚Äîinfrastructure, models, and applications. Explore security measures such as encryption, zero-trust architecture, continuous monitoring, and fine-grained access controls to safeguard generative AI workloads Beyond Automation: AI Agents as Your Ultimate Productivity Multipliers Paradigm shift where AI agents aren\u0026rsquo;t just tools, but intelligent partners actively driving businesses forward. Showcases upcoming AWS AI Agent: AWS Quick Suite Key Takeaways Bridging Theory with Industry Application The Critical Role of Data Pipelines in AI/ML: The sessions confirmed that sophisticated AI/ML models are entirely dependent on a robust data foundation. This involves more than just storing data; it requires well-architected pipelines for ingestion, processing, governance, and storage, which is the true engineering challenge before any model training can begin. The Shift Towards AI-Augmented Development and Asynchronous Systems: The future of software development was presented as a combination of two powerful trends. First, the AI-Driven Development Lifecycle (AI-DLC), with tools like Amazon Q Developer, is set to become a standard, augmenting developer productivity. Second, there was a strong preference for event-driven, asynchronous communication over traditional synchronous APIs to build more resilient and scalable systems. Integrating Security Throughout the Development Lifecycle: Security is not a final step but an integral part of the entire development process. The discussions covered securing the full stack‚Äîfrom the cloud infrastructure and the AI models to the application layer itself. Event Experience The \u0026ldquo;Vietnam Cloud Day 2025\u0026rdquo; was incredibly valuable as it provided a clear, practical context for many of the theoretical concepts I\u0026rsquo;m learning in my Computer Technology program. It bridged the gap between academic knowledge and real-world industry application.\nThe showcase of the AI-Driven Development Lifecycle was a highlight. It suggested a paradigm shift where the developer\u0026rsquo;s role evolves to focus more on architecture and complex problem-solving, while AI assistants like Amazon Q Developer handle more of the routine code generation and debugging. Learning about upcoming technologies like the AWS Quick Suite for AI agents provided a compelling look at the next wave of automation.\nI left the event with a much clearer understanding of current industry trends and a better sense of which skills to focus on to prepare for future internships and my career.\nSome event photos Selfie Group Picture\n"},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.1-cloudtrail-etl/","title":"CloudTrail ETL Code","tags":[],"description":"","content":" import json import boto3 import gzip import re import os from datetime import datetime, timezone s3 = boto3.client(\u0026#34;s3\u0026#34;) firehose= boto3.client(\u0026#34;firehose\u0026#34;) # -------------------------------------------------- # CONFIG # -------------------------------------------------- SOURCE_PREFIX = \u0026#34;exportedlogs/vpc-dns-logs/\u0026#34; FIREHOSE_STREAM_NAME = os.environ.get(\u0026#34;FIREHOSE_STREAM_NAME\u0026#34;) VPC_RE = re.compile(r\u0026#34;/(vpc-[0-9A-Za-z\\-]+)\u0026#34;) ISO_TS_RE = re.compile(r\u0026#34;^\\d{4}-\\d{2}-\\d{2}T\u0026#34;) def read_gz(bucket, key): obj = s3.get_object(Bucket=bucket, Key=key) with gzip.GzipFile(fileobj=obj[\u0026#34;Body\u0026#34;]) as f: return f.read().decode(\u0026#34;utf-8\u0026#34;, errors=\u0026#34;replace\u0026#34;) def flatten_once(d): out = {} for k, v in (d or {}).items(): if isinstance(v, dict): for k2, v2 in v.items(): out[f\u0026#34;{k}_{k2}\u0026#34;] = v2 else: out[k] = v return out def safe_int(x): try: return int(x) except: return None def parse_dns_line(line): raw = line.strip() if not raw: return None json_part = raw prefix_ts = None if ISO_TS_RE.match(raw): try: prefix_ts, rest = raw.split(\u0026#34; \u0026#34;, 1) json_part = rest except: pass if not json_part.startswith(\u0026#34;{\u0026#34;): idx = json_part.find(\u0026#34;{\u0026#34;) if idx != -1: json_part = json_part[idx:] try: obj = json.loads(json_part) except: return None flat = flatten_once(obj) if prefix_ts: flat[\u0026#34;_prefix_ts\u0026#34;] = prefix_ts return flat def lambda_handler(event, context): print(f\u0026#34;Received S3 Event. Records: {len(event.get(\u0026#39;Records\u0026#39;, []))}\u0026#34;) firehose_records = [] for record in event.get(\u0026#34;Records\u0026#34;, []): if \u0026#34;s3\u0026#34; not in record: continue bucket = record[\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] key = record[\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;] if not key.startswith(SOURCE_PREFIX) or not key.endswith(\u0026#34;.gz\u0026#34;): print(f\u0026#34;Skipping file: {key}\u0026#34;) continue print(f\u0026#34;Processing S3 file: {key}\u0026#34;) # Extract VPC ID from file path vpc_id_match = VPC_RE.search(key) vpc_id = vpc_id_match.group(1) if vpc_id_match else \u0026#34;unknown\u0026#34; # Read and process file content content = read_gz(bucket, key) if not content: continue for line in content.splitlines(): r = parse_dns_line(line) if not r: continue # Create flattened JSON record out = { \u0026#34;version\u0026#34;: r.get(\u0026#34;version\u0026#34;), \u0026#34;account_id\u0026#34;: r.get(\u0026#34;account_id\u0026#34;), \u0026#34;region\u0026#34;: r.get(\u0026#34;region\u0026#34;), \u0026#34;vpc_id\u0026#34;: r.get(\u0026#34;vpc_id\u0026#34;, vpc_id), \u0026#34;query_timestamp\u0026#34;: r.get(\u0026#34;query_timestamp\u0026#34;), \u0026#34;query_name\u0026#34;: r.get(\u0026#34;query_name\u0026#34;), \u0026#34;query_type\u0026#34;: r.get(\u0026#34;query_type\u0026#34;), \u0026#34;query_class\u0026#34;: r.get(\u0026#34;query_class\u0026#34;), \u0026#34;rcode\u0026#34;: r.get(\u0026#34;rcode\u0026#34;), \u0026#34;answers\u0026#34;: json.dumps(r.get(\u0026#34;answers\u0026#34;), ensure_ascii=False), \u0026#34;srcaddr\u0026#34;: r.get(\u0026#34;srcaddr\u0026#34;), \u0026#34;srcport\u0026#34;: safe_int(r.get(\u0026#34;srcport\u0026#34;)), \u0026#34;transport\u0026#34;: r.get(\u0026#34;transport\u0026#34;), \u0026#34;srcids_instance\u0026#34;: r.get(\u0026#34;srcids_instance\u0026#34;), \u0026#34;timestamp\u0026#34;: (r.get(\u0026#34;query_timestamp\u0026#34;) or r.get(\u0026#34;timestamp\u0026#34;) or r.get(\u0026#34;_prefix_ts\u0026#34;)) } # Add newline for JSONL format json_row = json.dumps(out, ensure_ascii=False) + \u0026#34;\\n\u0026#34; firehose_records.append({\u0026#39;Data\u0026#39;: json_row}) # Send to Firehose in batches of 500 if firehose_records: total_records = len(firehose_records) print(f\u0026#34;Sending {total_records} records to Firehose...\u0026#34;) batch_size = 500 for i in range(0, total_records, batch_size): batch = firehose_records[i:i + batch_size] try: response = firehose.put_record_batch( DeliveryStreamName=FIREHOSE_STREAM_NAME, Records=batch ) if response[\u0026#39;FailedPutCount\u0026#39;] \u0026gt; 0: print(f\u0026#34;Warning: {response[\u0026#39;FailedPutCount\u0026#39;]} records failed\u0026#34;) except Exception as e: print(f\u0026#34;Firehose error: {e}\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;total_records\u0026#34;: len(firehose_records)} "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.5-processing-setup/5.5.1-create-kinesis-data-firehose/","title":"Create Kinesis Data Firehose","tags":[],"description":"","content":"Create Kinesis Data Firehose Delivery Streams Create cloudtrail-firehose-stream Open Kinesis Console ‚Üí Delivery streams ‚Üí Create delivery stream\nConfigure:\nSource: Direct PUT Destination: Amazon S3 Stream name: cloudtrail-firehose-stream S3 bucket: processed-cloudtrail-logs-ACCOUNT_ID-REGION Prefix: processed-cloudtrail/date=!{timestamp:yyyy-MM-dd}/ Error prefix: processed-cloudtrail/errors/date=!{timestamp:yyyy-MM-dd}/error-type=!{firehose:error-output-type}/ Buffer size: 10 MB Buffer interval: 300 seconds Compression: GZIP IAM role: CloudTrailFirehoseRole Create delivery stream\nCreate vpc-dns-firehose-stream Stream name: vpc-dns-firehose-stream S3 bucket: processed-cloudwatch-logs-ACCOUNT_ID-REGION Prefix: vpc-logs/date=!{timestamp:yyyy-MM-dd}/ Error prefix: vpc-logs/errors/date=!{timestamp:yyyy-MM-dd}/error-type=!{firehose:error-output-type}/ IAM role: CloudWatchFirehoseRole (Same buffer/compression settings as above) Create vpc-flow-firehose-stream Stream name: vpc-flow-firehose-stream S3 bucket: processed-cloudwatch-logs-ACCOUNT_ID-REGION Prefix: eni-flow-logs/date=!{timestamp:yyyy-MM-dd}/ Error prefix: eni-flow-logs/errors/date=!{timestamp:yyyy-MM-dd}/error-type=!{firehose:error-output-type}/ IAM role: CloudWatchFirehoseRole (Same buffer/compression settings as above) "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.3-foundation-setup/5.3.3-create-iam-roles-and-policies/5.3.3.1-create-lambda-excecution-roles/","title":"Create Lambda Execution Roles","tags":[],"description":"","content":"Create CloudTrailETLLambdaServiceRole Open the IAM Console:\nNavigate to https://console.aws.amazon.com/iam/ Or: AWS Management Console ‚Üí Search for \u0026ldquo;IAM\u0026rdquo; ‚Üí Click \u0026ldquo;IAM\u0026rdquo; Navigate to Roles:\nIn the left sidebar, click \u0026ldquo;Roles\u0026rdquo; Click \u0026ldquo;Create role\u0026rdquo;\nSelect trusted entity:\nTrusted entity type: Select \u0026ldquo;AWS service\u0026rdquo; Use case: Select \u0026ldquo;Lambda\u0026rdquo; Click \u0026ldquo;Next\u0026rdquo; Add permissions:\nIn the search box, type AWSLambdaBasicExecutionRole Check the box next to \u0026ldquo;AWSLambdaBasicExecutionRole\u0026rdquo; Click \u0026ldquo;Next\u0026rdquo; Name, review, and create:\nRole name: Enter CloudTrailETLLambdaServiceRole Description: Enter Execution role for CloudTrail ETL Lambda function Click \u0026ldquo;Create role\u0026rdquo; Add inline policy:\nAfter creation, you\u0026rsquo;ll be on the role details page Click on the \u0026ldquo;Permissions\u0026rdquo; tab Click \u0026ldquo;Add permissions\u0026rdquo; ‚Üí \u0026ldquo;Create inline policy\u0026rdquo; Create inline policy:\nClick on the \u0026ldquo;JSON\u0026rdquo; tab Paste the following policy (replace ACCOUNT_ID and REGION): { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;firehose:PutRecord\u0026#34;, \u0026#34;firehose:PutRecordBatch\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:firehose:REGION:ACCOUNT_ID:deliverystream/cloudtrail-firehose-stream\u0026#34; } ] } Click \u0026ldquo;Next\u0026rdquo;\nPolicy name:\nPolicy name: Enter CloudTrailETLPolicy Click \u0026ldquo;Create policy\u0026rdquo; Verify role creation:\nYou should see the role with both managed and inline policies attached Create Remaining Lambda Roles Follow the same process for each role below (steps 3-11):\nGuardDutyETLLambdaServiceRole\nRole name: GuardDutyETLLambdaServiceRole Description: Execution role for GuardDuty ETL Lambda function Managed policy: AWSLambdaBasicExecutionRole Inline policy name: GuardDutyETLPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34;, \u0026#34;arn:aws:s3:::processed-guardduty-findings-ACCOUNT_ID-REGION/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:kms:REGION:ACCOUNT_ID:key/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:CreatePartition\u0026#34;, \u0026#34;glue:GetPartition\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:REGION:ACCOUNT_ID:catalog\u0026#34;, \u0026#34;arn:aws:glue:REGION:ACCOUNT_ID:database/security_logs\u0026#34;, \u0026#34;arn:aws:glue:REGION:ACCOUNT_ID:table/security_logs/processed_guardduty\u0026#34; ] } ] } CloudWatchETLLambdaServiceRole\nRole name: CloudWatchETLLambdaServiceRole Description: Execution role for VPC DNS logs ETL Lambda Managed policy: AWSLambdaBasicExecutionRole Inline policy name: CloudWatchETLPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;firehose:PutRecord\u0026#34;, \u0026#34;firehose:PutRecordBatch\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:firehose:REGION:ACCOUNT_ID:deliverystream/vpc-dns-firehose-stream\u0026#34; } ] } CloudWatchENIETLLambdaServiceRole\nRole name: CloudWatchENIETLLambdaServiceRole Description: Execution role for VPC Flow logs ETL Lambda Managed policy: AWSLambdaBasicExecutionRole Inline policy name: CloudWatchENIETLPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;firehose:PutRecord\u0026#34;, \u0026#34;firehose:PutRecordBatch\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:firehose:REGION:ACCOUNT_ID:deliverystream/vpc-flow-firehose-stream\u0026#34; } ] } CloudWatchExportLambdaServiceRole\nRole name: CloudWatchExportLambdaServiceRole Description: Execution role for CloudWatch log export Lambda Managed policy: AWSLambdaBasicExecutionRole Inline policy name: CloudWatchExportPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateExportTask\u0026#34;, \u0026#34;logs:DescribeExportTasks\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:REGION:ACCOUNT_ID:log-group:*\u0026#34;, \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34; ] } ] } ParseFindingsLambdaServiceRole\nRole name: ParseFindingsLambdaServiceRole Description: Execution role for parsing GuardDuty findings Managed policy: AWSLambdaBasicExecutionRole No inline policy needed IsolateEC2LambdaServiceRole\nRole name: IsolateEC2LambdaServiceRole Description: Execution role for isolating compromised EC2 instances Managed policy: AWSLambdaBasicExecutionRole Inline policy name: IsolateEC2Policy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeInstances\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } QuarantineIAMLambdaServiceRole\nRole name: QuarantineIAMLambdaServiceRole Description: Execution role for quarantining compromised IAM users Managed policy: AWSLambdaBasicExecutionRole Inline policy name: QuarantineIAMPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:AttachUserPolicy\u0026#34;, \u0026#34;iam:ListAttachedUserPolicies\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:iam::ACCOUNT_ID:user/*\u0026#34;, \u0026#34;arn:aws:iam::ACCOUNT_ID:policy/IrQuarantineIAMPolicy\u0026#34; ] } ] } AlertDispatchLambdaServiceRole\nRole name: AlertDispatchLambdaServiceRole Description: Execution role for dispatching alerts via SNS/SES/Slack Managed policy: AWSLambdaBasicExecutionRole Inline policy name: AlertDispatchPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;sns:Publish\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sns:REGION:ACCOUNT_ID:IncidentResponseAlerts\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ses:SendEmail\u0026#34;, \u0026#34;ses:SendRawEmail\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.7-dashboard-setup/5.7.2-setup-lambda/5.7.2.1-create-iam-role-and-policy-for-lambda/","title":"Lambda IAM Role and Policy setup","tags":[],"description":"","content":"In this guide, you will setup IAM Role and Policy for Lambda.\nCreate IAM Role for Lambda Open the IAM Console\nNavigate to https://console.aws.amazon.com/iam/ Or: AWS Management Console ‚Üí Services ‚Üí IAM Create Role:\nChoose the Role option on the left menu panel. Then click Create role. Select trusted entity:\nTrusted entity type: AWS Service Use case: Lambda Click \u0026ldquo;Next\u0026rdquo; Attach permissions policies:\nIn the search box, type AWSLambdaBasicExecutionRole Check the box next to \u0026ldquo;AWSLambdaBasicExecutionRole\u0026rdquo; Click \u0026ldquo;Next\u0026rdquo; Name, review, and create:\nRole name: Enter dashboard-query-role Description: Enter Execution role for Lambda function Click \u0026ldquo;Create role\u0026rdquo; Add inline policy:\nAfter creation, you\u0026rsquo;ll be on the role details page Click on the \u0026ldquo;Permissions\u0026rdquo; tab Click \u0026ldquo;Add permissions\u0026rdquo; ‚Üí \u0026ldquo;Create inline policy\u0026rdquo; Create inline policy:\nClick on the \u0026ldquo;JSON\u0026rdquo; tab Paste the following policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AthenaActions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;athena:StartQueryExecution\u0026#34;, \u0026#34;athena:GetQueryExecution\u0026#34;, \u0026#34;athena:GetQueryResults\u0026#34;, \u0026#34;athena:StopQueryExecution\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueCatalogRead\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetDatabase\u0026#34;, \u0026#34;glue:GetDatabases\u0026#34;, \u0026#34;glue:GetTable\u0026#34;, \u0026#34;glue:GetTables\u0026#34;, \u0026#34;glue:GetPartitions\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;S3SourceAndResultAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::vel-athena-results\u0026#34;, \u0026#34;arn:aws:s3:::vel-athena-results/*\u0026#34;, \u0026#34;arn:aws:s3:::vel-processed-cloudtrail-logs\u0026#34;, \u0026#34;arn:aws:s3:::vel-processed-cloudtrail-logs/*\u0026#34;, \u0026#34;arn:aws:s3:::vel-processed-guardduty\u0026#34;, \u0026#34;arn:aws:s3:::vel-processed-guardduty/*\u0026#34;, \u0026#34;arn:aws:s3:::cloudwatch-formatted\u0026#34;, \u0026#34;arn:aws:s3:::cloudwatch-formatted/*\u0026#34; ] } ] } Click \u0026ldquo;Next\u0026rdquo;\nPolicy name:\nPolicy name: Enter lambda-query-policy Click \u0026ldquo;Create policy\u0026rdquo; Verify role creation:\nYou should see the role with both managed and inline policies attached "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.10-cleanup/5.10.1-manual-cleanup/","title":"Manual Cleanup","tags":[],"description":"","content":"Clean up (Manual Infrastructure Setup) Phase 1: Automation and Monitoring Cleanup The goal here is to stop all active processes and delete the monitoring and core automation resources (EventBridge, Step Functions, SNS, GuardDuty, Flow Logs, CloudTrail).\n1. Delete Incident Response Automation 1.1 Delete EventBridge Rule\nGo to EventBridge Console ‚Üí Rules. Select the rule: IncidentResponseAlert. Click \u0026ldquo;Delete\u0026rdquo;. 1.2 Delete Step Functions State Machine\nGo to Step Functions Console ‚Üí State Machines. Select the State Machine: IncidentResponseStepFunctions. Click \u0026ldquo;Delete\u0026rdquo;. 1.3 Delete SNS Topic and Subscription\nGo to SNS Console ‚Üí Topics ‚Üí IncidentResponseAlerts. First, delete the subscription associated with ir-alert-dispatch. Then, delete the topic itself by clicking \u0026ldquo;Delete topic\u0026rdquo;. 1.4 Delete GuardDuty Detector\nGo to GuardDuty Console ‚Üí Settings ‚Üí General. Click \u0026ldquo;Suspend\u0026rdquo; to stop processing, then click \u0026ldquo;Disable GuardDuty\u0026rdquo; (or \u0026ldquo;Delete detector\u0026rdquo;). 1.5 Disable VPC Flow Logs\nGo to VPC Console ‚Üí VPC Flow Logs. Select the flow log created (associated with YOUR_VPC_ID). Click \u0026ldquo;Delete flow log\u0026rdquo;. 1.6 Delete CloudTrail Trail\nGo to CloudTrail Console ‚Üí Trails. Select the trail: incident-responses-cloudtrail-ACCOUNT_ID-REGION. Click \u0026ldquo;Delete\u0026rdquo;. Phase 2: Lambda and Compute Cleanup 2. Delete All Lambda Functions (9 Functions) Go to the Lambda Console and delete the following functions:\nincident-response-cloudtrail-etl incident-response-guardduty-etl cloudwatch-etl-lambda cloudwatch-eni-etl-lambda cloudwatch-export-lambda ir-parse-findings-lambda ir-isolate-ec2-lambda ir-quarantine-iam-lambda ir-alert-dispatch 3. Delete Isolation Security Group Go to EC2 Console ‚Üí Security Groups. Find and select the Security Group: IR-Isolation-SG (using ID sg-XXXXXXX). Click \u0026ldquo;Delete security group\u0026rdquo;. 4. Delete CloudWatch Log Groups Go to the CloudWatch Console ‚Üí Log Groups and delete:\nThe centralized log group: /aws/incident-response/centralized-logs. Any associated Lambda log groups for the 9 deleted functions (e.g., /aws/lambda/ir-parse-findings-lambda). Phase 3: Processing and Data Lake Cleanup 5. Delete Kinesis Data Firehose Streams Go to the Kinesis Console ‚Üí Delivery Streams and delete:\ncloudtrail-firehose-stream vpc-dns-firehose-stream vpc-flow-firehose-stream 6. Delete AWS Glue Tables and Database 6.1 Delete Glue Tables\nGo to Glue Console ‚Üí Tables. Select and delete: security_logs.processed_cloudtrail, security_logs.processed_guardduty, security_logs.vpc_logs, and security_logs.eni_flow_logs. 6.2 Delete Glue Database\nGo to Glue Console ‚Üí Databases. Select the database: security_logs and click \u0026ldquo;Delete\u0026rdquo;. 7. Delete IAM Roles and Policies 7.1 Delete IAM Policies\nGo to IAM Console ‚Üí Policies. Delete the custom managed policy: IrQuarantineIAMPolicy. Note: Inline policies created in the setup will be deleted automatically when the corresponding role is deleted. 7.2 Delete IAM Roles\nGo to IAM Console ‚Üí Roles. Delete the following 17 roles: Lambda Execution Roles: CloudTrailETLLambdaServiceRole, GuardDutyETLLambdaServiceRole, CloudWatchETLLambdaServiceRole, CloudWatchENIETLLambdaServiceRole, CloudWatchExportLambdaServiceRole, ParseFindingsLambdaServiceRole, IsolateEC2LambdaServiceRole, QuarantineIAMLambdaServiceRole, AlertDispatchLambdaServiceRole. Service Roles: CloudTrailFirehoseRole, CloudWatchFirehoseRole, StepFunctionsRole, IncidentResponseStepFunctionsEventRole, FlowLogsIAMRole, GlueCloudWatchRole. Phase 4: S3 Bucket Cleanup (Data Deletion) 8. Empty and Delete S3 Buckets This is the final step to ensure all storage charges are stopped.\nBucket Name Purpose incident-response-log-list-bucket-ACCOUNT_ID-REGION Primary Log Source (CloudTrail/GuardDuty/Exported CW) processed-cloudtrail-logs-ACCOUNT_ID-REGION Firehose Destination for CloudTrail logs processed-cloudwatch-logs-ACCOUNT_ID-REGION Firehose Destination for VPC DNS/Flow logs processed-guardduty-findings-ACCOUNT_ID-REGION ETL Destination for GuardDuty logs athena-query-results-ACCOUNT_ID-REGION Athena Query Results Storage Go to the S3 Console. For each of the 5 buckets: Click on the bucket name. Go to the \u0026ldquo;Objects\u0026rdquo; tab. Click \u0026ldquo;Empty\u0026rdquo; to clear all data. You must confirm the permanent delete by typing permanently delete. Go back to the S3 bucket list, select the bucket, and click \u0026ldquo;Delete\u0026rdquo;. "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.1-workshop-overview/","title":"Overview","tags":[],"description":"","content":"System Components Auto Incident Response and Forensics is an architecture that uses automation services to ingest, process, and automatically respond to security findings, minimizing the time required for human intervention and aids security personel in visualizing and analyzing logs. This system is built around AWS Security Services (CloudTrail, GuardDuty, VPC Flow Logs, CloudWatch) feeding data into a Centralized Data Lake (S3/Glue/Athena) for analysis. The core automation is driven by AWS EventBridge rules triggering AWS Step Functions workflows, which then execute AWS Lambda functions to perform isolation and alerting actions. Workshop Architecture\nWorkshop overview In this workshop, you will deploy a multi-phase system to achieve end-to-end security automation. This includes:\nFoundation Setup: Creating dedicated S3 buckets and IAM roles to support all services. Monitoring Setup: Enabling and configuring key security logs (CloudTrail, GuardDuty, VPC Flow Logs) to direct data to the central log ingestion point. Processing Setup: Deploying Kinesis Firehose, Lambda ETLs, and Glue/Athena tables to transform raw logs into an easily queryable security data lake. Automation Setup: Creating the Isolation Security Group, SNS Topic, Incident Response Lambda Functions, and the Step Functions State Machine that executes automatic quarantine actions when GuardDuty detects findings. Dashboard Setup: Hosting a secure, S3-based static web interface accelerated by CloudFront and protected by Cognito to provide analysts with real-time visualization of forensic data and direct query capabilities via API Gateway. "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.2-prerequiste/","title":"Prerequisites","tags":[],"description":"","content":"Required Access and Information Before proceeding with the setup of the Automated AWS Incident Response and Forensics System, ensure you have gathered the required access credentials and information below.\nüîë Access \u0026amp; Identifiers AWS Account with Administrative Access You need full administrative permissions to create resources across multiple AWS services. Access to the AWS Management Console. Your AWS Account ID Format: 12-digit number (e.g., 123456789012). Placeholder: Replace ACCOUNT_ID throughout the guide. Target AWS Region Choose the region where you\u0026rsquo;ll deploy the system (e.g., us-east-1). Placeholder: Replace REGION throughout the guide. VPC ID A VPC with at least one subnet is required for VPC Flow Logs. Placeholder: Replace YOUR_VPC_ID in the guide. Amazon SES Verified Email Address Required for sending and recieving email alerts. Verify this address in the SES Console. Placeholder: Replace YOUR_VERIFIED_EMAIL@example.com. Slack Webhook URL (Optional) If you want Slack notifications, obtain a webhook URL from your Slack workspace. Placeholder: Replace YOUR_SLACK_WEBHOOK_URL. "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.3-foundation-setup/5.3.1-set-up-s3-buckets/","title":"Set up S3 buckets","tags":[],"description":"","content":"In this section, you will create 5 S3 buckets that serve as the foundation for the Auto Incident Response system.\nImportant: Replace ACCOUNT_ID with your AWS Account ID and REGION with your target region (e.g., us-east-1) in all bucket names.\nBucket Names incident-response-log-list-bucket-ACCOUNT_ID-REGION - Primary log collection bucket processed-cloudtrail-logs-ACCOUNT_ID-REGION - Stores processed CloudTrail logs athena-query-results-ACCOUNT_ID-REGION - Stores Athena query results processed-cloudwatch-logs-ACCOUNT_ID-REGION - Stores processed CloudWatch logs processed-guardduty-findings-ACCOUNT_ID-REGION - Stores processed GuardDuty findings Bucket Creation Instructions Open the Amazon S3 Console Navigate to https://console.aws.amazon.com/s3/ Or: AWS Management Console ‚Üí Services ‚Üí S3 Click on \u0026ldquo;Create bucket\u0026rdquo; General configuration: Bucket name: Enter incident-response-log-list-bucket-ACCOUNT_ID-REGION Example: incident-response-log-list-bucket-123456789012-us-east-1 AWS Region: Select your target region (e.g., US East (N. Virginia) us-east-1) Object Ownership:\nKeep default: ACLs disabled (recommended) Block Public Access settings for this bucket:\nCheck \u0026ldquo;Block all public access\u0026rdquo; Ensure all 4 sub-options are checked: ‚úì Block public access to buckets and objects granted through new access control lists (ACLs) ‚úì Block public access to buckets and objects granted through any access control lists (ACLs) ‚úì Block public access to buckets and objects granted through new public bucket or access point policies ‚úì Block public and cross-account access to buckets and objects through any public bucket or access point policies Bucket Versioning:\nSelect \u0026ldquo;Enable\u0026rdquo; Tags (optional):\nAdd tags if desired Example: Key=Purpose, Value=IncidentResponse Default encryption:\nEncryption type: Select \u0026ldquo;Server-side encryption with Amazon S3 managed keys (SSE-S3)\u0026rdquo; Bucket Key: Keep default (Enabled) Advanced settings:\nKeep all defaults Click \u0026ldquo;Create bucket\u0026rdquo;\nVerify bucket creation:\nYou should see a success message The bucket should appear in your S3 buckets list Repeat steps 2-10 for the remaining 4 buckets:\nprocessed-cloudtrail-logs-ACCOUNT_ID-REGION athena-query-results-ACCOUNT_ID-REGION processed-cloudwatch-logs-ACCOUNT_ID-REGION processed-guardduty-findings-ACCOUNT_ID-REGION Verify all 5 buckets are created:\nNavigate to S3 Console You should see all 5 buckets listed "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.7-dashboard-setup/5.7.1-setup-s3/","title":"Setup S3 Bucket for Dashboard","tags":[],"description":"","content":"In this guide, you will setup a S3 to contain web files and folder. Important: Replace ACCOUNT_ID with your AWS Account ID and REGION with your target region (e.g., us-east-1) in all bucket names.\nBucket Names static-dashboard-bucket-ACCOUNT_ID-REGION - Store builded web files and folder\nBucket Creation Instructions Open the Amazon S3 Console\nNavigate to https://console.aws.amazon.com/s3/ Or: AWS Management Console ‚Üí Services ‚Üí S3 Click on \u0026ldquo;Create bucket\u0026rdquo;\nBucket create setting:\nKeep the setting like default: Bucket name: Enter static-dashboard-bucket-ACCOUNT_ID-REGION Example: static-dashboard-bucket-123456789012-us-east-1 Ownership: ACLs disabled Block Public Access: Block all public access Bucket versioning: Disable Tags(Optional): Add if you want Encryption: SSE-S3 Bucket key: Enable Click Create bucket Verify bucket creation:\nYou should see a success message The bucket should appear in your S3 buckets list Upload files and folder:\nGo to Github to get the web content and upload to S3 "},{"uri":"https://veljg.github.io/AWS-Worklog/2-proposal/","title":"Proposal","tags":[],"description":"","content":"\nAutomated AWS Incident Response and Forensics System Proposal Link: Proposal 1. Executive Summary Our team is building an automated incident response and forensics solution as part of the AWS First Cloud Journey internship program. The idea is straightforward‚Äîwhen a security issue happens in AWS, we want the system to respond automatically without waiting for manual intervention.\nWe\u0026rsquo;re creating a platform that automatically detects security findings from GuardDuty, isolates affected resources, captures forensic evidence through comprehensive data collection, and provides analytics and dashboards where security teams can investigate what happened. Everything is built using Infrastructure-as-Code with AWS CDK, so customers can easily deploy it into their own AWS accounts.\n2. Problem Statement What‚Äôs the Problem? The increasing frequency and sophistication of cyber threats pose significant risks to organizations relying on cloud infrastructure. Manual incident response processes are often slow, inconsistent, and prone to human error, which can lead to prolonged system downtime, data breaches, and financial losses. The project aims to address these challenges by developing an automated, reliable, and scalable incident response system that minimizes response time, enhances forensic capabilities, and reduces operational costs.\nThe Solution The main use cases include detecting unauthorized AWS credential use, identifying compromised EC2 instances, and ensuring forensic data is properly collected, processed, and stored for investigation. Our architecture integrates VPC Flow Logs, CloudTrail, CloudWatch, and GuardDuty to detect threats, while Step Functions orchestrates the automated response workflow including EC2 isolation, ASG detachment, Create Snapshot and IAM quarantine. All evidence is collected and processed through custom ETL Lambda and Data Firehose, using Athena for forensic analysis. The system also includes alert dispatching, notification via messaging and email, and provides dashboards and analytics for security teams to investigate what happened.\nBenefits and Return on Investment Rapid threat detection: Automated response reduces the window of vulnerability. Comprehensive evidence gathering: Automated forensic data collection facilitates faster investigations. Cost-effective deployment: Leveraging AWS serverless services minimizes infrastructure expenses. Improved security posture: Continuous monitoring and real-time alerts. Actionable insights: Dashboards and analytics empower security teams. Scalability: Adaptable to organizations of various sizes and incident volumes. 3. Solution Architecture Our solution uses a comprehensive multi-stage architecture for automated incident response and forensics:\nAWS Services Used Amazon GuardDuty: Continuously monitors for security threats and suspicious activity. AWS Step Functions: Orchestrates the incident response workflow. AWS Lambda: Runs automation code for isolation and data processing. Amazon EventBridge: Routes findings from GuardDuty to Step Functions. Amazon S3: Stores forensic evidence and hosts static dashboard. Amazon Athena: Enables SQL queries against forensic datasets. Amazon API Gateway: Facilitates communication between dashboard and backend. Amazon Cognito: Secures access for dashboard users. Amazon CloudFront: Accelerates dashboard delivery across the globe. Amazon SNS \u0026amp; SES: Handles notifications via messaging and email. AWS CloudTrail: Logs all actions for auditing. Amazon CloudWatch: Monitoring and dashboards. Amazon EC2: Optional instances for analysis. AWS KMS: Key management for encryption. Amazon Kinesis Data Firehose: Streams data to S3. Component Design Data Collection \u0026amp; Detection Layer: Collects events from VPC Flow Logs, CloudTrail, CloudWatch, EC2, and GuardDuty. Event Processing Layer: Alert Dispatch, EventBridge routes findings to Step Functions; events are classified by type. Automated Response Orchestration: Step Functions handle parsing, decision making, EC2 isolation, termination protection, ASG detachment, snapshot creation, and IAM quarantine. Alerting \u0026amp; Notification Layer: SNS, Slack \u0026amp; SES handles notifications via messaging and email, Alert Dispatch. Data Processing \u0026amp; Analytics Layer: ETL pipeline with Lambda and Data Firehose processes raw logs into S3; Athena queries the data. Dashboard \u0026amp; Analysis Layer: S3-hosted React dashboard with Cognito auth, consuming data via API Gateway and Athena. 4. Technical Implementation Implementation Phases We use Agile Scrum with 1-week sprints over 6 weeks:\nSprint 1: Foundation \u0026amp; Setup (VPC, Security Groups, Training). Sprint 2: Core Orchestration (Step Functions, Lambda, GuardDuty integration). Sprint 3: Data \u0026amp; Analytics (S3, Athena, ETL pipeline). Sprint 4: Dashboard \u0026amp; UI (Static site, API Gateway, CloudFront). Sprint 5: Testing \u0026amp; Optimization (Cognito, Performance testing, Simulations). Sprint 6: Documentation \u0026amp; Handover (Guides, Demos, Final Polish). Technical Requirements Frontend \u0026amp; Dashboards: Custom HTML/CSS/JS hosted on S3, served via CloudFront. Backend \u0026amp; Processing: Python 3.12 for Lambda, Step Functions for orchestration. Data \u0026amp; Storage: S3 for evidence, Athena for querying, Firehose for streaming. Infrastructure: All defined in AWS CDK (Python). Security: GuardDuty for detection, IAM for least privilege, KMS for encryption. 5. Timeline \u0026amp; Milestones Project Timeline Project Timeline\nWeek 6-7 (Foundation \u0026amp; Setup) Activities: Team training on GuardDuty/Step Functions, architecture design review, VPC and security setup. Deliverables: Architecture document v1, team training completion, GitHub repository established. Week 7-9 (Core Orchestration) Activities: Step Functions workflow development, Lambda function coding for all response actions, EventBridge integration, SNS/SES setup, integration testing. Deliverables: Step Functions state machine definition, 7+ Lambda functions with documentation, GuardDuty integration, notification system, API Gateway. Week 10 (Data \u0026amp; Analytics) Activities: S3 forensic storage setup, Athena table creation, ETL pipeline development, SQL query library. Deliverables: 15+ Athena queries documented, forensic analysis runbooks, processed data storage. Week 11 (Dashboard \u0026amp; UI) Activities: Static dashboard development, Cognito authentication, API Gateway setup, CloudFront CDN configuration, dashboard integration. Deliverables: S3-hosted dashboard, authentication system, query interface, real-time results integration. Week 12 (Testing \u0026amp; Validation \u0026amp; Optimization) Activities: Manual testing, security scanning including simulated incident scenarios (5+ workflows), performance testing, attack simulation. Optimize data with Athena query and Data Firehose. Deliverables: Security scan results, incident simulation videos, data optimization. Week 13 (Documentation \u0026amp; Handover) Activities: Deployment guide, API documentation, knowledge transfer sessions, final demo, GitHub cleanup. Deliverables: Complete GitHub repository (public), deployment guide instructions, live workshop demonstration. 6. Budget Estimation You can find the detailed budget estimation on the AWS Pricing Calculator.\nInfrastructure Costs Typical monthly deployment cost (Free Tier / Low scale): ~$5.01\nGuardDuty: ~$1.80/month S3: ~$1.07/month KMS: ~$1.12/month CloudTrail: ~$0.55/month Athena: ~$0.29/month Amazon Simple Email Service (SES): ~$0.09/month Amazon API Gateway: ~$0.05/month Amazon Data firehose: ~$0.04/month Lambda, Step Functions, SNS: Generally within Free Tier limits for typical usage. Note: Costs assume typical usage of 20-150 incidents per month.\n7. Risk Assessment Risk Matrix Performance Bottlenecks: High data volume slowing down queries. Security Breaches: Compromise of the forensic data itself. Cost Overruns: Unchecked logging or infinite loops. Mitigation Strategies Performance: Monitor Athena/Firehose; optimize queries; dynamic resource adjustment. Security: Encryption (KMS), strict IAM roles, audit logging, compliance checks. Cost: AWS budget alerts, cost anomaly detection, auto-scaling limits. Disaster Recovery: Backups, failover procedures, and redundancy measures. 8. Expected Outcomes Technical Improvements Automated Response: Zero-touch isolation of compromised resources. Speed: Reduction of investigation time from hours to minutes. Reliability: Consistent, repeatable evidence collection without human error. Long-term Value Scalable Architecture: Foundation for future security automation. Knowledge: Team competency in advanced AWS security and serverless concepts. Reusable Asset: A deployable solution for other AWS customers or teams. Status: Ready for Review \u0026amp; Approval Project Code: AWS-FCJ-IR-FORENSICS-2025\n"},{"uri":"https://veljg.github.io/AWS-Worklog/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: ‚ÄúAI-Driven Development Life Cycle: Reimagining Software Engineering‚Äù Event Objectives Explore the transformative shift in software development driven by generative AI. Introduce the AI-Driven Development Life Cycle (AI-DLC) and its core concepts. Kiro and Amazon Q Developer demonstration Speakers Toan Huynh ‚Äì Specialist SA, PACE My Nguyen - Sr. Prototyping Architect, Amazon Web Services - ASEAN Key Highlights Focused on the concept of AI-DLC, a framework where AI orchestrates the development process, including planning, task decomposition, and architectural suggestions, while developers retain ultimate responsibility for validation, decision-making, and oversight - AI-DLC Core Concept: The approach is Human-Centric, with AI acting as a Collaborator to enhance developer capabilities, leading to Accelerated Delivery (cycles measured in hours/days instead of weeks/months).\n- AI-DLC Workflow: It\u0026rsquo;s an iterative loop involving AI Tasks (Create plan, Implement Plan, Seek clarification) and Human Tasks (Provide clarification, Implement Plan), where the AI repeatedly asks clarifying questions and only implements solutions after human validation.\n- AI-DLC Stages: The lifecycle is broken down into Inception, Construction, and Operation. Each stage builds richer context for the next:\nInception: Includes building context, elaborating intent with User Stories, and planning with Units of Work.\nConstruction: Involves Domain Modeling, code generation and testing, adding architectural components, and deploying with IaC \u0026amp; tests.\nOperation: Focuses on deploying in production and managing incidents.\n- Challenges AI-DLC Aims to Solve:\nScaling AI development: AI coding tools can fail with complex projects.\nLimited control: Existing tools make it difficult to collaborate with and manage AI agents.\nCode quality: Maintaining quality control when moving from proof-of-concept to production becomes difficult.\nDeep Dive: Kiro - The AI IDE for Prototype to Production Kiro, an AI-first Integrated Development Environment (IDE) that supports the AI-DLC, focusing on Spec-driven development - Spec-driven Development: Kiro turns a high-level prompt (e.g., \u0026ldquo;I want to create a chat application like Slack\u0026rdquo;) into clear requirements (requirements.md), system design (design.md), and discrete tasks (tasks.md), fundamentally shifting development from \u0026ldquo;vibe coding\u0026rdquo; to a structured, traceable process. Developers collaborate with Kiro on these specs, which serve as the source of truth.\n- Agentic Workflows: Kiro\u0026rsquo;s AI agents implement the spec while keeping the human developer in control, with the key features being:\n+ Implementation Plan: Kiro generates a detailed Implementation Plan with start tasks, sub-tasks (e.g., \u0026ldquo;Implement user registration and login endpoints,\u0026rdquo; \u0026ldquo;Implement JWT middleware\u0026rdquo;), and links them back to specific requirements for validation.\n+ Agent Hooks: These delegate tasks to AI agents that trigger on events such as \u0026ldquo;file save.\u0026rdquo; They autonomously execute in the background based on predefined prompts, helping to scale work by generating documentation, unit tests, or optimizing code performance.\nKey Takeaways - AI Ensures Production Readiness: Kiro creating detailed design documents (like data flow diagrams and API contracts), and generating unit tests before the code is written, ensures that AI-generated code is production-ready and maintainable, not just a quick prototype.\n- Human Control via Artifacts: Developers maintain control not by writing the bulk of the code, but by validating and refining the artifacts‚Äîthe requirements, the design, and the task plan‚Äîbefore the AI agents execute the implementation.\nApplying to Work - Integrate Amazon Q Developer/Similar Tools: Integrating AI coding assistants into my academic projects to automate boilerplate code and common tasks to boost productivity.\n- Focus on High-Value Tasks: By letting AI automate undifferentiated heavy lifting, I can focus my time on mastering higher-value, creative tasks like Domain Modeling and Architectural Design, which are crucial human-centric activities in the Construction phase.\nEvent Experience Attending the AI-Driven Development Life Cycle: Reimagining Software Engineering event provided a fascinating glimpse into the future of software development. It was clear that Generative AI isn\u0026rsquo;t just a coding assistant; it\u0026rsquo;s poised to become a core orchestrator of the entire development process. The session was well-structured, moving from the overarching concept of AI-DLC to specific demonstrations of Amazon Q Developer and Kiro. The demo of Kiro was particularly impactful, showing how a single text prompt can be transformed into a full, executable, and traceable development plan inside the IDE.\nLessons learned The three main challenges with current AI development (scaling, limited control, and code quality) made the structured, human-validated approach of AI-DLC seem highly necessary and well-thought-out. Some event photos Group Picture\n"},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Complete Module 3 \u0026amp; 4 Help team members get up to speed Discuss workshop ideas Do first optional research: AWS Well Architected Framework Check out AWS Advanced Networking - Specialty Study Guide Check out AWS Microsoft Workload Check out AWS Skill Builder Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Started on module 4 - Learnt about AWS Storage services - Learnt about S3 Bucket Access Point and Storage class. 15/09/2025 15/09/2025 3 - Lab 4 - Lab 6: RDS Database - Successfully used Linux via EC2 instance to: + Install and use MySQL database + Install and run a web application, can be connected to from browsers - Created Load Balancer and Target Groups - Paessler Webstress tool has been discontinued, cannot test using the given tool - Successfully installed Siege on EC2 instance to load test: + Ran load test, simulated 50 users at the same time for 10 minutes + The EC2 instance is terminated and load balanced 10 times in succession + Siege automatically stopped after 5 minutes due to too much packet loss + The reason might be due to the EC2 instance and RDS Database were created using the only available free tier options, and could not handle the increased traffic. - Successfully hosted database using RDS 16/09/2025 16/09/2025 Lab 6 4 Module 4 3, module 4 4 - Help teammate with Lab 5 - Instruction of lab 5 is missing some steps: 5.5.3: The given script didn\u0026rsquo;t connect the RDS database to MySQL, 5.5.5: The instruction is missing the step: cd to the application folder 17/09/2025 17/09/2025 Lab 5 5 - Joined the AWS Cloud Day 2025 HCM Event: Gen AI and Data track 18/09/2025 18/09/2025 Vietnam Cloud Day Agenda Event Summary and Experience 6 - Retry Lab 10: + Fix given template: Region changed to ap-southeast-1, instance changed to t3.micro + Successfully configured endpoints and rules in Route 53 for hybrid DNS + Successfully deployed Microsoft AD\n- Lab 8:\n+ Viewed metrics and graph using CloudWatch on selected EC2 Instances + Learnt the basics on monitoring logs + 8.4.2: cannot be done: Can\u0026rsquo;t find the resource s3://workshop-template-bucket/logger.py . + Configured CloudWatch Alarm and Dashboard - Lab 14: Installing Ubuntu - Reformatted worklog - Wrote about Cloud Day 2025 experiences - Additional research on AWS Well-Architected Framework: + Documents a set of foundational questions that enable you to understand how a specific architecture aligns with cloud best practices + The pillars: ‚Ä¢ Operational Excellence: Focuses on running and monitoring systems to deliver business value, and continually improving supporting processes and procedures. ‚Ä¢ Security: Focuses on protecting information, systems, and assets, while delivering business value through risk assessments and mitigation strategies. ‚Ä¢ Reliability: Focuses on the ability of a workload to perform its intended function correctly and consistently when it\u0026rsquo;s expected to. ‚Ä¢ Performance Efficiency: Focuses on using computing resources efficiently to meet system requirements, and maintaining that efficiency as demand changes. ‚Ä¢ Cost Optimization: Focuses on avoiding unnecessary costs by managing and controlling where money is spent in the cloud. ‚Ä¢ Sustainability: Focuses on minimizing the environmental impacts of running cloud workloads. + Purpose: ‚Ä¢ A cloud service for reviewing and measuring your workloads against AWS best practices to build more secure, resilient, high-performing, and cost-effective systems.\n‚Ä¢ Core Function: Identifies High Risk Issues (HRIs) and Medium Risk Issues (MRIs) in your architecture and provides an improvement plan to mitigate them. + Usage: ‚Ä¢ Step 1: Define a Workload: Specify the name, environment, owner, and regions for the application or system you are reviewing.\n‚Ä¢ Step 2: Document the State: Answer questions based on the pillars of the AWS Well-Architected Framework (Security, Reliability, etc.) and save a \u0026ldquo;milestone\u0026rdquo; to capture your progress.\n‚Ä¢ Step 3: Review the Improvement Plan: The tool generates a prioritized list of risks (HRIs and MRIs) based on your answers.\n‚Ä¢ Step 4: Make Improvements \u0026amp; Measure: Update your architecture based on the plan, then update your answers in the tool to track the reduction in risks over time + Key Features: ‚Ä¢ Workloads: The central component representing your application; can be viewed, edited, shared, and deleted. ‚Ä¢ Lenses: Provide focused questions for specific technologies (e.g., Serverless Lens) or industries. You can also create Custom Lenses for internal standards. ‚Ä¢ Review Templates \u0026amp; Profiles: Help standardize reviews by pre-filling common answers (Templates) and prioritizing questions based on business goals (Profiles). ‚Ä¢ Jira Integration: Allows you to sync improvement items directly from the Well-Architected Tool into your Jira projects as epics, tasks, and sub-tasks for streamlined tracking. + Security: ‚Ä¢ Shared Responsibility Model: AWS secures the cloud infrastructure, while you are responsible for securing your workloads in the cloud. ‚Ä¢ IAM Integration: Access is controlled through AWS IAM, with pre-built policies for full access and read-only access. ‚Ä¢ Data Protection: Recommends using IAM users (not root), enabling MFA, and avoiding placing sensitive data in free-form text fields. ‚Ä¢ Monitoring \u0026amp; Auditing: Integrates with AWS CloudTrail to log all API activity and with Amazon EventBridge to trigger automated notifications. 19/09/2025 20/09/2025 Lab 10 Lab 8 Lab 14 AWS Well Architected Framework Week 2 Achievements: Successfully completed core labs in Module 3 (focused on RDS, Load Balancing) and made significant progress in Module 4.\nUtilized Linux via EC2 to install and run a MySQL database and deploy a connected web application, successfully hosting the database using RDS (Relational Database Service).\nSuccessfully set up a Load Balancer and Target Groups, and adapted to a discontinued tool by installing and using Siege on an EC2 instance to execute a load test.\nConfigured Route 53 endpoints and rules for a hybrid DNS setup, including the deployment of Microsoft AD.\nGained foundational knowledge in cloud monitoring by viewing metrics and graphs in CloudWatch, configuring Alarms and Dashboards, and learning the basics of log management.\nTeam Support: Assisted team members with labs, identifying and helping to correct missing steps in the lab instructions.\nAttended the AWS Cloud Day 2025 HCM Event (Gen AI and Data track).\nOptional Research: Completed research on the AWS Well-Architected Framework, documenting its purpose, usage steps, key features (Lenses, Templates, Profiles), and security considerations.\nTechnical Setup: Successfully installed Ubuntu on a machine and reformatted the worklog for improved presentation.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"AWS DevOps \u0026amp; Developer Productivity Blog Amazon Q Developer CLI h·ªó tr·ª£ ƒë·∫ßu v√†o h√¨nh ·∫£nh trong terminal c·ªßa b·∫°n b·ªüi Keerthi Sreenivas Konjety v√†o ng√†y 21 th√°ng 5 2025 trong Amazon Q Developer, Announcements | Permalink | Chia s·∫ª\nTrong b√†i ƒëƒÉng n√†y, t√¥i s·∫Ω kh√°m ph√° c√°ch t√≠nh nƒÉng h·ªó tr·ª£ h√¨nh ·∫£nh trong Amazon Q Developer Command Line Interface (CLI) thay ƒë·ªïi quy tr√¨nh l√†m vi·ªác ph√°t tri·ªÉn. Q Developer CLI g·∫ßn ƒë√¢y ƒë√£ b·ªï sung h·ªó tr·ª£ h√¨nh ·∫£nh, m·ªü r·ªông kh·∫£ nƒÉng x·ª≠ l√Ω th√¥ng tin tr·ª±c quan v√† tƒÉng c∆∞·ªùng nƒÉng su·∫•t c·ªßa nh√† ph√°t tri·ªÉn. T√≠nh nƒÉng m·ªõi n√†y cho ph√©p c√°c nh√† ph√°t tri·ªÉn t∆∞∆°ng t√°c tr·ª±c ti·∫øp v·ªõi s∆° ƒë·ªì, b·∫£n thi·∫øt k·∫ø ki·∫øn tr√∫c v√† c√°c t√†i s·∫£n tr·ª±c quan kh√°c th√¥ng qua d√≤ng l·ªánh.\nPh√°t tri·ªÉn ph·∫ßn m·ªÅm hi·ªán ƒë·∫°i ng√†y c√†ng d·ª±a v√†o c√°c bi·ªÉu di·ªÖn tr·ª±c quan ƒë·ªÉ truy·ªÅn ƒë·∫°t √Ω t∆∞·ªüng. V√≠ d·ª•, s∆° ƒë·ªì ki·∫øn tr√∫c minh h·ªça c√°c th√†nh ph·∫ßn h·ªá th·ªëng v√† s·ª± t∆∞∆°ng t√°c c·ªßa ch√∫ng, trong khi s∆° ƒë·ªì th·ª±c th·ªÉ li√™n k·∫øt ph√°c th·∫£o c·∫•u tr√∫c c∆° s·ªü d·ªØ li·ªáu. Vi·ªác chuy·ªÉn ƒë·ªïi t√†i s·∫£n tr·ª±c quan th√†nh m√£ ho·∫°t ƒë·ªông th∆∞·ªùng l√† m·ªôt quy tr√¨nh gi·∫£i th√≠ch v√† tri·ªÉn khai th·ªß c√¥ng, d·ªÖ x·∫£y ra l·ªói.\nT√≠nh nƒÉng h·ªó tr·ª£ h√¨nh ·∫£nh m·ªõi trong Q Developer CLI thu h·∫πp kho·∫£ng c√°ch n√†y b·∫±ng c√°ch cho ph√©p c√°c nh√† ph√°t tri·ªÉn cung c·∫•p h√¨nh ·∫£nh tr·ª±c ti·∫øp cho t√°c nh√¢n Q Developer CLI ƒë·ªÉ ph√¢n t√≠ch. T√¥i r·∫•t h√†o h·ª©ng khi s·ª≠ d·ª•ng t√≠nh nƒÉng n√†y ƒë·ªÉ chuy·ªÉn ƒë·ªïi c√°c s∆° ƒë·ªì ki·∫øn tr√∫c c·ªßa m√¨nh t·ª´ c√°c √Ω t∆∞·ªüng ph√°c th·∫£o, v·∫Ω tay th√†nh c√°c t√†i li·ªáu thi·∫øt k·∫ø trau chu·ªët, v√† sau ƒë√≥ th√†nh c∆° s·ªü h·∫° t·∫ßng d∆∞·ªõi d·∫°ng m√£. T√¥i mong mu·ªën √°p d·ª•ng n√≥ trong nhi·ªÅu tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng kh√°c nhau, cho d√π t√¥i ƒëang b·∫Øt ƒë·∫ßu m·ªôt d·ª± √°n m·ªõi hay tinh gi·∫£n c√°c quy tr√¨nh l√†m vi·ªác h√†ng ng√†y c·ªßa m√¨nh.\nT·∫°i th·ªùi ƒëi·ªÉm ra m·∫Øt, Q Developer CLI h·ªó tr·ª£ c√°c ƒë·ªãnh d·∫°ng h√¨nh ·∫£nh JPEG, PNG, WEBP v√† GIF, c√πng v·ªõi kh·∫£ nƒÉng t·∫£i l√™n 10 h√¨nh ·∫£nh cho m·ªói y√™u c·∫ßu. B·∫°n ph·∫£i s·ª≠ d·ª•ng phi√™n b·∫£n m·ªõi nh·∫•t (1.10.0 tr·ªü l√™n) c·ªßa Q Developer CLI ƒë·ªÉ t·∫≠n h∆∞·ªüng t√≠nh nƒÉng h·ªó tr·ª£ h√¨nh ·∫£nh trong Q Developer CLI. H√£y s·ª≠ d·ª•ng h∆∞·ªõng d·∫´n n√†y ƒë·ªÉ n√¢ng c·∫•p ho·∫∑c c√†i ƒë·∫∑t phi√™n b·∫£n m·ªõi nh·∫•t.\nT√¥i s·∫Ω s·ª≠ d·ª•ng b·ªën t√¨nh hu·ªëng sau l√†m v√≠ d·ª• ƒë·ªÉ ch·ª©ng minh l·ª£i √≠ch c·ªßa h·ªó tr·ª£ h√¨nh ·∫£nh cho Q Developer CLI.\nTr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng 1: T·∫°o c∆° s·ªü h·∫° t·∫ßng d∆∞·ªõi d·∫°ng m√£ t·ª´ s∆° ƒë·ªì ki·∫øn tr√∫c S∆° ƒë·ªì sau m√¥ t·∫£ m·ªôt ·ª©ng d·ª•ng thay ƒë·ªïi k√≠ch th∆∞·ªõc h√¨nh ·∫£nh. N√≥ bao g·ªìm m·ªôt bucket Amazon S3 ngu·ªìn m√† ng∆∞·ªùi d√πng t·∫£i h√¨nh ·∫£nh l√™n, v√† m·ªôt h√†m AWS Lambda thay ƒë·ªïi k√≠ch th∆∞·ªõc h√¨nh ·∫£nh v√† l∆∞u tr·ªØ n√≥ trong m·ªôt S3 Bucket ƒë√≠ch. Gi·ªù ƒë√¢y t√¥i c√≥ th·ªÉ chuy·ªÉn ƒë·ªïi s∆° ƒë·ªì ki·∫øn tr√∫c th√†nh m√£ b·∫±ng Q Developer CLI.\nKi·∫øn tr√∫c cho m·ªôt ·ª©ng d·ª•ng thay ƒë·ªïi k√≠ch th∆∞·ªõc h√¨nh ·∫£nh\nTrong ·∫£nh ch·ª•p m√†n h√¨nh sau, t√¥i ƒë√£ y√™u c·∫ßu Q Developer CLI: ‚ÄúVui l√≤ng cung c·∫•p cho t√¥i m·ªôt m·∫´u terraform tham chi·∫øu s·ª≠ d·ª•ng c√°c ph∆∞∆°ng ph√°p hay nh·∫•t‚Äù. L∆∞u √Ω r·∫±ng vi·ªác k√©o v√† th·∫£ h√¨nh ·∫£nh v√†o CLI s·∫Ω th√™m ƒë∆∞·ªùng d·∫´n v√†o l·ªùi nh·∫Øc c·ªßa b·∫°n. CLI v·ªõi m√£ Terraform ƒë∆∞·ª£c t·∫°o b·ªüi Q Developer\nH√¨nh ·∫£nh tr√™n cho th·∫•y m·ªôt ph·∫ßn ph·∫£n h·ªìi m√† Q Developer CLI ƒë√£ t·∫°o ra.\nQ Developer ph·∫£n h·ªìi b·∫±ng m·∫´u terraform c·∫ßn thi·∫øt ƒë·ªÉ b·∫Øt ƒë·∫ßu x√¢y d·ª±ng ·ª©ng d·ª•ng thay ƒë·ªïi k√≠ch th∆∞·ªõc h√¨nh ·∫£nh. Q Developer CLI ƒë√£ ph√¢n t√≠ch h√¨nh ·∫£nh, x√°c ƒë·ªãnh c√°c th√†nh ph·∫ßn v√† m·ªëi quan h·ªá c·ªßa ch√∫ng, r·ªìi t·∫°o m√£ Terraform t∆∞∆°ng ·ª©ng. M·∫∑c d√π kh√¥ng hi·ªÉn th·ªã trong h√¨nh ·∫£nh, ph·∫£n h·ªìi bao g·ªìm m√£ c·ªßa h√†m Lambda b·∫±ng Python v√† quy·ªÅn IAM c·∫ßn thi·∫øt cho h√†m Lambda.\nTr∆∞·ªõc ƒë√¢y, vi·ªác chuy·ªÉn ƒë·ªïi s∆° ƒë·ªì n√†y th√†nh c∆° s·ªü h·∫° t·∫ßng d∆∞·ªõi d·∫°ng m√£ s·∫Ω y√™u c·∫ßu t√¥i ph·∫£i t·ª± gi·∫£i th√≠ch th·ªß c√¥ng t·ª´ng th√†nh ph·∫ßn v√† vi·∫øt c·∫•u h√¨nh t∆∞∆°ng ·ª©ng. V·ªõi h·ªó tr·ª£ h√¨nh ·∫£nh, gi·ªù ƒë√¢y t√¥i c√≥ th·ªÉ t·ª± ƒë·ªông h√≥a ph·∫ßn l·ªõn quy tr√¨nh n√†y v√† tinh ch·ªânh m√£ ƒë∆∞·ª£c t·∫°o th√¥ng qua m·ªôt cu·ªôc tr√≤ chuy·ªán v·ªõi Q Developer. Sau ƒë√≥, t√¥i c√≥ th·ªÉ tr√≤ chuy·ªán v·ªõi Q Developer ƒë·ªÉ tinh ch·ªânh m√£ ƒë√£ t·∫°o, ƒë·∫∑t c√¢u h·ªèi v·ªÅ c√°c chi ti·∫øt tri·ªÉn khai c·ª• th·ªÉ ho·∫∑c y√™u c·∫ßu s·ª≠a ƒë·ªïi d·ª±a tr√™n c√°c y√™u c·∫ßu b·ªï sung v√† xu·∫•t m√£ sang t·ªáp .tf.\nTr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng 2: Chuy·ªÉn ƒë·ªïi s∆° ƒë·ªì ER th√†nh l∆∞·ª£c ƒë·ªì c∆° s·ªü d·ªØ li·ªáu ƒê·ªëi v·ªõi t√¨nh hu·ªëng th·ª© hai, h√£y xem x√©t m·ªôt tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng trong ƒë√≥ t√¥i l√† m·ªôt ph·∫ßn c·ªßa nh√≥m m√¥ h√¨nh h√≥a d·ªØ li·ªáu ƒëang ph√°t tri·ªÉn ph·∫ßn m·ªÅm qu·∫£n l√Ω kh√≥a h·ªçc cho c√°c tr∆∞·ªùng ƒë·∫°i h·ªçc. T√¥i ƒë√£ t·∫°o m·ªôt s∆° ƒë·ªì th·ª±c th·ªÉ li√™n k·∫øt (ER) cho c√°c c·∫•u tr√∫c d·ªØ li·ªáu c·ªët l√µi c·ªßa h·ªç. Gi·ªù ƒë√¢y t√¥i c√≥ th·ªÉ s·ª≠ d·ª•ng Q Developer ƒë·ªÉ gi√∫p t√¥i chuy·ªÉn ƒë·ªïi s∆° ƒë·ªì ER th√†nh SQL.\nS∆° ƒë·ªì th·ª±c th·ªÉ li√™n k·∫øt cho h·ªá th·ªëng Qu·∫£n l√Ω Kh√≥a h·ªçc\nTrong ·∫£nh ch·ª•p m√†n h√¨nh sau, t√¥i ƒë√£ y√™u c·∫ßu Q Developer CLI s·ª≠ d·ª•ng s∆° ƒë·ªì ER ƒë·ªÉ t·∫°o l∆∞·ª£c ƒë·ªì c∆° s·ªü d·ªØ li·ªáu.\nCLI v·ªõi c√¢u l·ªánh c·ªßa ng∆∞·ªùi d√πng v√† m√£ SQL ƒë∆∞·ª£c t·∫°o b·ªüi Q Developer CLI v·ªõi m√£ SQL ƒë∆∞·ª£c t·∫°o b·ªüi Q Developer\nH√¨nh ·∫£nh tr√™n cho th·∫•y ph·∫£n h·ªìi m√† Q Developer CLI ƒë√£ t·∫°o ra.\nQ Developer ƒë√£ ph√¢n t√≠ch s∆° ƒë·ªì, x√°c ƒë·ªãnh c√°c th·ª±c th·ªÉ, thu·ªôc t√≠nh v√† m·ªëi quan h·ªá, sau ƒë√≥ t·∫°o m√£ SQL th√≠ch h·ª£p ƒë·ªÉ t·∫°o l∆∞·ª£c ƒë·ªì c∆° s·ªü d·ªØ li·ªáu.\nSau khi Q Developer ƒë∆∞a ra k·∫øt qu·∫£, t√¥i c√≥ th·ªÉ tinh ch·ªânh l∆∞·ª£c ƒë·ªì n√†y th√¥ng qua m·ªôt cu·ªôc tr√≤ chuy·ªán v·ªõi Q Developer b·∫±ng c√°ch y√™u c·∫ßu thay ƒë·ªïi ƒë·ªô d√†i chu·ªói, ch·ªâ m·ª•c, v.v., ho·∫∑c y√™u c·∫ßu gi·∫£i th√≠ch v·ªÅ c√°c quy·∫øt ƒë·ªãnh thi·∫øt k·∫ø.\nTr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng 3: Chuy·ªÉn ƒë·ªïi h√¨nh ·∫£nh v·∫Ω tay th√†nh t√†i li·ªáu thi·∫øt k·∫ø H√£y xem x√©t m·ªôt t√¨nh hu·ªëng trong ƒë√≥ t√¥i ƒë√£ ƒë·ªông n√£o ra m·ªôt √Ω t∆∞·ªüng tr√™n gi·∫•y v√† t√¥i mu·ªën chia s·∫ª √Ω t∆∞·ªüng n√†y v·ªõi nh√≥m c·ªßa m√¨nh. Trong h√¨nh ·∫£nh sau, t√¥i ƒë√£ v·∫Ω tay quy tr√¨nh ƒë·∫∑t h√†ng cho m·ªôt trang web. Khi ng∆∞·ªùi d√πng trang web ƒë·∫∑t mua s√°ch t·ª´ trang web, ·ª©ng d·ª•ng s·∫Ω c·∫≠p nh·∫≠t kho h√†ng, sau ƒë√≥ g·ªçi c√°c h√†nh ƒë·ªông thanh to√°n v√† giao h√†ng. Gi·ªù ƒë√¢y t√¥i c√≥ th·ªÉ s·ª≠ d·ª•ng Q Developer CLI ƒë·ªÉ ph√°c th·∫£o t√†i li·ªáu t·ª´ √Ω t∆∞·ªüng v·∫Ω tay.\nS∆° ƒë·ªì v·∫Ω tay c·ªßa lu·ªìng quy tr√¨nh ƒë·∫∑t h√†ng cho m·ªôt trang web\nTrong v√≠ d·ª• sau, t√¥i ƒë√£ y√™u c·∫ßu Q Developer vi·∫øt m·ªôt t√†i li·ªáu thi·∫øt k·∫ø b·∫±ng c√°ch s·ª≠ d·ª•ng h√¨nh ·∫£nh n√†y l√†m tham chi·∫øu. CLI v·ªõi c√¢u l·ªánh c·ªßa ng∆∞·ªùi d√πng v√† ph·∫£n h·ªìi ƒë∆∞·ª£c t·∫°o b·ªüi Q Developer\n·∫¢nh ch·ª•p m√†n h√¨nh tr√™n cho th·∫•y, Q Developer tr∆∞·ªõc ti√™n ƒë√£ ƒë·ªçc h√¨nh ·∫£nh v√† hi·ªÉu n·ªôi dung t·ª´ s∆° ƒë·ªì v·∫Ω tay. CLI v·ªõi ph·∫£n h·ªìi ƒë∆∞·ª£c t·∫°o b·ªüi Q Developer\n·∫¢nh ch·ª•p m√†n h√¨nh tr√™n l√† m·ªôt ph·∫ßn ph·∫£n h·ªìi m√† Q Developer CLI ƒë√£ t·∫°o ra.\nQ Developer ƒë√£ chuy·ªÉn ƒë·ªïi √Ω t∆∞·ªüng th√†nh m·ªôt t√†i li·ªáu thi·∫øt k·∫ø bao g·ªìm ki·∫øn tr√∫c h·ªá th·ªëng, lu·ªìng x·ª≠ l√Ω, m√¥ h√¨nh d·ªØ li·ªáu, c√°c y√™u c·∫ßu ch·ª©c nƒÉng, v√† c√°c y√™u c·∫ßu k·ªπ thu·∫≠t. T√¥i c≈©ng c√≥ th·ªÉ y√™u c·∫ßu Q Developer xu·∫•t to√†n b·ªô n·ªôi dung sang t·ªáp .md. ƒêi·ªÅu n√†y gi·∫£m l∆∞·ª£ng th·ªùi gian t·ª´ √Ω t∆∞·ªüng ƒë·∫øn th·ª±c thi v√† tinh gi·∫£n vi·ªác vi·∫øt t√†i li·ªáu.\nTr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng 4: X√¢y d·ª±ng b·∫£n m√¥ ph·ªèng UI/wireframe t·ª´ ·∫£nh ch·ª•p m√†n h√¨nh Gi·∫£ s·ª≠ t√¥i mu·ªën b·∫Øt ƒë·∫ßu x√¢y d·ª±ng Giao di·ªán Ng∆∞·ªùi d√πng (UI) t·ª´ t√†i li·ªáu thi·∫øt k·∫ø c·ªßa m√¨nh trong tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng 3. T√¥i c√≥ th·ªÉ cung c·∫•p m·ªôt h√¨nh ·∫£nh tham chi·∫øu cho Q Developer ƒë·ªÉ t·∫°o c√°c wireframe ban ƒë·∫ßu cho UI c·ªßa m√¨nh.\nTrang ch·ªß m·∫´u c·ªßa trang web b√°n s√°ch\nTrong v√≠ d·ª• n√†y, t√¥i ƒë√£ y√™u c·∫ßu Q Developer gi√∫p t·∫°o front-end cho m·ªôt trang web m·ªõi b·∫±ng Vue.js CLI v·ªõi c√¢u l·ªánh c·ªßa ng∆∞·ªùi d√πng v√† ph·∫£n h·ªìi ƒë∆∞·ª£c t·∫°o b·ªüi Q Developer CLI v·ªõi m√£ Vue.js ƒë∆∞·ª£c t·∫°o b·ªüi Q Developer\nH√¨nh ·∫£nh tr√™n cho th·∫•y m·ªôt ph·∫ßn m√£ Vue.js ƒë∆∞·ª£c t·∫°o b·ªüi Q Developer CLI ƒë·ªÉ t√°i t·∫°o front-end c·ªßa trang web trong ·∫£nh ch·ª•p m√†n h√¨nh. Sau khi t√¥i x√°c minh m√£, t√¥i c√≥ th·ªÉ y√™u c·∫ßu Q Developer CLI t·∫°o c√°c t·ªáp n√†y c·ª•c b·ªô.\nC√°ch ti·∫øp c·∫≠n n√†y gi·∫£m c√°c kh√≠a c·∫°nh d·ªÖ x·∫£y ra l·ªói c·ªßa vi·ªác t·∫°o wireframe, cho ph√©p t√¥i t·∫≠p trung v√†o c√°c quy·∫øt ƒë·ªãnh thi·∫øt k·∫ø s√°ng t·∫°o thay v√¨ c√°c t√°c v·ª• thi·∫øt l·∫≠p l·∫∑p ƒëi l·∫∑p l·∫°i. B·∫±ng c√°ch n√†y, t√¥i c√≥ th·ªÉ tƒÉng t·ªëc chu k·ª≥ ph√°t tri·ªÉn, ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n gi·ªØa c√°c th√†nh ph·∫ßn v√† cung c·∫•p m·ªôt n·ªÅn t·∫£ng c√≥ th·ªÉ d·ªÖ d√†ng t√πy ch·ªânh ƒë·ªÉ ƒë√°p ·ª©ng c√°c y√™u c·∫ßu d·ª± √°n c·ª• th·ªÉ.\nC√°c kh·∫£ nƒÉng b·ªï sung: Ngo√†i c√°c v√≠ d·ª• tr√™n, Q Developer CLI c√≥ th·ªÉ ph√¢n t√≠ch nhi·ªÅu lo·∫°i h√¨nh ·∫£nh, bao g·ªìm:\nS∆° ƒë·ªì lu·ªìng (Flow charts) v√† s∆° ƒë·ªì quy tr√¨nh S∆° ƒë·ªì l·ªõp (Class diagrams) cho thi·∫øt k·∫ø h∆∞·ªõng ƒë·ªëi t∆∞·ª£ng S∆° ƒë·ªì c·∫•u tr√∫c li√™n k·∫øt m·∫°ng (Network topology diagrams) ·∫¢nh ch·ª•p m√†n h√¨nh c·ªßa th√¥ng b√°o l·ªói ho·∫∑c tr·∫°ng th√°i ·ª©ng d·ª•ng T√≠nh linh ho·∫°t n√†y l√†m cho Q Developer CLI tr·ªü th√†nh m·ªôt c√¥ng c·ª• m·∫°nh m·∫Ω cho c√°c quy tr√¨nh l√†m vi·ªác ph√°t tri·ªÉn kh√°c nhau.\nK·∫øt lu·∫≠n:\nVi·ªác b·ªï sung h·ªó tr·ª£ h√¨nh ·∫£nh cho Amazon Q Developer CLI ƒë·∫°i di·ªán cho m·ªôt b∆∞·ªõc ti·∫øn ƒë√°ng k·ªÉ trong vi·ªác thu h·∫πp kho·∫£ng c√°ch gi·ªØa c√°c bi·ªÉu di·ªÖn tr·ª±c quan v√† vƒÉn b·∫£n trong ph√°t tri·ªÉn ph·∫ßn m·ªÅm. B·∫±ng c√°ch cho ph√©p t√¥i l√†m vi·ªác v·ªõi s∆° ƒë·ªì v√† c√°c t√†i s·∫£n tr·ª±c quan kh√°c tr·ª±c ti·∫øp t·ª´ d√≤ng l·ªánh, Amazon Q Developer c·∫£i thi·ªán hi·ªáu su·∫•t c·ªßa t√¥i trong vi·ªác chuy·ªÉn ƒë·ªïi thi·∫øt k·∫ø th√†nh tri·ªÉn khai, gi·∫£m l·ªói v√† tƒÉng t·ªëc c√°c chu k·ª≥ ph√°t tri·ªÉn. T√¥i khuy·∫øn kh√≠ch b·∫°n kh√°m ph√° kh·∫£ nƒÉng m·ªõi n√†y v√† t√¨m hi·ªÉu c√°ch n√≥ c√≥ th·ªÉ tƒÉng c∆∞·ªùng quy tr√¨nh l√†m vi·ªác ph√°t tri·ªÉn c·ªßa b·∫°n.\nƒê·ªÉ t√¨m hi·ªÉu th√™m v·ªÅ Q Developer v√† c√°c kh·∫£ nƒÉng c·ªßa n√≥, h√£y truy c·∫≠p t√†i li·ªáu.\nGi·ªõi thi·ªáu v·ªÅ T√°c gi·∫£: Keerthi Sreenivas Konjety\nKeerthi Sreenivas Konjety l√† Specialist Solutions Architect cho Amazon Q Developer, v·ªõi h∆°n 3,5 nƒÉm kinh nghi·ªám v·ªÅ AI, ML v√† K·ªπ thu·∫≠t D·ªØ li·ªáu. Chuy√™n m√¥n c·ªßa c√¥ l√† tƒÉng c∆∞·ªùng nƒÉng su·∫•t c·ªßa nh√† ph√°t tri·ªÉn cho kh√°ch h√†ng AWS. Ngo√†i c√¥ng vi·ªác, c√¥ y√™u th√≠ch nhi·∫øp ·∫£nh v√† s√°ng t·∫°o n·ªôi dung AI.\nTAGS: Developer Tools, Development\n"},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.10-cleanup/5.10.2-cdk-cleanup/","title":"CDK Cleanup","tags":[],"description":"","content":"Clean up (CDK) This guide ensures you correctly decommission all resources provisioned by the AWS CDK stack and clean up manually created data to avoid ongoing charges.\nPhase 1: Manual Data Cleanup (Before CDK Destroy) The CDK automatically deletes most resources failed in deleting content from S3 buckets. You must empty the contents of these buckets before running the cdk destroy command.\nResource Name Purpose Action Required incident-response-log-list-bucket Primary Log Source Empty Contents processed-cloudwatch-logs ETL Destination Empty Contents processed-guardduty-findings ETL Destination Empty Contents processed-cloudtrail-logs ETL Destination Empty Contents athena-query-results Athena Query Results Empty Contents aws-incident-response-automation-dashboard React Dashboard S3 Bucket Empty Contents Instructions for Emptying Buckets:\nOpen the Amazon S3 Console in your browser. For each of the buckets listed above (look for the names based on your AWS Account ID and Region): Click on the bucket name. Navigate to the \u0026ldquo;Objects\u0026rdquo; tab. Click the \u0026ldquo;Empty\u0026rdquo; button. Follow the prompts to confirm the permanent deletion of all objects. Phase 2: CDK Stack Destruction This step uses the CDK CLI to destroy all resources provisioned by the CloudFormation stack.\nEnsure Virtual Environment is Active\nIf you deactivated your Python environment, re-activate it (e.g., source .venv/bin/activate). Navigate to the Project Root\nEnsure you are in the main directory where the cdk.json file is located. Execute the Destroy Command\nRun the command to destroy all deployed stacks. When prompted, type y to approve the deletion. $ cdk destroy --all Phase 3: Post-Destruction Cleanup This step addresses remaining manual cleanup of lingering resources.\nDelete Remaining S3 Buckets\nThe cdk destroy command should remove the empty S3 buckets. If any remain (due to final checks or service protections), delete them now via the S3 Console. Disable Amazon GuardDuty\nGo to GuardDuty Console ‚Üí Settings ‚Üí General. Verify the service is disabled to ensure billing stops. Remove Cognito User and Pool\nGo to Cognito Console ‚Üí User pools. Delete the test user you created. Delete the User Pool created for the dashboard. Remove SES Identity\nGo to Amazon SES Console ‚Üí Verified Identities. Delete the sender email identity (sender_email) you verified. Deactivate Virtual Environment\nDeactivate the Python virtual environment: $ deactivate "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.5-processing-setup/5.5.2-create-aws-glue-database-and-tables/","title":"Create AWS Glue Database and Tables","tags":[],"description":"","content":"Create AWS Glue Database and Tables Create Database Open Glue Console ‚Üí Databases ‚Üí Add database\nDatabase name: security_logs\nCreate database\nCreate Tables (Using Athena DDL) Open Athena Console\nSet query result location: s3://athena-query-results-ACCOUNT_ID-REGION/\nSelect database: security_logs\nCreate processed_cloudtrail Table Run this DDL in Athena (replace ACCOUNT_ID and REGION):\nCREATE EXTERNAL TABLE IF NOT EXISTS security_logs.processed_cloudtrail ( `eventtime` string, `eventname` string, `eventsource` string, `awsregion` string, `sourceipaddress` string, `useragent` string, `useridentity` struct\u0026lt; type:string, invokedby:string, principalid:string, arn:string, accountid:string, accesskeyid:string, username:string, sessioncontext:struct\u0026lt; attributes:map\u0026lt;string,string\u0026gt;, sessionissuer:struct\u0026lt; type:string, principalid:string, arn:string, accountid:string, username:string \u0026gt; \u0026gt;, inscopeof:struct\u0026lt; issuertype:string, credentialsissuedto:string \u0026gt; \u0026gt;, `requestparameters` string, `responseelements` string, `resources` array\u0026lt;struct\u0026lt;arn:string,type:string\u0026gt;\u0026gt;, `recipientaccountid` string, `serviceeventdetails` string, `errorcode` string, `errormessage` string, `hour` string, `usertype` string, `username` string, `isconsolelogin` boolean, `isfailedlogin` boolean, `isrootuser` boolean, `isassumedrole` boolean, `ishighriskevent` boolean, `isprivilegedaction` boolean, `isdataaccess` boolean, `target_bucket` string, `target_key` string, `target_username` string, `target_rolename` string, `target_policyname` string, `new_access_key` string, `new_instance_id` string, `target_group_id` string, `identity_principalid` string ) PARTITIONED BY ( `date` string ) ROW FORMAT SERDE \u0026#39;org.openx.data.jsonserde.JsonSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;1\u0026#39; ) LOCATION \u0026#39;s3://processed-cloudtrail-logs-ACCOUNT_ID-REGION/processed-cloudtrail/\u0026#39; TBLPROPERTIES ( \u0026#39;projection.enabled\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;projection.date.type\u0026#39; = \u0026#39;date\u0026#39;, \u0026#39;projection.date.format\u0026#39; = \u0026#39;yyyy-MM-dd\u0026#39;, \u0026#39;projection.date.range\u0026#39; = \u0026#39;2025-01-01,NOW\u0026#39;, \u0026#39;projection.date.interval\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;projection.date.interval.unit\u0026#39; = \u0026#39;DAYS\u0026#39;, \u0026#39;storage.location.template\u0026#39; = \u0026#39;s3://processed-cloudtrail-logs-ACCOUNT_ID-REGION/processed-cloudtrail/date=${date}/\u0026#39;, \u0026#39;classification\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;compressionType\u0026#39; = \u0026#39;gzip\u0026#39; ); Create processed_guardduty Table Run this DDL in Athena:\nCREATE EXTERNAL TABLE IF NOT EXISTS security_logs.processed_guardduty ( `finding_id` string, `finding_type` string, `title` string, `severity` double, `account_id` string, `region` string, `created_at` string, `event_last_seen` string, `remote_ip` string, `remote_port` int, `connection_direction` string, `protocol` string, `dns_domain` string, `dns_protocol` string, `scanned_ip` string, `scanned_port` int, `aws_api_service` string, `aws_api_name` string, `aws_api_caller_type` string, `aws_api_error` string, `aws_api_remote_ip` string, `target_resource_arn` string, `instance_id` string, `instance_type` string, `image_id` string, `instance_tags` string, `resource_region` string, `access_key_id` string, `principal_id` string, `user_name` string, `s3_bucket_name` string, `service_raw` string, `resource_raw` string, `metadata_raw` string ) PARTITIONED BY ( `date` string ) ROW FORMAT SERDE \u0026#39;org.openx.data.jsonserde.JsonSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;1\u0026#39; ) LOCATION \u0026#39;s3://processed-guardduty-findings-ACCOUNT_ID-REGION/processed-guardduty/\u0026#39; TBLPROPERTIES ( \u0026#39;classification\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;compressionType\u0026#39; = \u0026#39;gzip\u0026#39;, \u0026#39;projection.enabled\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;projection.date.type\u0026#39; = \u0026#39;date\u0026#39;, \u0026#39;projection.date.range\u0026#39; = \u0026#39;2025-01-01,NOW\u0026#39;, \u0026#39;projection.date.format\u0026#39; = \u0026#39;yyyy-MM-dd\u0026#39;, \u0026#39;projection.date.interval\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;projection.date.interval.unit\u0026#39; = \u0026#39;DAYS\u0026#39;, \u0026#39;storage.location.template\u0026#39; = \u0026#39;s3://processed-guardduty-findings-ACCOUNT_ID-REGION/processed-guardduty/date=${date}/\u0026#39; ); Create vpc_logs Table Run this DDL in Athena:\nCREATE EXTERNAL TABLE IF NOT EXISTS security_logs.vpc_logs ( `version` string, `account_id` string, `region` string, `vpc_id` string, `query_timestamp` string, `query_name` string, `query_type` string, `query_class` string, `rcode` string, `answers` string, `srcaddr` string, `srcport` int, `transport` string, `srcids_instance` string, `timestamp` string ) PARTITIONED BY ( `date` string ) ROW FORMAT SERDE \u0026#39;org.openx.data.jsonserde.JsonSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;ignore.malformed.json\u0026#39; = \u0026#39;true\u0026#39; ) LOCATION \u0026#39;s3://processed-cloudwatch-logs-ACCOUNT_ID-REGION/vpc-logs/\u0026#39; TBLPROPERTIES ( \u0026#39;projection.enabled\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;projection.date.type\u0026#39; = \u0026#39;date\u0026#39;, \u0026#39;projection.date.format\u0026#39; = \u0026#39;yyyy-MM-dd\u0026#39;, \u0026#39;projection.date.range\u0026#39; = \u0026#39;2025-01-01,NOW\u0026#39;, \u0026#39;projection.date.interval\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;projection.date.interval.unit\u0026#39; = \u0026#39;DAYS\u0026#39;, \u0026#39;storage.location.template\u0026#39; = \u0026#39;s3://processed-cloudwatch-logs-ACCOUNT_ID-REGION/vpc-logs/date=${date}/\u0026#39;, \u0026#39;classification\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;compressionType\u0026#39; = \u0026#39;gzip\u0026#39; ); Create eni_flow_logs Table Run this DDL in Athena:\nCREATE EXTERNAL TABLE IF NOT EXISTS security_logs.eni_flow_logs ( `version` int, `account_id` string, `interface_id` string, `srcaddr` string, `dstaddr` string, `srcport` int, `dstport` int, `protocol` int, `packets` bigint, `bytes` bigint, `start_time` bigint, `end_time` bigint, `action` string, `log_status` string, `timestamp_str` string ) PARTITIONED BY ( `date` string ) ROW FORMAT SERDE \u0026#39;org.openx.data.jsonserde.JsonSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;1\u0026#39; ) LOCATION \u0026#39;s3://processed-cloudwatch-logs-ACCOUNT_ID-REGION/eni-flow-logs/\u0026#39; TBLPROPERTIES ( \u0026#39;projection.enabled\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;projection.date.type\u0026#39; = \u0026#39;date\u0026#39;, \u0026#39;projection.date.format\u0026#39; = \u0026#39;yyyy-MM-dd\u0026#39;, \u0026#39;projection.date.range\u0026#39; = \u0026#39;2025-01-01,NOW\u0026#39;, \u0026#39;projection.date.interval\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;projection.date.interval.unit\u0026#39; = \u0026#39;DAYS\u0026#39;, \u0026#39;storage.location.template\u0026#39; = \u0026#39;s3://processed-cloudwatch-logs-ACCOUNT_ID-REGION/eni-flow-logs/date=${date}/\u0026#39;, \u0026#39;classification\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;compressionType\u0026#39; = \u0026#39;gzip\u0026#39; ); "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.7-dashboard-setup/5.7.2-setup-lambda/","title":"Create IAM Roles and Policies","tags":[],"description":"","content":"In this section, you will create IAM role and Policy for Lambda. After that you will create Lambda Function to execute query\nContent Create Lambda Execution Roles and Policy Create Lambda Function "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.3-foundation-setup/5.3.3-create-iam-roles-and-policies/5.3.3.2-create-service-roles/","title":"Create Service Roles","tags":[],"description":"","content":"Create Firehose Roles Create CloudTrailFirehoseRole Open IAM Console ‚Üí Roles ‚Üí Create role\nSelect trusted entity:\nTrusted entity type: AWS service Use case: Select \u0026ldquo;Kinesis\u0026rdquo; ‚Üí \u0026ldquo;Kinesis Firehose\u0026rdquo; Click \u0026ldquo;Next\u0026rdquo; Add permissions:\nSkip adding managed policies (we\u0026rsquo;ll add inline policy) Click \u0026ldquo;Next\u0026rdquo; Name and create:\nRole name: CloudTrailFirehoseRole Description: Allows Firehose to write CloudTrail logs to S3 Click \u0026ldquo;Create role\u0026rdquo; Add inline policy:\nPolicy name: FirehosePolicy Policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::processed-cloudtrail-logs-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;arn:aws:s3:::processed-cloudtrail-logs-ACCOUNT_ID-REGION/*\u0026#34; ] } ] } Create CloudWatchFirehoseRole Role name: CloudWatchFirehoseRole Description: Allows Firehose to write CloudWatch logs to S3 Trusted entity: Kinesis Firehose Inline policy name: FirehosePolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::processed-cloudwatch-logs-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;arn:aws:s3:::processed-cloudwatch-logs-ACCOUNT_ID-REGION/*\u0026#34; ] } ] } Create Step Functions Role Create StepFunctionsRole Create role:\nTrusted entity: Step Functions Role name: StepFunctionsRole Description: Execution role for Incident Response Step Functions Add TWO inline policies:\nPolicy 1: LambdaInvokePolicy\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:lambda:REGION:ACCOUNT_ID:function:ir-isolate-ec2-lambda\u0026#34;, \u0026#34;arn:aws:lambda:REGION:ACCOUNT_ID:function:ir-parse-findings-lambda\u0026#34;, \u0026#34;arn:aws:lambda:REGION:ACCOUNT_ID:function:ir-quarantine-iam-lambda\u0026#34; ] } ] } Policy 2: EC2AutoScalingPolicy\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;autoscaling:DescribeAutoScalingInstances\u0026#34;, \u0026#34;autoscaling:DetachInstances\u0026#34;, \u0026#34;autoscaling:UpdateAutoScalingGroup\u0026#34;, \u0026#34;ec2:CreateSnapshot\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:DescribeVolumes\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Create EventBridge Role Create IncidentResponseStepFunctionsEventRole Role name: IncidentResponseStepFunctionsEventRole Description: Allows EventBridge to trigger Step Functions Trusted entity: EventBridge Inline policy name: StartStepFunctionsPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;states:StartExecution\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:REGION:ACCOUNT_ID:stateMachine:IncidentResponseStepFunctions\u0026#34; } ] } Create VPC Flow Logs Role Create FlowLogsIAMRole Create role:\nTrusted entity: EC2 (will edit trust policy) Role name: FlowLogsIAMRole Edit trust relationship to:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;vpc-flow-logs.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Add inline policy: Policy name: FlowLogsPolicy Policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Create Glue Role Create GlueCloudWatchRole Role name: GlueCloudWatchRole Description: Allows Glue to access S3 and CloudWatch Logs Trusted entity: Glue Managed policies (attach 3): AWSGlueServiceRole CloudWatchLogsReadOnlyAccess AmazonS3FullAccess No inline policies needed "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.2-guardduty-etl/","title":"GuardDuty ETL Code","tags":[],"description":"","content":" import json import boto3 import gzip import os from datetime import datetime from urllib.parse import unquote_plus s3_client = boto3.client(\u0026#39;s3\u0026#39;) DATABASE_NAME = os.environ.get(\u0026#34;DATABASE_NAME\u0026#34;, \u0026#34;security_logs\u0026#34;) TABLE_NAME_GUARDDUTY = os.environ.get(\u0026#34;TABLE_NAME_GUARDDUTY\u0026#34;, \u0026#34;processed_guardduty\u0026#34;) S3_LOCATION_GUARDDUTY = os.environ.get(\u0026#34;S3_LOCATION_GUARDDUTY\u0026#34;, \u0026#34;s3://vel-processed-guardduty/processed-guardduty/\u0026#34;) DESTINATION_BUCKET = os.environ.get(\u0026#34;DESTINATION_BUCKET\u0026#34;, \u0026#34;vel-processed-guardduty\u0026#34;) def promote_network_details(finding_service): if not finding_service: return {} action = finding_service.get(\u0026#39;action\u0026#39;, {}) net_conn_action = action.get(\u0026#39;networkConnectionAction\u0026#39;, {}) if net_conn_action: remote_ip = net_conn_action.get(\u0026#39;remoteIpDetails\u0026#39;, {}).get(\u0026#39;ipAddressV4\u0026#39;) or \\ net_conn_action.get(\u0026#39;remoteIpDetails\u0026#39;, {}).get(\u0026#39;ipAddressV6\u0026#39;) return { \u0026#39;remote_ip\u0026#39;: remote_ip, \u0026#39;remote_port\u0026#39;: net_conn_action.get(\u0026#39;remotePortDetails\u0026#39;, {}).get(\u0026#39;port\u0026#39;), \u0026#39;connection_direction\u0026#39;: net_conn_action.get(\u0026#39;connectionDirection\u0026#39;), \u0026#39;protocol\u0026#39;: net_conn_action.get(\u0026#39;protocol\u0026#39;), } dns_action = action.get(\u0026#39;dnsRequestAction\u0026#39;, {}) if dns_action: return {\u0026#39;dns_domain\u0026#39;: dns_action.get(\u0026#39;domain\u0026#39;), \u0026#39;dns_protocol\u0026#39;: dns_action.get(\u0026#39;protocol\u0026#39;)} port_probe_action = action.get(\u0026#39;portProbeAction\u0026#39;, {}) if port_probe_action and port_probe_action.get(\u0026#39;portProbeDetails\u0026#39;): detail = port_probe_action[\u0026#39;portProbeDetails\u0026#39;][0] return { \u0026#39;scanned_ip\u0026#39;: detail.get(\u0026#39;remoteIpDetails\u0026#39;, {}).get(\u0026#39;ipAddressV4\u0026#39;), \u0026#39;scanned_port\u0026#39;: detail.get(\u0026#39;localPortDetails\u0026#39;, {}).get(\u0026#39;port\u0026#39;), } return {} def promote_api_details(finding_service): if not finding_service: return {} action = finding_service.get(\u0026#39;action\u0026#39;, {}) aws_api_action = action.get(\u0026#39;awsApiCallAction\u0026#39;, {}) if aws_api_action: return { \u0026#39;aws_api_service\u0026#39;: aws_api_action.get(\u0026#39;serviceName\u0026#39;), \u0026#39;aws_api_name\u0026#39;: aws_api_action.get(\u0026#39;api\u0026#39;), \u0026#39;aws_api_caller_type\u0026#39;: aws_api_action.get(\u0026#39;callerType\u0026#39;), \u0026#39;aws_api_error\u0026#39;: aws_api_action.get(\u0026#39;errorCode\u0026#39;), \u0026#39;aws_api_remote_ip\u0026#39;: aws_api_action.get(\u0026#39;remoteIpDetails\u0026#39;, {}).get(\u0026#39;ipAddressV4\u0026#39;), } return {} def promote_resource_details(finding_resource): if not finding_resource: return {} instance_details = finding_resource.get(\u0026#39;instanceDetails\u0026#39;, {}) if instance_details: return { \u0026#39;target_resource_arn\u0026#39;: instance_details.get(\u0026#39;arn\u0026#39;), \u0026#39;instance_id\u0026#39;: instance_details.get(\u0026#39;instanceId\u0026#39;), \u0026#39;resource_region\u0026#39;: instance_details.get(\u0026#39;awsRegion\u0026#39;), \u0026#39;instance_type\u0026#39;: instance_details.get(\u0026#39;instanceType\u0026#39;), \u0026#39;image_id\u0026#39;: instance_details.get(\u0026#39;imageId\u0026#39;), \u0026#39;instance_tags\u0026#39;: instance_details.get(\u0026#39;tags\u0026#39;) } access_key_details = finding_resource.get(\u0026#39;accessKeyDetails\u0026#39;, {}) if access_key_details: return { \u0026#39;access_key_id\u0026#39;: access_key_details.get(\u0026#39;accessKeyId\u0026#39;), \u0026#39;principal_id\u0026#39;: access_key_details.get(\u0026#39;principalId\u0026#39;), \u0026#39;user_name\u0026#39;: access_key_details.get(\u0026#39;userName\u0026#39;), } s3_details = finding_resource.get(\u0026#39;s3BucketDetails\u0026#39;, []) if s3_details: return { \u0026#39;target_resource_arn\u0026#39;: s3_details[0].get(\u0026#39;arn\u0026#39;), \u0026#39;s3_bucket_name\u0026#39;: s3_details[0].get(\u0026#39;name\u0026#39;), } return {} def process_guardduty_log(bucket, key): response = s3_client.get_object(Bucket=bucket, Key=key) if key.endswith(\u0026#39;.gz\u0026#39;): content = gzip.decompress(response[\u0026#39;Body\u0026#39;].read()).decode(\u0026#39;utf-8\u0026#39;) else: content = response[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) processed_findings = [] for line in content.splitlines(): if not line: continue try: finding = json.loads(line) except json.JSONDecodeError: print(f\u0026#34;Skipping malformed JSON line in {key}\u0026#34;); continue finding_type = finding.get(\u0026#39;type\u0026#39;, \u0026#39;UNKNOWN\u0026#39;) finding_service = finding.get(\u0026#39;service\u0026#39;, {}) network_fields = promote_network_details(finding_service) api_fields = promote_api_details(finding_service) resource_fields = promote_resource_details(finding.get(\u0026#39;resource\u0026#39;, {})) created_at_str = finding.get(\u0026#39;createdAt\u0026#39;) event_last_seen_str = finding_service.get(\u0026#39;eventLastSeen\u0026#39;) dt_obj = datetime.now() if event_last_seen_str: try: dt_obj = datetime.strptime(event_last_seen_str, \u0026#39;%Y-%m-%dT%H:%M:%S.%fZ\u0026#39;) except ValueError: try: dt_obj = datetime.strptime(event_last_seen_str, \u0026#39;%Y-%m-%dT%H:%M:%SZ\u0026#39;) except ValueError: pass elif created_at_str: try: dt_obj = datetime.strptime(created_at_str, \u0026#39;%Y-%m-%dT%H:%M:%S.%fZ\u0026#39;) except ValueError: try: dt_obj = datetime.strptime(created_at_str, \u0026#39;%Y-%m-%dT%H:%M:%SZ\u0026#39;) except ValueError: pass processed_record = { \u0026#39;finding_id\u0026#39;: finding.get(\u0026#39;id\u0026#39;), \u0026#39;finding_type\u0026#39;: finding_type, \u0026#39;title\u0026#39;: finding.get(\u0026#39;title\u0026#39;), \u0026#39;severity\u0026#39;: finding.get(\u0026#39;severity\u0026#39;), \u0026#39;account_id\u0026#39;: finding.get(\u0026#39;accountId\u0026#39;), \u0026#39;region\u0026#39;: finding.get(\u0026#39;region\u0026#39;), \u0026#39;created_at\u0026#39;: created_at_str, \u0026#39;event_last_seen\u0026#39;: event_last_seen_str, **network_fields, **api_fields, **resource_fields, \u0026#39;date\u0026#39;: dt_obj.strftime(\u0026#39;%Y-%m-%d\u0026#39;), \u0026#39;service_raw\u0026#39;: json.dumps(finding_service), \u0026#39;resource_raw\u0026#39;: json.dumps(finding.get(\u0026#39;resource\u0026#39;, {})), \u0026#39;metadata_raw\u0026#39;: json.dumps(finding.get(\u0026#39;metadata\u0026#39;, {})), } processed_findings.append(processed_record) return processed_findings def save_processed_data(processed_events, source_key): if not processed_events: return first_event = processed_events[0] date_str = first_event.get(\u0026#39;date\u0026#39;, datetime.now().strftime(\u0026#39;%Y-%m-%d\u0026#39;)) original_filename = source_key.split(\u0026#39;/\u0026#39;)[-1].replace(\u0026#39;.gz\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;.json\u0026#39;, \u0026#39;\u0026#39;) output_key = f\u0026#34;processed-guardduty/date={date_str}/{original_filename}_processed.jsonl.gz\u0026#34; json_lines = \u0026#34;\u0026#34; for event in processed_events: event_to_dump = event.copy() json_lines += json.dumps(event_to_dump) + \u0026#34;\\n\u0026#34; compressed_data = gzip.compress(json_lines.encode(\u0026#39;utf-8\u0026#39;)) s3_client.put_object( Bucket=DESTINATION_BUCKET, Key=output_key, Body=compressed_data, ContentType=\u0026#39;application/jsonl\u0026#39;, ContentEncoding=\u0026#39;gzip\u0026#39; ) print(f\u0026#34;Saved processed data to: s3://{DESTINATION_BUCKET}/{output_key}\u0026#34;) def lambda_handler(event, context): for record in event[\u0026#39;Records\u0026#39;]: bucket = record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = unquote_plus(record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;]) print(f\u0026#34;Processing GuardDuty finding file: s3://{bucket}/{key}\u0026#34;) try: processed_findings = process_guardduty_log(bucket, key) save_processed_data(processed_findings, key) print(f\u0026#34;Successfully processed {len(processed_findings)} findings from {key}\u0026#34;) except Exception as e: print(f\u0026#34;Error processing {key}: {str(e)}\u0026#34;) raise e return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;GuardDuty findings processed successfully\u0026#39;) } "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.7-dashboard-setup/5.7.2-setup-lambda/5.7.2.2-create-lambda-function/","title":"Lambda setup","tags":[],"description":"","content":"In this guide, you will setup a Lambda using Python to execute query using Athena service.\nCreate Lambda Function Open the Lambda Console\nNavigate to https://console.aws.amazon.com/lambda/ Or: AWS Management Console ‚Üí Services ‚Üí Lambda Create Function:\nClick the Create Function In the create setting use the following setting: Choose Author from scratch Name: dashboard-query Runtime: Python 3.12 Architecture: x86_64 Change default execution role: Use an existing role Choose dashboard-query-role Click Create Add code:\nIn the code editor copy and paste the codes below then click Deply: import boto3 import time import os import json athena = boto3.client(\u0026#39;athena\u0026#39;) RESOURCE_MAP = { \u0026#39;/logs/cloudtrail\u0026#39;: { \u0026#39;db\u0026#39;: \u0026#39;security_logs\u0026#39;, \u0026#39;table\u0026#39;: \u0026#39;processed_cloudtrail\u0026#39; }, \u0026#39;/logs/guardduty\u0026#39;: { \u0026#39;db\u0026#39;: \u0026#39;security_logs\u0026#39;, \u0026#39;table\u0026#39;: \u0026#39;processed_guardduty\u0026#39; }, \u0026#39;/logs/vpc\u0026#39;: { \u0026#39;db\u0026#39;: \u0026#39;security_logs\u0026#39;, \u0026#39;table\u0026#39;: \u0026#39;vpc_logs\u0026#39; }, \u0026#39;/logs/eni_logs\u0026#39;:{ \u0026#39;db\u0026#39;: \u0026#39;security_logs\u0026#39;, \u0026#39;table\u0026#39;: \u0026#39;eni_flow_logs\u0026#39; } } OUTPUT_BUCKET_NAME = os.environ.get(\u0026#34;ATHENA_OUTPUT_BUCKET\u0026#34;) REGION = os.environ.get(\u0026#34;REGION\u0026#34;) OUTPUT_BUCKET = f\u0026#39;s3://{OUTPUT_BUCKET_NAME}/\u0026#39; def lambda_handler(event, context): print(\u0026#34;Received event:\u0026#34;, json.dumps(event)) resource_path = event.get(\u0026#39;resource\u0026#39;) config = RESOURCE_MAP.get(resource_path) if not config: return api_response(400, {\u0026#39;error\u0026#39;: f\u0026#39;Unknown resource path: {resource_path}\u0026#39;}) database_name = config[\u0026#39;db\u0026#39;] table_name = config[\u0026#39;table\u0026#39;] query_params = event.get(\u0026#39;queryStringParameters\u0026#39;, {}) or {} if config[\u0026#39;table\u0026#39;] == \u0026#39;processed_cloudtrail\u0026#39;: query_string = f\u0026#34;\u0026#34;\u0026#34;SELECT * FROM {table_name} where \u0026#34;date\u0026#34; \u0026gt;= cast((current_date - interval \u0026#39;3\u0026#39; day) as varchar) order by eventtime desc\u0026#34;\u0026#34;\u0026#34; elif config[\u0026#39;table\u0026#39;] == \u0026#39;processed_guardduty\u0026#39;: query_string = f\u0026#34;\u0026#34;\u0026#34;SELECT * FROM {table_name} where \u0026#34;date\u0026#34; \u0026gt;= cast((current_date - interval \u0026#39;3\u0026#39; day) as varchar) order by date desc\u0026#34;\u0026#34;\u0026#34; elif config[\u0026#39;table\u0026#39;] == \u0026#39;vpc_logs\u0026#39;: query_string = f\u0026#34;\u0026#34;\u0026#34;SELECT * FROM {table_name} where \u0026#34;date\u0026#34; \u0026gt;= cast((current_date - interval \u0026#39;3\u0026#39; day) as varchar) order by timestamp desc\u0026#34;\u0026#34;\u0026#34; elif config[\u0026#39;table\u0026#39;] == \u0026#39;eni_flow_logs\u0026#39;: query_string = f\u0026#34;\u0026#34;\u0026#34;SELECT * FROM {table_name} where \u0026#34;date\u0026#34; \u0026gt;= cast((current_date - interval \u0026#39;3\u0026#39; day) as varchar) order by timestamp_str desc\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Querying DB: {database_name}, Table: {table_name}, Output: {OUTPUT_BUCKET}\u0026#34;) try: response = athena.start_query_execution( QueryString=query_string, QueryExecutionContext={\u0026#39;Database\u0026#39;: database_name}, ResultConfiguration={\u0026#39;OutputLocation\u0026#39;: OUTPUT_BUCKET} ) query_execution_id = response[\u0026#39;QueryExecutionId\u0026#39;] status = \u0026#39;RUNNING\u0026#39; while status in [\u0026#39;RUNNING\u0026#39;, \u0026#39;QUEUED\u0026#39;]: response = athena.get_query_execution(QueryExecutionId=query_execution_id) status = response[\u0026#39;QueryExecution\u0026#39;][\u0026#39;Status\u0026#39;][\u0026#39;State\u0026#39;] if status in [\u0026#39;FAILED\u0026#39;, \u0026#39;CANCELLED\u0026#39;]: reason = response[\u0026#39;QueryExecution\u0026#39;][\u0026#39;Status\u0026#39;].get(\u0026#39;StateChangeReason\u0026#39;, \u0026#39;Unknown\u0026#39;) return api_response(500, {\u0026#39;error\u0026#39;: f\u0026#39;Query Failed: {reason}\u0026#39;}) time.sleep(1) results = athena.get_query_results(QueryExecutionId=query_execution_id) return api_response(200, results) except Exception as e: print(f\u0026#34;Error: {str(e)}\u0026#34;) return api_response(500, {\u0026#39;error\u0026#39;: str(e)}) def api_response(code, body): return { \u0026#34;statusCode\u0026#34;: code, \u0026#34;headers\u0026#34;: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Access-Control-Allow-Origin\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Access-Control-Allow-Methods\u0026#34;: \u0026#34;GET, OPTIONS\u0026#34; }, \u0026#34;body\u0026#34;: json.dumps(body) } "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.3-foundation-setup/5.3.2-set-up-s3-buckets-policies/","title":"Set up S3 buckets policies","tags":[],"description":"","content":"In this section, you will configure the bucket policy for the primary log bucket to allow CloudTrail, GuardDuty, and CloudWatch Logs to write logs.\nConfigure Bucket Policy Navigate to the primary log bucket: In S3 Console, click on incident-response-log-list-bucket-ACCOUNT_ID-REGION Open the Permissions tab:\nClick on the \u0026ldquo;Permissions\u0026rdquo; tab Scroll to Bucket policy:\nScroll down to the \u0026ldquo;Bucket policy\u0026rdquo; section Click \u0026ldquo;Edit\u0026rdquo; Paste the bucket policy: Copy the following JSON policy Important: Replace ACCOUNT_ID and REGION with your actual values in the policy { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowGuardDutyPutObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;guardduty.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:guardduty:REGION:ACCOUNT_ID:detector/*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowGuardDutyGetBucketLocation\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;guardduty.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:guardduty:REGION:ACCOUNT_ID:detector/*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudWatchLogsGetBucketAcl\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;logs.REGION.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:logs:REGION:ACCOUNT_ID:log-group:*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudWatchLogsPutObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;logs.REGION.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:logs:REGION:ACCOUNT_ID:log-group:*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudTrailAclCheck\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudtrail:REGION:ACCOUNT_ID:trail/*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudTrailWrite\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/AWSLogs/ACCOUNT_ID/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;s3:x-amz-acl\u0026#34;: \u0026#34;bucket-owner-full-control\u0026#34;, \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudtrail:REGION:ACCOUNT_ID:trail/*\u0026#34; } } } ] } Click \u0026ldquo;Save changes\u0026rdquo;\nVerify policy is saved: You should see the policy displayed in the Bucket policy section\n"},{"uri":"https://veljg.github.io/AWS-Worklog/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: ‚ÄúAWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS‚Äù Event Objectives Introduce AI/ML/GenAI on AWS Speakers Lam Tuan Kiet ‚Äì Sr DevOps Engineer, FPT Software Danh Hoang Hieu Nghi - AI Engineer, Renova Cloud Dinh Le Hoang Anh - Cloud Engineer Trainee, First Cloud AI Journey Van Hoang Kha - Cloud Security Engineer, AWS Community Builder Key Highlights Explored Generative AI with Amazon Bedrock: - Foundation Models: Different from Traditional Model in a sense that it can be adapted for many tasks, provided many fully managed model from leading AI Companies such as: OpenAI, Claude, Anthropic, etc.\n- Prompt Engineering: Crafting and Refining Instructions\nZero-Shot Prompting: A prompt with no prior context or example Few-shot Prompting: A prompt with a few prior context and example Chain of Thought: A prompt with thought processes and steps for the actual answer - Retrieval Augmented Generation(RAG): Retrieving relevant information from a data source R: Retrieval - Retrieves relevant information from a knowledge base or data sources A: Augmented - Adding the information retrieved as additional context in the user\u0026rsquo;s prompt before inputting it into the model G: Generation - Responses from the model for the augmented prompt Use cases: Improved content quality, contextual chatbots and question answering, personalized search and real time data summarization - Amazon Titan Embedding: Lightweight model that excels in translating text into numerical representations(embeddings) for high accuracy retrieval tasks, with support for 100+ languages\n- Pretrained AI Services:\nAmazon Rekognition: Image and Video analysis Amazon Translate: Detect and translate text Amazon Textract: Extract Texts and Layouts from documents Amazon Transcribe: Speech-to-text Amazon Polly: Text-to-speech Amazon Comprehend: Extract Insights and Relationships from text Amazon Kendra: Intelligent Search Service Amazon Lookout: Detect Anomalies in business metrics, equipment and images Amazon Personalize: Tailor recommendations to user - Demo: AMZPhoto: Face recognition from images using AI\n- Amazon Bedrock AgentCore: A comprehensive agentic platform designed to address challenges in bringing agents into production:\nSecurely execute and scale agent code. Incorporate memory (remembering past interactions and learning). Implement identity and access controls for agents and tools. Provide agentic tool use for complex workflows. Discover and connect with custom tools and resources. Understand and audit every interaction (observability). + Foundational Services: These services are categorized for running agents securely at scale. + Enhance with tools \u0026amp; memory: Includes Memory, Gateway, Browser tool, and Code Interpreter.\n+ Deploy securely at scale: Includes Runtime and Identity.\n+ Gain operational insights: Includes Observability.\n+ Enabling Agents at Scale (Architecture): Connects to the AgentCore Gateway (via MCP), Memory, Identity, Observability, Browser, and Code Interpreter.\n+ Frameworks for Building Agents: CrewAI, Google ADK, LangGraph/LangChain, LlamaIndex, OpenAI Agents SDK, and Strands Agents SDK.\nKey Takeaways Bedrock is the GenAI Hub: Amazon Bedrock provides fully managed Foundation Models from top companies for many different tasks.\nCustomization via Prompts and Data: Various ways to prompts (Zero-Shot, Few-shot, CoT) and utilize RAG to add info for better model responses.\nEmbeddings Power Search: Amazon Titan Embedding is a key lightweight model for translating text to numbers, which helps achieve high accuracy in retrieval tasks (like RAG).\nPretrained Models: AWS offers many ready-to-use AI services for common needs, like Rekognition for images and Textract for documents.\nAgentCore Solves Production Issues: Amazon Bedrock AgentCore is the new comprehensive platform that handles the difficult parts of running AI Agents at scale (like Memory, Identity, and Observability).\nApplying to Work Very useful in our team\u0026rsquo;s later projects which could include more usage of AI Foundation Models in our architecture. Event Experience The speakers are very well spoken and informative Q\u0026amp;A: Team member asked an out of topic question but crucial to our project Q: The SNS in our architecture which is used to process Guard Duty Findings have encountered a situation where over 1000+ alerts appear at once, how would we resolve this? A: Add SQS to queue the events and ensure no alert is to be missed Placed top 10 in the end of event Kahoot Quiz and got a picture with the speakers Created an unofficial group: \u0026ldquo;M√®o Cam ƒêeo KhƒÉn\u0026rdquo;, a joint collaboration between my group \u0026ldquo;The Ballers\u0026rdquo; and \u0026ldquo;Vinhomies\u0026rdquo; Some event photos Top 10 Kahoot Picture with top 10 players Meo Cam Dao Khan group picture\n"},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Complete Module 5 Help teammates with previous labs Redo the labs that are unavailable with free tiers Do 2 additional research Discuss project idea Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Lab 25: Cannot be done for now, account is on free tier - Upgraded account to paid tier - Retry lab 25: + The given template used runtime nodejs12.x for Lambda functions, which is no longer supported, fixed it by changing it to nodejs20.x + Created FSx file system + The S3 bucket endpoint for testing data is only reachable in the US region, so it must be changed to Read-S3Object -BucketName nasanex -KeyPrefix /AVHRR -Folder Z:/nasanex/AVHRR -Region us-west-2 + Created file shares + Created HDD and SSD FSx + 25.4: The given tool version is outdated, downloaded latest version + Successfully tested drive performance with various parameters + Monitored performance using CloudWatch: Alarm got triggered, throughput was maxed at 400MB + Learnt how to deduplicate files:\n‚Ä¢ Default dedup schedule is every Saturday ‚Ä¢ Initial dedup run optimized nothing due to the default fileAge -\u0026gt; changed to 0 =\u0026gt; Optimized half of the files + Created shadow copies for backup + Learnt how to manage open files and how to close them from the connection + Successfully created user quotas to manage storage space + Enabled Continuously Available (CA) file share on Amazon FSx to be used by multiple users at the same time + Scaled throughput and storage on AWS Console - Learnt shared responsibility model: Both the provider and the customer have responsibility in security - Module 5-2: Best practice is to create an admin IAM user rather than using root account - IAM Principal: Access resources in AWS Account - IAM Policy: Identity based and Resource based - IAM Role: A set of rules that control access to resources and services for IAM User - IAM Role can be used to enable cross account - School subject: + ENW493c: Completed Understanding Research Methods 22/09/2025 22/09/2025 Lab25 Understanding Research Methods 3 - Module 5: - Amazon Cognito: An authentication, permission and user management service, with two main features: + User pool: A collection of user accounts and authentication information, allowing for 3rd party authentication services + Identity pools: A mapping of permissions and credentials that can be applied to users - AWS Organization: Manage many AWS Accounts and resources + Organizes accounts by OU and use Service Control Policies to define permissions for users on the organization AWS Identity Center(SSO): Manage AWS Authorizations and Applications: + Utilize permission sets - AWS KMS: Create and manage encryption keys: + CMK (Customer Managed Key) is the main resource, used to create, encrypt and decrypt Data Key - AWS Security Hub: Scan and test security-based policies and best practices - Continue with lab 14: + Created role and S3 Bucket + The latest Ubuntu version (25.04) included unsupported kernel version, required reinstallation to proceed + Installing Ubuntu 24.04: Failed, its kernel is still unsupported + Installing Ubuntu 22.04 + Successfully imported VM to AWS + Successfully connected to EC2 instance created from the AMI using VM\u0026rsquo;s username and password 23/09/2025 23/09/2025 Lab 14 4 Continue with lab 14: + Created export bucket, configured permission + Successfully exported instances into .OVA format for usage - Lab 18: Enabled Security Hub and configured AWS Config to record data for analyzing (It can take a long time for a score to be calculated) 24/09/2025 24/09/2025 Lab 14 5 - Lab 22: + Created Lambda functions for running and stopping EC2 instances based on schedules and tags + Logged notification via Slack - Lab 28:\n+ Created IAM policies and role, only allowing access from Singapore Region(ap-southeast-1) + Restricted access to EC2 from regions outside of policy + Restricted creation of EC2 instances without valid tags Lab 30: Restricted IAM user to only use the specified region to access EC2 - Lab 18 (Update): Security finished scanning, got a security score of 85%, 1 critical exposure: IAM User has administrative access policy - Lab 33: + Created Key Management Service + Set up CloudTrail to log data in S3 Bucket + Created Athena to query logs + KMS successfully denied access to users without authorization 25/09/2025 25/09/2025 Lab 22 Lab 28 Lab 30 Lab 18 Lab 33 6 - Lab 44: Configured role conditions, restricting access by IP, time and others - Lab 48:\n+ Used IAM access key to upload file to S3 via EC2 Instance + Uploaded file to S3 via EC2 Instance without access key by using IAM Roles - Lab 12: + Created AWS Organization + Created accounts and moved them into units + Invited accounts to organization + Switched roles for accounts under the organization + Set up policies for the accounts under the organization + Installed Python to continue with the lab + Created and configured users and groups using Identity Store APIs via AWS CLI Labs from AWS for Microsoft Workloads: + Managed user and group on Microsoft AD via AWS CLI + Learnt how to troubleshoot EC2 instances by detaching the volumes of the erroneous instance and attach it to another running instance to configure and fix the problems + Learnt how to attach licenses to EC2 instances with Microsoft AD, demo with LibreOffice 26/09/2025 26/09/2025 Lab 44 Lab 48 Lab 12 Microsoft Workloads Week 3 Achievements: Successfully upgraded the AWS account to complete labs previously unavailable on the Free Tier and learned to adapt resources, such as fixing outdated Lambda runtime versions. Advanced Storage (FSx): Completed Lab 25, creating and configuring an Amazon FSx file system. Gained practical skills in managing data deduplication, creating shadow copies for backup, setting user quotas, and scaling throughput while monitoring performance with CloudWatch.\nCompleted Module 5 theory and extensive security labs:\nLearnt concepts of IAM, Roles, Policies, Cognito, Organizations, Identity Center (SSO), and KMS.\nImplemented Region Restriction policies (Lab 28 \u0026amp; 30) for EC2 creation and access.\nConfigured Role Conditions to restrict access based on IP and time (Lab 44).\nPracticed securing file uploads to S3 using IAM Roles instead of access keys (Lab 48).\nEnabled Security Hub and AWS Config (Lab 18), achieving a security score of 85% and identifying a critical exposure (IAM User administrative access).\nCreated Lambda functions to schedule the start and stop of EC2 instances based on tags (Lab 22) and logged notifications via Slack.\nSuccessfully navigated kernel compatibility issues by selecting the correct Ubuntu version (22.04), imported a VM to AWS, created an AMI, and exported the instance back into the .OVA format (Lab 14).\nSet up an AWS Organization, created Organizational Units (OUs) and accounts, configured Service Control Policies, and practiced switching roles for accounts (Lab 12).\nAWS for Microsoft Workloads: Completed additional labs focused on:\nManaging users/groups in Microsoft AD via AWS CLI.\nAdvanced EC2 troubleshooting by detaching and re-attaching volumes.\nAttaching licenses to EC2 instances using Microsoft AD (demonstrated with Libre Office).\nSet up CloudTrail to log data to S3 and used Amazon Athena to query these logs for audit and security analysis (Lab 33).\n"},{"uri":"https://veljg.github.io/AWS-Worklog/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Blog AWS Cloud Operations M√¥ ph·ªèng l·ªói c·ª•c b·ªô v·ªõi AWS Fault Injection Service b·ªüi Ozgur Canibeyaz v√† Pablo Colazurdo v√†o ng√†y 30 TH√ÅNG 6 2025 trong AWS Fault Injection Service (FIS), AWS Resilience Hub (ARH), AWS Systems Manager, Management Tools, Resilience, Technical How-to | Permalink | Chia s·∫ª\nC√°c h·ªá th·ªëng ph√¢n t√°n hi·ªán ƒë·∫°i ph·∫£i c√≥ kh·∫£ nƒÉng ch·ªëng ch·ªãu l·ªói tr∆∞·ªõc c√°c gi√°n ƒëo·∫°n kh√¥ng mong mu·ªën ƒë·ªÉ duy tr√¨ t√≠nh kh·∫£ d·ª•ng, hi·ªáu su·∫•t v√† ƒë·ªô ·ªïn ƒë·ªãnh. Chaos engineering gi√∫p c√°c nh√≥m ph√°t hi·ªán nh·ªØng ƒëi·ªÉm y·∫øu ti·ªÅm ·∫©n b·∫±ng c√°ch th·ª±c hi·ªán fault injection l√™n h·ªá th·ªëng v√† quan s√°t c√°ch n√≥ ph·ª•c h·ªìi. Trong khi th·ª≠ nghi·ªám truy·ªÅn th·ªëng x√°c th·ª±c h√†nh vi mong ƒë·ª£i, chaos engineering ki·ªÉm tra kh·∫£ nƒÉng ph·ª•c h·ªìi c·ªßa h·ªá th·ªëng trong su·ªët th·ªùi gian x·∫£y ra l·ªói. AWS Fault Injection Service (AWS FIS) l√† m·ªôt d·ªãch v·ª• AWS ƒë∆∞·ª£c qu·∫£n l√Ω to√†n di·ªán gi√∫p c√°c nh√≥m ch·∫°y c√°c th√≠ nghi·ªám fault injection tr√™n c√°c workloads c·ªßa AWS. N√≥ h·ªó tr·ª£ c√°c t√¨nh hu·ªëng nh∆∞ ch·∫•m d·ª©t Amazon EC2 instances, ƒëi·ªÅu ti·∫øt c√°c Amazon API Gateway requests, v√† t·∫°o ƒë·ªô tr·ªÖ m·∫°ng. ƒêi·ªÅu n√†y cho ph√©p b·∫°n x√°c th·ª±c kh·∫£ nƒÉng ph·ª•c h·ªìi trong c√°c m√¥i tr∆∞·ªùng gi·ªëng nh∆∞ m√¥i tr∆∞·ªùng s·∫£n xu·∫•t. M·∫∑c d√π nh·ªØng kh·∫£ nƒÉng n√†y r·∫•t m·∫°nh m·∫Ω, nhi·ªÅu l·ªói th·ª±c t·∫ø ch·ªâ ·∫£nh h∆∞·ªüng ƒë·∫øn m·ªôt ph·∫ßn l∆∞u l∆∞·ª£ng truy c·∫≠p.\nTrong b√†i ƒëƒÉng n√†y, b·∫°n s·∫Ω t√¨m hi·ªÉu c√°ch m√¥ ph·ªèng l·ªói c·ª•c b·ªô. M·ªôt ki·ªÉu l·ªói ph·ªï bi·∫øn nh∆∞ng √≠t ƒë∆∞·ª£c ki·ªÉm tra‚Äîb·∫±ng c√°ch k·∫øt h·ª£p AWS FIS v·ªõi weighted routing trong Application Load Balancer (ALB) v√† m·ªôt h√†m AWS Lambda tr·∫£ v·ªÅ c√°c ph·∫£n h·ªìi l·ªói t√πy ch·ªânh. Ph∆∞∆°ng ph√°p n√†y cho ph√©p b·∫°n ki·ªÉm tra c√°ch ·ª©ng d·ª•ng c·ªßa b·∫°n x·ª≠ l√Ω c√°c ƒëi·ªÅu ki·ªán suy gi·∫£m m√† kh√¥ng c·∫ßn thay ƒë·ªïi m√£ ho·∫∑c l√†m gi√°n ƒëo·∫°n lu·ªìng truy c·∫≠p th√¥ng th∆∞·ªùng.\nT·ªïng quan v·ªÅ gi·∫£i ph√°p Gi·∫£i ph√°p c·ªßa ch√∫ng t√¥i k·∫øt h·ª£p AWS FIS v·ªõi ƒë·ªãnh tuy·∫øn c√≥ tr·ªçng s·ªë c·ªßa ALB ƒë·ªÉ ƒëi·ªÅu h∆∞·ªõng m·ªôt t·ª∑ l·ªá ph·∫ßn trƒÉm l∆∞u l∆∞·ª£ng truy c·∫≠p c√≥ th·ªÉ ƒë·ªãnh c·∫•u h√¨nh ƒë·∫øn m·ªôt h√†m Lambda c√≥ th·ªÉ tr·∫£ v·ªÅ c√°c l·ªói m√¥ ph·ªèng. C√°ch ti·∫øp c·∫≠n n√†y kh√¥ng y√™u c·∫ßu thay ƒë·ªïi m√£ ngu·ªìn ·ª©ng d·ª•ng v√† s·∫Ω t·ª± ƒë·ªông kh√¥i ph·ª•c v·ªÅ ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng sau khi ki·ªÉm th·ª≠. H√¨nh 1 Gi·∫£i ph√°p n√†y cho th·∫•y c√°ch s·ª≠a ƒë·ªïi Load Balancer c·ªßa b·∫°n m·ªôt c√°ch an to√†n ƒë·ªÉ m√¥ ph·ªèng l·ªói trong qu√° tr√¨nh th·ª±c hi·ªán th·ª≠ nghi·ªám v√† kh√¥i ph·ª•c an to√†n sau khi k·∫øt th√∫c\nL·ª£i √≠ch ch√≠nh Gi·∫£i ph√°p n√†y cung c·∫•p c√°c l·ª£i √≠ch ch√≠nh sau cho c√°c nh√≥m tri·ªÉn khai k·ªπ thu·∫≠t h·ªón lo·∫°n:\nM√¥ ph·ªèng l·ªói c√≥ ki·ªÉm so√°t. Kh√¥ng c·∫ßn s·ª≠a ƒë·ªïi ·ª©ng d·ª•ng. Thi·∫øt l·∫≠p v√† kh√¥i ph·ª•c t·ª± ƒë·ªông. T·ª∑ l·ªá l·ªói c√≥ th·ªÉ ƒë·ªãnh c·∫•u h√¨nh. H∆∞·ªõng d·∫´n tri·ªÉn khai ƒêi·ªÅu ki·ªán ti√™n quy·∫øt Tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu, h√£y x√°c minh b·∫°n c√≥:\nM·ªôt t√†i kho·∫£n AWS v·ªõi quy·ªÅn tri·ªÉn khai c√°c stack AWS CloudFormation v√† qu·∫£n l√Ω c√°c th√≠ nghi·ªám AWS FIS. M·ªôt ALB hi·ªán c√≥ ƒë∆∞·ª£c c·∫•u h√¨nh v·ªõi m·ªôt target group ƒë·ªãnh tuy·∫øn l∆∞u l∆∞·ª£ng truy c·∫≠p ƒë·∫øn m·ªôt microservice ƒëang ch·∫°y. ALB ph·∫£i ƒë√£ ho·∫°t ƒë·ªông v√† c√≥ th·ªÉ truy c·∫≠p c√¥ng khai ƒë·ªÉ ki·ªÉm tra c√°c l·ªói m√¥ ph·ªèng. Truy c·∫≠p v√†o AWS Command Line Interface(AWS CLI) ho·∫∑c AWS Management Console. B∆∞·ªõc 1: Tri·ªÉn khai m·∫´u CloudFormation M·∫´u CloudFormation thi·∫øt l·∫≠p t·∫•t c·∫£ c√°c t√†i nguy√™n c·∫ßn thi·∫øt, bao g·ªìm:\nM·ªôt h√†m Lambda ƒë·ªÉ m√¥ ph·ªèng c√°c ph·∫£n h·ªìi l·ªói. M·ªôt t√†i li·ªáu AWS Systems Manager (SSM) automation. M·ªôt IAM Role c·∫•p quy·ªÅn cho AWS FIS ƒë·ªÉ g·ªçi t√†i li·ªáu SSM Automation. M·ªôt AWS FIS experiment template ƒë∆∞·ª£c c·∫•u h√¨nh s·∫µn. H√¨nh 2 C√°i nh√¨n c·∫•p cao v·ªÅ c√°c th√†nh ph·∫ßn gi·∫£i ph√°p v√† s·ª± t∆∞∆°ng t√°c c·ªßa ch√∫ng.\nC√°c tham s·ªë th√≠ nghi·ªám c√≥ th·ªÉ ƒë·ªãnh c·∫•u h√¨nh M·∫´u CloudFormation y√™u c·∫ßu ba tham s·ªë sau khi tri·ªÉn khai:\nT√™n Application Load Balancer. ARN c·ªßa Listener ALB rule c·∫ßn s·ª≠a ƒë·ªïi. Th·ªùi l∆∞·ª£ng ki·ªÉm tra b·∫±ng gi√¢y ‚Äî l·ªói c·ª•c b·ªô n√™n k√©o d√†i bao l√¢u. C√°c c√†i ƒë·∫∑t th√≠ nghi·ªám kh√°c, ch·∫≥ng h·∫°n nh∆∞ t·ª∑ l·ªá ph·∫ßn trƒÉm l∆∞u l∆∞·ª£ng truy c·∫≠p c·∫ßn chuy·ªÉn h∆∞·ªõng v√† m√£ ph·∫£n h·ªìi Lambda, ƒë∆∞·ª£c c·∫•u h√¨nh s·∫µn trong ƒë·ªãnh nghƒ©a th√≠ nghi·ªám. N·∫øu b·∫°n mu·ªën t√πy ch·ªânh c√°c gi√° tr·ªã n√†y, b·∫°n c√≥ hai t√πy ch·ªçn:\nT√πy ch·ªçn 1: S·ª≠a ƒë·ªïi M·∫´u CloudFormation v√† tri·ªÉn khai l·∫°i\nB·∫°n c√≥ th·ªÉ ch·ªânh s·ª≠a tr∆∞·ªùng documentParameters trong ph·∫ßn ƒë·ªãnh nghƒ©a th√≠ nghi·ªám c·ªßa m·∫´u ƒë·ªÉ thay ƒë·ªïi:\nFailurePercentage (v√≠ d·ª•: 10, 50, 100). ƒê·ªÉ thay ƒë·ªïi HTTP status code ƒë∆∞·ª£c h√†m Lambda tr·∫£ v·ªÅ (v√≠ d·ª•: t·ª´ 500 th√†nh 404), h√£y s·ª≠a ƒë·ªïi gi√° tr·ªã statusCode tr·ª±c ti·∫øp trong kh·ªëi m√£ n·ªôi tuy·∫øn b√™n trong m·∫´u.\nSau khi ch·ªânh s·ª≠a, h√£y tri·ªÉn khai l·∫°i stack ƒë·ªÉ √°p d·ª•ng c√°c thay ƒë·ªïi c·ªßa b·∫°n.\nT√πy ch·ªçn 2: T·∫°o phi√™n b·∫£n m·ªõi c·ªßa t√†i li·ªáu SSM Automation\nN·∫øu b·∫°n kh√¥ng mu·ªën tri·ªÉn khai l·∫°i stack:\nTruy c·∫≠p v√†o AWS Systems Manager ‚Üí Documents console. ƒê·ªãnh v·ªã t√†i li·ªáu SSM ƒë∆∞·ª£c t·∫°o b·ªüi m·∫´u. Ch·ªçn Create new version v√† ƒëi·ªÅu ch·ªânh c√°c gi√° tr·ªã m·∫∑c ƒë·ªãnh nh∆∞ FailurePercentage. S·ª≠ d·ª•ng phi√™n b·∫£n ƒë√£ c·∫≠p nh·∫≠t b·∫±ng c√°ch tham chi·∫øu n√≥ trong m·ªôt th√≠ nghi·ªám AWS FIS m·ªõi (th√¥ng qua CLI ho·∫∑c console). IAM Permissions:\nB·∫°n c·∫ßn c√≥ quy·ªÅn t·∫°o c√°c IAM role v√† policy khi tri·ªÉn khai CloudFormation template. Khi tri·ªÉn khai th√¥ng qua AWS Management Console, b·∫°n s·∫Ω c·∫ßn x√°c nh·∫≠n r·∫±ng template t·∫°o ra c√°c t√†i nguy√™n IAM. N·∫øu s·ª≠ d·ª•ng AWS CLI, h√£y th√™m flag --capabilities CAPABILITY_NAMED_IAM.\nT·∫£i xu·ªëng template: B·∫°n c√≥ th·ªÉ t·∫£i xu·ªëng CloudFormation template t·∫°i ƒë√¢y v√† l∆∞u c·ª•c b·ªô d∆∞·ªõi d·∫°ng fis_template.yaml tr∆∞·ªõc khi tri·ªÉn khai n√≥ th√¥ng qua AWS Console ho·∫∑c CLI.\naws cloudformation create-stack --stack-name alb-fis-experiment \\ --template-body file://fis_template.yaml \\ --parameters \\ ParameterKey=LoadBalancerName,ParameterValue=LoadBalancerName \\ ParameterKey=ListenerRuleArn,ParameterValue=RuleARN \\ ParameterKey=TestDurationInSeconds,ParameterValue=60 \\ --capabilities CAPABILITY_NAMED_IAM LoadBalancerName v√† RuleARN ƒë·ªÅ c·∫≠p ƒë·∫øn t√™n Load Balancer v√† ARN ƒë·∫ßy ƒë·ªß c·ªßa Listener rule tr∆∞·ªõc d·ªãch v·ª• m√† b·∫°n mu·ªën m√¥ ph·ªèng l·ªói. 60 ch·ªâ ƒë·ªãnh th·ªùi l∆∞·ª£ng c·ªßa l·ªói m√¥ ph·ªèng b·∫±ng gi√¢y.\nL∆∞u √Ω: FISExperimentRole IAM policy s·ª≠ d·ª•ng \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; cho c√°c h√†nh ƒë·ªông nh·∫•t ƒë·ªãnh ƒë·ªÉ cho ph√©p AWS FIS s·ª≠a ƒë·ªïi c√°c t√†i nguy√™n Load Balancer ƒë∆∞·ª£c t·∫°o ƒë·ªông. B·ªüi v√¨ t√™n t√†i nguy√™n nh∆∞ ARN c·ªßa target group kh√¥ng ƒë∆∞·ª£c bi·∫øt t·∫°i th·ªùi ƒëi·ªÉm tri·ªÉn khai, n√™n vi·ªác gi·ªõi h·∫°n ph·∫°m vi c√°c quy·ªÅn n√†y l√† kh√¥ng kh·∫£ thi trong ng·ªØ c·∫£nh c·ªßa b√†i ƒëƒÉng n√†y. M·∫∑c d√π ƒëi·ªÅu n√†y mang l·∫°i s·ª± linh ho·∫°t, AWS security best practices khuy·∫øn ngh·ªã gi·ªõi h·∫°n ph·∫°m vi quy·ªÅn ƒë·ªëi v·ªõi c√°c t√†i nguy√™n c·ª• th·ªÉ b·∫•t c·ª© khi n√†o c√≥ th·ªÉ. N·∫øu b·∫°n bi·∫øt ch√≠nh x√°c c√°c t√†i nguy√™n s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng, h√£y c√¢n nh·∫Øc c·∫≠p nh·∫≠t ch√≠nh s√°ch ƒë·ªÉ h·∫°n ch·∫ø quy·ªÅn truy c·∫≠p cho ph√π h·ª£p.\nB∆∞·ªõc 2: X√°c minh H√†m Lambda Sau khi tri·ªÉn khai, h√£y ki·ªÉm tra h√†m Lambda trong AWS console ƒë·ªÉ x√°c nh·∫≠n n√≥ tr·∫£ v·ªÅ ph·∫£n h·ªìi l·ªói mong ƒë·ª£i. H√†m ph·∫£i tr·∫£ v·ªÅ n·ªôi dung nh∆∞ sau:\n{ \u0026#34;statusCode\u0026#34;: 503, \u0026#34;body\u0026#34;: \u0026#34;Service Unavailable - Simulated Error Response\u0026#34; } B∆∞·ªõc 3: B·∫Øt ƒë·∫ßu Th√≠ nghi·ªám AWS FIS M·ªü AWS Fault Injection Service console. ƒê·ªãnh v·ªã template ƒë∆∞·ª£c c·∫•u h√¨nh s·∫µn trong Experiment Templates. Ch·ªçn Start experiment. X√°c nh·∫≠n v√† kh·ªüi ch·∫°y ki·ªÉm tra. H√¨nh 3 AWS FIS console hi·ªÉn th·ªã experiment template t√πy ch·ªânh ƒë∆∞·ª£c t·∫°o b·ªüi m·∫´u CloudFormation.\nKhi b·∫°n b·∫Øt ƒë·∫ßu th√≠ nghi·ªám, AWS FIS s·∫Ω g·ªçi m·ªôt AWS Systems Manager Automation Document ƒë∆∞·ª£c t·∫°o trong qu√° tr√¨nh tri·ªÉn khai. Automation n√†y th·ª±c hi·ªán c√°c h√†nh ƒë·ªông sau:\nT·∫°o m·ªôt ALB target group m·ªõi tr·ªè ƒë·∫øn m·ªôt h√†m Lambda ƒë∆∞·ª£c c·∫•u h√¨nh ƒë·ªÉ tr·∫£ v·ªÅ c√°c error responses m√¥ ph·ªèng. S·ª≠a ƒë·ªïi m·ªôt ALB Listener rule ƒë·ªÉ chia m·ªôt ph·∫ßn l∆∞u l∆∞·ª£ng truy c·∫≠p ƒë·∫øn target group m·ªõi n√†y, m√¥ ph·ªèng hi·ªáu qu·∫£ m·ªôt l·ªói c·ª•c b·ªô. ƒê·ª£i trong m·ªôt kho·∫£ng th·ªùi gian x√°c ƒë·ªãnh (c√≥ th·ªÉ ƒë·ªãnh c·∫•u h√¨nh th√¥ng qua CloudFormation template). Kh√¥i ph·ª•c ALB listener rule v·ªÅ tr·∫°ng th√°i ban ƒë·∫ßu v√† x√≥a target group t·∫°m th·ªùi. To√†n b·ªô v√≤ng ƒë·ªùi n√†y ƒë∆∞·ª£c t·ª± ƒë·ªông h√≥a ‚Äî b·∫°n kh√¥ng c·∫ßn vi·∫øt b·∫•t k·ª≥ m√£ n√†o ho·∫∑c th·ª±c hi·ªán c·∫≠p nh·∫≠t th·ªß c√¥ng n√†o cho Load Balancer c·ªßa m√¨nh. T·∫•t c·∫£ nh·ªØng g√¨ b·∫°n l√†m l√† b·∫Øt ƒë·∫ßu th√≠ nghi·ªám t·ª´ FIS console v√† quan s√°t c√°ch d·ªãch v·ª• c·ªßa b·∫°n ph·∫£n h·ªìi v·ªõi m·ªôt tr∆∞·ªùng h·ª£p l·ªói c·ª•c b·ªô ƒë∆∞·ª£c ki·ªÉm so√°t.\nTrong ·∫£nh ch·ª•p m√†n h√¨nh sau, b·∫°n s·∫Ω th·∫•y quy t·∫Øc Listener ALB ban ƒë·∫ßu ch·ªâ ƒë∆∞·ª£c c·∫•u h√¨nh v·ªõi target group m·∫∑c ƒë·ªãnh.\nH√¨nh 4 ALB listener rule tr∆∞·ªõc khi th√≠ nghi·ªám b·∫Øt ƒë·∫ßu, hi·ªÉn th·ªã m·ªôt target group duy nh·∫•t nh·∫≠n 100% l∆∞u l∆∞·ª£ng truy c·∫≠p.\nSau khi th√≠ nghi·ªám b·∫Øt ƒë·∫ßu, AWS FIS s·ª≠a ƒë·ªïi rule ƒë·ªÉ chia l∆∞u l∆∞·ª£ng truy c·∫≠p ‚Äî nh∆∞ ƒë∆∞·ª£c hi·ªÉn th·ªã trong ·∫£nh ch·ª•p m√†n h√¨nh Sau.\nH√¨nh 5 ALB listener rule sau khi th√≠ nghi·ªám b·∫Øt ƒë·∫ßu, hi·ªÉn th·ªã m·ªôt target group m·ªõi v·ªõi Lambda ƒë∆∞·ª£c c·∫•u h√¨nh ƒë·ªÉ nh·∫≠n 50% l∆∞u l∆∞·ª£ng truy c·∫≠p v√† ph·∫£n h·ªìi b·∫±ng m·ªôt m√£ l·ªói ƒë∆∞·ª£c x√°c ƒë·ªãnh tr∆∞·ªõc.\nB∆∞·ªõc 4: Quan s√°t v√† ph√¢n t√≠ch k·∫øt qu·∫£ B·∫°n c√≥ th·ªÉ x√°c th·ª±c th√≠ nghi·ªám b·∫±ng c√°ch l√†m m·ªõi ALB DNS trong tr√¨nh duy·ªát c·ªßa b·∫°n ho·∫∑c ch·∫°y m·ªôt curl loop:\nwhile true; do curl -s http://\u0026lt;your-alb-dns-name\u0026gt;; sleep 1; done H√¨nh 6 ƒê·∫ßu ra CLI ƒë·ªông hi·ªÉn th·ªã c√°c y√™u c·∫ßu l·∫∑p l·∫°i ƒë·∫øn URL ALB trong m·ªôt v√≤ng l·∫∑p ƒë·ªÉ ch·ª©ng minh c√°ch gi·∫£i ph√°p ti√™m l·ªói.\nB·∫°n s·∫Ω th·∫•y ƒë·∫ßu ra xen k·∫Ω nh∆∞:\nBackend service is healthy (d·ªãch v·ª• backend ƒëang ho·∫°t ƒë·ªông t·ªët) Service Unavailable ‚Äì Simulated Error Response (Lambda) B·∫°n c√≥ th·ªÉ gi√°m s√°t Amazon CloudWatch Logs ƒë·ªÉ xem c√°c ch·ªâ s·ªë th·ª±c thi h√†m Lambda v√† h√†nh vi c·ªßa ·ª©ng d·ª•ng (logic th·ª≠ l·∫°i, c∆° ch·∫ø chuy·ªÉn ƒë·ªïi d·ª± ph√≤ng)\nL∆∞u √Ω: Sau khi b·∫Øt ƒë·∫ßu th√≠ nghi·ªám, c√≥ th·ªÉ m·∫•t ƒë·∫øn m·ªôt ph√∫t tr∆∞·ªõc khi target group m·ªõi ƒë∆∞·ª£c g·∫Øn v√†o ALB v√† l∆∞u l∆∞·ª£ng truy c·∫≠p b·∫Øt ƒë·∫ßu ƒë·ªãnh tuy·∫øn ƒë·∫øn h√†m Lambda. Trong kho·∫£ng th·ªùi gian ng·∫Øn n√†y, t·∫•t c·∫£ c√°c y√™u c·∫ßu c√≥ th·ªÉ ti·∫øp t·ª•c ƒë·∫øn d·ªãch v·ª• backend ban ƒë·∫ßu.\nC∆° ch·∫ø kh√¥i ph·ª•c Th√≠ nghi·ªám nh·∫±m m·ª•c ƒë√≠ch gi√∫p √≠ch cho c√°c ho·∫°t ƒë·ªông kh√¥i ph·ª•c, m·∫∑c d√π n√™n ki·ªÉm tra trong m√¥i tr∆∞·ªùng c·ªßa b·∫°n:\nALB rule s·∫Ω t·ª± ƒë·ªông ƒë∆∞·ª£c kh√¥i ph·ª•c v√†o cu·ªëi th·ªùi gian ki·ªÉm tra. Target group t·∫°m th·ªùi ƒë∆∞·ª£c g·ª° b·ªè v√† x√≥a ƒë·ªÉ ngƒÉn ch·∫∑n b·∫•t k·ª≥ c·∫•u h√¨nh c√≤n s√≥t l·∫°i n√†o. N·∫øu th√≠ nghi·ªám b·ªã h·ªßy, quy tr√¨nh kh√¥i ph·ª•c s·∫Ω ƒë∆∞a h·ªá th·ªëng tr·ªü l·∫°i tr·∫°ng th√°i ban ƒë·∫ßu. C√°c ƒëi·ªÉm c·∫ßn c√¢n nh·∫Øc B√†i ƒëƒÉng n√†y cung c·∫•p th√¥ng tin k·ªπ thu·∫≠t v√† c·∫•u h√¨nh v√≠ d·ª•. Vi·ªác tri·ªÉn khai trong m√¥i tr∆∞·ªùng c·ªßa b·∫°n c√≥ th·ªÉ y√™u c·∫ßu th√™m c√°c c√¢n nh·∫Øc v·ªÅ b·∫£o m·∫≠t, tu√¢n th·ªß v√† k·ªπ thu·∫≠t. Lu√¥n ki·ªÉm tra k·ªπ l∆∞·ª°ng tr∆∞·ªõc trong c√°c m√¥i tr∆∞·ªùng phi s·∫£n xu·∫•t.\nD·ªçn d·∫πp ƒê·ªÉ tr√°nh ph√°t sinh chi ph√≠ trong t∆∞∆°ng lai, h√£y x√≥a c√°c t√†i nguy√™n ƒë√£ tri·ªÉn khai:\naws cloudformation delete-stack --stack-name alb-fis-experiment K·∫øt lu·∫≠n Trong b√†i ƒëƒÉng n√†y, ch√∫ng t√¥i ƒë√£ ch·ª©ng minh c√°ch m·ªü r·ªông kh·∫£ nƒÉng c·ªßa AWS FIS b·∫±ng c√°ch m√¥ ph·ªèng l·ªói c·ª•c b·ªô cho c√°c workload ƒë·∫±ng sau ALB b·∫±ng c√°ch s·ª≠ d·ª•ng Lambda. Gi·∫£i ph√°p n√†y cho ph√©p c√°c nh√≥m ki·ªÉm tra kh·∫£ nƒÉng ph·ª•c h·ªìi c·ªßa ·ª©ng d·ª•ng ƒë·ªëi v·ªõi c√°c l·ªói kh√¥ng li√™n t·ª•c m√† kh√¥ng g√¢y ra s·ª± c·ªë ng·ª´ng ho·∫°t ƒë·ªông ho√†n to√†n. B·∫±ng c√°ch t·∫≠n d·ª•ng AWS FIS, Lambda, v√† c√°c ALB routing rules, b·∫°n c√≥ th·ªÉ t·∫°o ra c√°c k·ªãch b·∫£n l·ªói c√≥ ki·ªÉm so√°t v√† tƒÉng c∆∞·ªùng ƒë·ªô v·ªØng ch·∫Øc c·ªßa h·ªá th·ªëng.\nƒê·ªÉ t√¨m hi·ªÉu th√™m, h√£y kh√°m ph√° c√°c t√†i nguy√™n sau:\nT√†i li·ªáu AWS Fault Injection Service T√†i li·ªáu Amazon ALB S·ª≠ d·ª•ng SSM Systems Manager document v·ªõi AWS FIS B·∫Øt ƒë·∫ßu v·ªõi CloudFormation template v√† chia s·∫ª kinh nghi·ªám c·ªßa b·∫°n trong ph·∫ßn b√¨nh lu·∫≠n b√™n d∆∞·ªõi.\nTAGS: aws fault injection simulator, chaos engineering\nOzgur Canibeyaz Ozgur l√† Senior Technical Account Manager t·∫°i Amazon Web Services v·ªõi 8 nƒÉm kinh nghi·ªám. Ozgur gi√∫p kh√°ch h√†ng t·ªëi ∆∞u h√≥a vi·ªác s·ª≠ d·ª•ng AWS c·ªßa h·ªç b·∫±ng c√°ch x·ª≠ l√Ω c√°c th√°ch th·ª©c k·ªπ thu·∫≠t, kh√°m ph√° c√°c c∆° h·ªôi ti·∫øt ki·ªám chi ph√≠, ƒë·∫°t ƒë∆∞·ª£c s·ª± xu·∫•t s·∫Øc trong v·∫≠n h√†nh v√† x√¢y d·ª±ng c√°c d·ªãch v·ª• s√°ng t·∫°o b·∫±ng c√°c s·∫£n ph·∫©m AWS.\nPablo Colazurdo Pablo l√† Principal Solutions Architect t·∫°i AWS, n∆°i anh ·∫•y th√≠ch gi√∫p kh√°ch h√†ng ra m·∫Øt c√°c d·ª± √°n th√†nh c√¥ng tr√™n Cloud. Anh ·∫•y c√≥ nhi·ªÅu nƒÉm kinh nghi·ªám l√†m vi·ªác v·ªõi nhi·ªÅu c√¥ng ngh·ªá ƒëa d·∫°ng v√† ƒëam m√™ h·ªçc h·ªèi nh·ªØng ƒëi·ªÅu m·ªõi. Pablo l·ªõn l√™n ·ªü Argentina nh∆∞ng hi·ªán ƒëang t·∫≠n h∆∞·ªüng c∆°n m∆∞a ·ªü Ireland trong khi nghe nh·∫°c, ƒë·ªçc s√°ch ho·∫∑c ch∆°i D\u0026amp;D v·ªõi c√°c con.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.7-dashboard-setup/5.7.3-setup-api-gateway/","title":"API Gateway Setup","tags":[],"description":"","content":"In this guide, you will setup an API Gateway to route api call from dashboard to Lambda.\nCreate API Gateway Open the API Gateway Console\nNavigate to https://console.aws.amazon.com/apigateway/ Or: AWS Management Console ‚Üí Services ‚Üí API Gateway Create API:\nClick Create API Choose REST API and click Build Use this setting for creation: Choose New API Name: dashboard-api API endpoint type: Regional Security policy: SecurityPolicy_TLS13_1_3_2025_09 Endpoint access mode: Basic IP address type: IPv4 Create Resources:\nEnable CORS for the root resource Click Create resource and name it logs Then click on /logs resource that just created and click Create Resource to create child resource of /logs Name it cloudtrail and enable CORS Repeat this three more times for eni_logs, guardduty and vpc Create methods:\nClick on /cloudtrail that just created and click Cretae method\nIn method creation, use this setting:\nMethod type: GET Intergration type: Lambda function Enable Lambda proxy intergration choose Buffered Lambda function: select your region search for dashboard-query and choose it Timout: 29000 Repeat this three more time for eni_logs, guardduty and vpc\nDeploy API:\nClick the Deploy API on the right corner In deploy API, use this setting: Stage: New stage Name: prod Click Deploy "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.3-cloudwatch-etl/","title":"CloudWatch ETL Code","tags":[],"description":"","content":"import json import boto3 import gzip import re import os from datetime import datetime, timezone s3 = boto3.client(\u0026#34;s3\u0026#34;) firehose= boto3.client(\u0026#34;firehose\u0026#34;) # -------------------------------------------------- # CONFIG # -------------------------------------------------- SOURCE_PREFIX = \u0026#34;exportedlogs/vpc-dns-logs/\u0026#34; FIREHOSE_STREAM_NAME = os.environ.get(\u0026#34;FIREHOSE_STREAM_NAME\u0026#34;) VPC_RE = re.compile(r\u0026#34;/(vpc-[0-9A-Za-z\\-]+)\u0026#34;) ISO_TS_RE = re.compile(r\u0026#34;^\\d{4}-\\d{2}-\\d{2}T\u0026#34;) def read_gz(bucket, key): obj = s3.get_object(Bucket=bucket, Key=key) with gzip.GzipFile(fileobj=obj[\u0026#34;Body\u0026#34;]) as f: return f.read().decode(\u0026#34;utf-8\u0026#34;, errors=\u0026#34;replace\u0026#34;) def flatten_once(d): out = {} for k, v in (d or {}).items(): if isinstance(v, dict): for k2, v2 in v.items(): out[f\u0026#34;{k}_{k2}\u0026#34;] = v2 else: out[k] = v return out def safe_int(x): try: return int(x) except: return None def parse_dns_line(line): raw = line.strip() if not raw: return None json_part = raw prefix_ts = None if ISO_TS_RE.match(raw): try: prefix_ts, rest = raw.split(\u0026#34; \u0026#34;, 1) json_part = rest except: pass if not json_part.startswith(\u0026#34;{\u0026#34;): idx = json_part.find(\u0026#34;{\u0026#34;) if idx != -1: json_part = json_part[idx:] try: obj = json.loads(json_part) except: return None flat = flatten_once(obj) if prefix_ts: flat[\u0026#34;_prefix_ts\u0026#34;] = prefix_ts return flat def lambda_handler(event, context): print(f\u0026#34;Received S3 Event. Records: {len(event.get(\u0026#39;Records\u0026#39;, []))}\u0026#34;) firehose_records = [] for record in event.get(\u0026#34;Records\u0026#34;, []): if \u0026#34;s3\u0026#34; not in record: continue bucket = record[\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] key = record[\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;] if not key.startswith(SOURCE_PREFIX) or not key.endswith(\u0026#34;.gz\u0026#34;): print(f\u0026#34;Skipping file: {key}\u0026#34;) continue print(f\u0026#34;Processing S3 file: {key}\u0026#34;) # Extract VPC ID from file path vpc_id_match = VPC_RE.search(key) vpc_id = vpc_id_match.group(1) if vpc_id_match else \u0026#34;unknown\u0026#34; # Read and process file content content = read_gz(bucket, key) if not content: continue for line in content.splitlines(): r = parse_dns_line(line) if not r: continue # Create flattened JSON record out = { \u0026#34;version\u0026#34;: r.get(\u0026#34;version\u0026#34;), \u0026#34;account_id\u0026#34;: r.get(\u0026#34;account_id\u0026#34;), \u0026#34;region\u0026#34;: r.get(\u0026#34;region\u0026#34;), \u0026#34;vpc_id\u0026#34;: r.get(\u0026#34;vpc_id\u0026#34;, vpc_id), \u0026#34;query_timestamp\u0026#34;: r.get(\u0026#34;query_timestamp\u0026#34;), \u0026#34;query_name\u0026#34;: r.get(\u0026#34;query_name\u0026#34;), \u0026#34;query_type\u0026#34;: r.get(\u0026#34;query_type\u0026#34;), \u0026#34;query_class\u0026#34;: r.get(\u0026#34;query_class\u0026#34;), \u0026#34;rcode\u0026#34;: r.get(\u0026#34;rcode\u0026#34;), \u0026#34;answers\u0026#34;: json.dumps(r.get(\u0026#34;answers\u0026#34;), ensure_ascii=False), \u0026#34;srcaddr\u0026#34;: r.get(\u0026#34;srcaddr\u0026#34;), \u0026#34;srcport\u0026#34;: safe_int(r.get(\u0026#34;srcport\u0026#34;)), \u0026#34;transport\u0026#34;: r.get(\u0026#34;transport\u0026#34;), \u0026#34;srcids_instance\u0026#34;: r.get(\u0026#34;srcids_instance\u0026#34;), \u0026#34;timestamp\u0026#34;: (r.get(\u0026#34;query_timestamp\u0026#34;) or r.get(\u0026#34;timestamp\u0026#34;) or r.get(\u0026#34;_prefix_ts\u0026#34;)) } # Add newline for JSONL format json_row = json.dumps(out, ensure_ascii=False) + \u0026#34;\\n\u0026#34; firehose_records.append({\u0026#39;Data\u0026#39;: json_row}) # Send to Firehose in batches of 500 if firehose_records: total_records = len(firehose_records) print(f\u0026#34;Sending {total_records} records to Firehose...\u0026#34;) batch_size = 500 for i in range(0, total_records, batch_size): batch = firehose_records[i:i + batch_size] try: response = firehose.put_record_batch( DeliveryStreamName=FIREHOSE_STREAM_NAME, Records=batch ) if response[\u0026#39;FailedPutCount\u0026#39;] \u0026gt; 0: print(f\u0026#34;Warning: {response[\u0026#39;FailedPutCount\u0026#39;]} records failed\u0026#34;) except Exception as e: print(f\u0026#34;Firehose error: {e}\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;total_records\u0026#34;: len(firehose_records)} "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.3-foundation-setup/5.3.3-create-iam-roles-and-policies/5.3.3.3-create-iam-policy/","title":"Create IAM Policy","tags":[],"description":"","content":"Create IAM Quarantine Policy Create IrQuarantineIAMPolicy Navigate to IAM Console ‚Üí Policies ‚Üí Create policy\nPolicy JSON:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Policy name: IrQuarantineIAMPolicy Description: Deny-all policy for quarantining compromised IAM users "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.3-foundation-setup/5.3.3-create-iam-roles-and-policies/","title":"Create IAM Roles and Policies","tags":[],"description":"","content":"In this section, you will create 17 IAM roles with their associated policies for Lambda functions, Firehose streams, Step Functions, and other services.\nOverview of IAM Roles Lambda Execution Roles (9 roles):\nCloudTrailETLLambdaServiceRole GuardDutyETLLambdaServiceRole CloudWatchETLLambdaServiceRole CloudWatchENIETLLambdaServiceRole CloudWatchExportLambdaServiceRole ParseFindingsLambdaServiceRole IsolateEC2LambdaServiceRole QuarantineIAMLambdaServiceRole AlertDispatchLambdaServiceRole Service Roles (6 roles): 10. CloudTrailFirehoseRole 11. CloudWatchFirehoseRole 12. StepFunctionsRole 13. IncidentResponseStepFunctionsEventRole 14. FlowLogsIAMRole 15. GlueCloudWatchRole\nIAM Policy (1 policy): 16. IrQuarantineIAMPolicy\nContent Create Lambda Execution Roles Create Service Roles Create IAM Policy "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.5-processing-setup/5.5.3-create-lambda-function-etl-processing/","title":"Create Lambda Function - ETL Processing","tags":[],"description":"","content":"Create Lambda Functions - ETL Processing In this section, you will create 5 Lambda functions that process logs and send them to Kinesis Firehose or S3.\nincident-response-cloudtrail-etl Runtime: Python 3.12 Handler: CloudTrailETL.lambda_handler Role: CloudTrailETLLambdaServiceRole Timeout: 300s, Memory: 128MB Env: FIREHOSE_STREAM_NAME=cloudtrail-firehose-stream Code: cloudtrail-etl incident-response-guardduty-etl Runtime: Python 3.12 Handler: guardduty_etl.lambda_handler Role: GuardDutyETLLambdaServiceRole Timeout: 300s, Memory: 128MB Env: DESTINATION_BUCKET, S3_LOCATION_GUARDDUTY, DATABASE_NAME, TABLE_NAME_GUARDDUTY Code: guardduty-etl cloudwatch-etl-lambda Runtime: Python 3.12 Handler: cloudwatch_etl.lambda_handler Role: CloudWatchETLLambdaServiceRole Env: FIREHOSE_STREAM_NAME=vpc-dns-firehose-stream Code: cloudwatch-etl cloudwatch-eni-etl-lambda Runtime: Python 3.12 Handler: cloudwatch_eni_etl.lambda_handler Role: CloudWatchENIETLLambdaServiceRole Env: FIREHOSE_STREAM_NAME=vpc-flow-firehose-stream Code: cloudwatch-eni-etl cloudwatch-export-lambda Runtime: Python 3.12 Handler: cloudwatch_autoexport.lambda_handler Role: CloudWatchExportLambdaServiceRole Env: DESTINATION_BUCKET=incident-response-log-list-bucket-ACCOUNT_ID-REGION Code: cloudwatch-autoexport Configure CloudWatch Logs Subscription Filter Configure Subscription Filter Open the CloudWatch Console.\nIn the left navigation pane, select Log Management.\nClick on the centralized log group: /aws/incident-response/centralized-logs.\nCreate Subscription Filter:\nClick the \u0026ldquo;Subscription filters\u0026rdquo; tab. Click \u0026ldquo;Create Lambda subscription filter\u0026rdquo;. Configure Destination:\nDestination Lambda function: Select the function cloudwatch-export-lambda. Log format: Select \u0026ldquo;Other\u0026rdquo;. (This ensures the raw log data is passed efficiently for Lambda processing). Configure Log Format and Filter:\nSubscription filter name: Enter a descriptive name, e.g., VPC-Log-Export-Filter. Filter pattern: Leave this field blank. (Ensures all logs in the group are processed). Click \u0026ldquo;Start streaming\u0026rdquo;.\nConfigure S3 Event Notifications S3 Console ‚Üí incident-response-log-list-bucket-ACCOUNT_ID-REGION ‚Üí Properties ‚Üí Event notifications\nCreate 4 notifications with Event types/Object creation/‚úÖAll object create events:\nCloudTrailETLTrigger: Prefix AWSLogs/ACCOUNT_ID/CloudTrail/ ‚Üí Lambda incident-response-cloudtrail-etl VPCDNSLogsTrigger: Prefix exportedlogs/vpc-dns-logs/ ‚Üí Lambda cloudwatch-etl-lambda VPCFlowLogsTrigger: Prefix exportedlogs/vpc-flow-logs/ ‚Üí Lambda cloudwatch-eni-etl-lambda GuardDutyFindingsTrigger: Prefix AWSLogs/ACCOUNT_ID/GuardDuty/ ‚Üí Lambda incident-response-guardduty-etl "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.3-foundation-setup/","title":"Foundation Setup","tags":[],"description":"","content":"This initial Foundation Setup phase establishes the core prerequisites for the Auto Incident Response System, concentrating on the deployment of dedicated storage and essential security authorization. This mandates the creation of five secure Amazon S3 buckets for centralized log ingestion and processing, applying a necessary Bucket Policy for secure log delivery, and defining 17 IAM roles and a quarantine policy to enforce least-privilege access across all integrated AWS services.\nContent Set up Amazon S3 Bucket Configure S3 Bucket Policy for Primary Log Bucket Create IAM Roles and Policies "},{"uri":"https://veljg.github.io/AWS-Worklog/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Achieve Excellence in Aftermarket Service with Syncron and AWS This blog explains how manufacturers can enhance their aftermarket services by implementing the Syncron Service Lifecycle Management (SLM) platform on AWS. You will learn why traditional, isolated operations often lead to inefficiencies and increased costs , and how Syncron‚Äôs SLM platform creates a connected business ecosystem by unifying data from parts, service, and warranty management. The article also explores the solution\u0026rsquo;s architecture, which leverages services like Amazon S3 and AWS Glue, and walks through customer use cases such as gaining instant access to data and building custom AI/ML models for price optimization.\nBlog 2 - Amazon Q Developer CLI supports image inputs in your terminal This blog introduces the powerful new capability of the Amazon Q Developer CLI to accept and analyze image inputs directly in the terminal. You will learn how this feature bridges the gap between visual design assets and functional code, streamlining development by reducing the manual, error-prone work of translating diagrams into implementation. The article also provides a hands-on guide with several practical use cases, demonstrating how to generate Terraform code from an architecture diagram, create a SQL schema from an ER diagram, transform a hand-drawn sketch into a formal design document, and build UI code from a simple screenshot.\nBlog 3 - Simulating partial failures with AWS Fault Injection Service This blog details an advanced chaos engineering technique for simulating partial, or localized, system failures using AWS Fault Injection Service (FIS). You will learn why testing for these non-total failures is critical for building truly resilient applications and how traditional fault injection methods often overlook this important scenario. The article also provides a complete, step-by-step walkthrough of the solution, guiding you on how to combine FIS with an Application Load Balancer (ALB) and an AWS Lambda function to inject controlled faults that impact only a percentage of traffic, all without requiring any changes to your application\u0026rsquo;s code.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: ‚ÄúAWS Cloud Mastery Series #2 - DevOps on AWS‚Äù Event Objectives Introduce AWS DevOps Services ‚Äì CI/CD Pipeline Introduce Infrastructure as Code (IaC) and related tools Introduce Container Services on AWS Ensure Monitoring and Observability capability using AWS Services Speakers Truong Quang Tinh ‚Äì AWS Community Builder, Platform Engineer - TymeX Bao Huynh ‚Äì AWS Community Builder Nguyen Khanh Phuc Thinh ‚Äì AWS Community Builder Tran Dai Vi ‚Äì AWS Community Builder Huynh Hoang Long ‚Äì AWS Community Builder Pham Hoang Quy ‚Äì AWS Community Builder Nghiem Le ‚Äì AWS Community Builder Dinh Le Hoang Anh - Cloud Engineer Trainee, First Cloud AI Journey Key Highlights DevOps Mindset - Culture: Collaboration, Automation, Continuous Learning and Measurement - DevOps Roles: DevOps Engineer, Cloud Engineer, Platform Engineer, Site Reliability Engineer - Success Metrics: + Ensure deployment health + Improve agility + System stability + Optimize customer Experience + Justify technology investments\nDO DON\u0026rsquo;T Start with Fundamentals Stay in Tutorial Hell Learn by Building Real Projects Copy-paste blindly Document Everything Compare Your Progress to Others Master one thing at a time Give Up After Failures Soft Skills Enhancement - Continuous Integration: Team members integrate their work frequently, aims for continuous Delivery and Deployment\nInfrastructure as Code (IaC) -Benefits: Automation, Scalability, Reproducibility and better Collaboration\nAWS CloudFormation AWS\u0026rsquo;s own built in IaC tool, use templates written with YAML or JSON, can build every AWS Infrastructure automatically\n- Stack: A set of AWS Resources defined in a template, can be used by CloudFormation to create, update or delete said resources\n- CloudFormation Template: A YAML/JSON file that define an AWS Infrastructure, act like a blueprint to deploy and configure resources\n- How it works: Create template -\u0026gt; Store in S3 Bucket or Local storage -\u0026gt; Use CloudFormation to create Stacks based on template -\u0026gt; CloudFormation built resources\n- Drift Detection: Detect changes in the infrastructure compared to the Stack =\u0026gt; Update Stack or revert change, useful for versioning\nAWS Cloud Development Kit(CDK) Open-source software development framework, support IaC using real programming languages(Python,Java,C#.Net, Type/JavaScript and Go)\n- Construct: Building blocks, comprised of components that represent AWS Resources and their Configuration, have 3 construct level:\nL1 Construct: Low-level resources maps directly to a single AWS CloudFormation resource L2 Construct: Provide a higher-level abstraction through an intuitive intent-based API,encapsulate best practices and security defaults L3 Construct: Complete architecture patterns with multiple resources, opinionated implementation and fast deployment AWS Amplify AWS platform that makes it easy to build, deploy, and scale web and mobile apps, uses CloudFormation under the hood: Stacks deployed to built infrastructure programmatically\nTerraform IaC tool, start by defining infrastructure in Terraform code and plan then apply the infrastructure on multiple cloud platforms like Azure, AWS, Google Cloud, etc..\n- Strength: Multi-Cloud support, State tracking with the same configuration\nHow to choose IaC Tools? -Criteria:\nPlan using one Cloud or many? Role as Developer or Ops? Does the Cloud and Ecosystem support the tool? Container Services on AWS Dockerfile A Dockerfile defines how to build a container image, which describe the environment, dependencies, build steps, and final runtime configuration, ensuring that the application run consistently across any system that support Dockers\n- Images: A packaged blueprint of an application, build from a Dockerfile using layered file system, used to create containers consistently across environments\n- Workflow: Dockerfile build a Docker Image which can be used to run Container and push to ECR/Docker Hub\nAmazon ECR A fully managed container registry that make it easy to store, manage, and securely share Docker container image. AWS\u0026rsquo;s own secure and scalable private container registry\n-Features:\nImage Scanning Immutable Tags Lifecycle Policies Encryption \u0026amp; IAM - Orchestration: Orchestrate many containers processes: restart containers, scale up automatically under high load, distribute traffic efficiently, manage where containers are placed and run\nKubernetes Open source, automates deployment, scaling, healing, and load balancing - Components:\nMaster Node: Control Plane, manage worker nodes and pods Worker Node: Run application workloads inside pods Pod: Smallest deployable unit, can contain one or more containers Service ECS vs EKS\nFeature Amazon ECS (Elastic Container Service) Amazon EKS (Elastic Kubernetes Service) Core Technology AWS-native container orchestration Kubernetes-based (open-source standard) Complexity Simpler, easier to operate Highly flexible but more complex Knowledge Required No Kubernetes knowledge needed Requires Kubernetes knowledge (pods, deployments, etc.) AWS Integration Deep AWS integration (ALB, IAM, CloudWatch, etc.) Standard Kubernetes integration Use Case/Benefits Great for fast deployments \u0026amp; lower ops overhead Multi-cluster, multi-cloud portability Ecosystem/Community AWS-native tools and community Larger ecosystem \u0026amp; community tools Summary ECS = easier, faster to run, lower operational overhead EKS = more flexibility, more control, more complexity App Runner Suitable for quick deployment of web applications and REST APIS, ideal for small to medium production workload\nMonitoring \u0026amp; Observability CloudWatch Monitor AWS Resources and Applications running on AWS in real time Provide observabilty Alarms and automated responses Dashboard to help with operational and cost optimization - CloudWatch metrics: Data of the performance of system on AWS or on premise with CloudWatch Agent, integrate well with EventBridge, Auto Scaling and DevOps workflow\nAWS X-Ray - Distributed Tracing: Tracks requests end-to-end, and draw maps and paths between service visited, add SDK to code to trace IDs\n- Performance Insight: Root cause analysis for latency and errors, deduce insights from traces and provide Real User Monitoring\nEvent Experience This event was very important for our project as it tackle our plan of adding IaC using CDK, instead of using ClickOps for maintainability and reproducibility. Also some more insights on CloudWatch helped greatly with our data monitoring feature\nThe speakers answered our team\u0026rsquo;s question:\nQ: Our project up until now have been purely built with ClickOps, and we are planning to use CDK. Are there any tool that could scan and turn our existing infrastructure into CDK or CloudFormation rather than reproducing the infrastructure from scratch with IaC?\nA: Unfortunately no, there isn\u0026rsquo;t a tool that can assist with that problem yet, your team is going to have to built the infrastructure from scratch again. If there by any chance that you do found a tool that can assist with please share with us too.\nQ: We noticed that AWS X-Ray used with CloudWatch is similar to CloudTrail in its tracing method, can you explain more on what differentiate them?\nA: X-Ray is used for CloudWatch and used to trail the resources and services that the system interacted with, meanwhile CloudTrail is commonly used to trail the AWS user\u0026rsquo;s actions\nQ: Our project is built around Guard Duty Findings, do you have any experiences on how to reliably trigger Findings for a demo scenarios?\nA: In my experience I know that Guard Duty Findings can be triggered by port scanning activities but i\u0026rsquo;m sure there are other ways too\nA: Guard Duty can be configured to have a threat list containing custom rules to trigger findings upon activities relating the configured malicious domains or IPs\nThis event is also the first time some of the speaker\u0026rsquo;s first time presenting a topic:\nThe DevOps and IaC sections was well presented Monitoring \u0026amp; Observability wasn\u0026rsquo;t as great and we can notice the speaker\u0026rsquo;s nervousness but still delivered great values regardless Some event photos Group picture by speaker Tran Dai Vi\n"},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Complete Module 6 Started on proposal Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Module 6: Database Concept review:\n+ Database + Session + Primary/Foreign Key + Index + Partitions + Execution/Query Plan + Log \u0026amp; Buffer + RDBMS (Relational Database Manangement System) + NOSQL + OLTP(Online Transaction Processing): For payments, transactions + OLAP (Online Analytical Processing): Analyze data, predict trends and patterns - AWS RDS (Relational Database Serive): Include Aurora, MySQL, Postgres SQL , MSSQL, Oracle , Maria + Automatic backup + Generate read replica +Read replica can be turned into primary code + Auto Fail Over/Multi AZ (Backups on mutiple AZs) + Commonly used for OLTP + Encrypt data while at rest/in transit + Protected my Security Group and NACL + Can change instance size + Storage Auto SCaling - Amazon Aurora: Optimized underlying storage infrastructure, uses MySQL and PostgreSQL + Back track: revert to previous state + Clone + Global Database (Multi Region) + Multi Master: Many Master Databases - Amazon Redshift: Data warehouse service: PostgreSQL core, optimized for OLAP + Uses MMP Database: data is partitioned and saved at computer nodes, a Leader node is used to coordinate and compile queries + Stores data in a columnar storage format, useful for OLAP applications + Uses SQL and drivers like JDBC and ODBC + Provide cost effective services (Transient Cluster/ Redshift spectrum) - Amazon ElastiCache: Creates Cluster Caching Engines (Redis/Memcached) + Detects and replaces failed nodes + Put before CSDL layer in order to cache data + Recommended to use Redis for new workloads + Using ElastiCache requires caching logic on applications, not recommended to use default system caching - Formulated a proposal for workshop with teammates - School subject: + KS57: Completed Qu·∫£n tr·ªã d·ªØ li·ªáu v√† an to√†n th√¥ng tin 29/09/2025 29/09/2025 Qu·∫£n tr·ªã d·ªØ li·ªáu v√† an to√†n th√¥ng tin 3 - Lab 43: Guide is broken, the link doesnt go anywhere, going by video + Downloaded Schema Conversion Tool + Downloaded MSSQL in EC2 Instance + No SQL script was given, trying with custom basic MSSQL Database + No CloudFormation Stack was given, skipping Oracle Database connection + Installed MySQL on EC2 Instance + Migrated custom MSSQL Database to MySQL Database using AWS Schema Conversion Tool + Created custom RDS to test migration task + Attempted to migrate from local machine to RDS + Tried to use AWS Replication Agent: Unsuccessful due to it being made for Window/Linux server only, not OS + Tried to portforward PC to be used as an endpoint + Failed portforwarding, not allowed by ISP 30/09/2025 30/09/2025 Lab 43 Application Mirgation Service Guide 4 - Found out AWS account\u0026rsquo;s credits are all expired from doing lab 12 - Wrote a support case - Stopping labs for now - Focus on researching about team\u0026rsquo;s proposal 01/10/2025 01/10/2025 - School subject: + ENW439c: Completed Research Methodologies Research Methodologies 5 - Continued doing labs by aquiring help from team member: Created an IAM User with admin privilege for me to log in and use their account - Translate first blog 02/10/2025 02/10/2025 Blog 1 6 - Joined the AI-Driven Development Life Cycle: Reimagining Software Engineering event - Translated second and third blog 03/10/2025 04/10/2025 Blog 2 Blog 3 Event Summary and Experience Week 4 Achievements: Completed a comprehensive review of core database concepts including RDBMS, keys, indexes, partitioning, OLTP/OLAP, and AWS-specific database services.\nGained theoretical knowledge of the features and use cases for AWS RDS, Amazon Aurora (e.g., Backtrack, Global Database), Amazon Redshift (Data Warehouse for OLAP), and Amazon ElastiCache (caching with Redis/Memcached).\nDatabase Migration: Attempted a complex database migration lab, demonstrating resourcefulness by:\nSourcing a custom MSSQL Database and installing necessary services on an EC2 instance due to broken lab guides.\nSuccessfully migrating the custom MSSQL database to MySQL using the AWS Schema Conversion Tool (SCT).\nIdentified and addressed the issue of expired AWS credits by raising a support case.\nSecured continuation of lab work by setting up an IAM User with admin privileges on a team member\u0026rsquo;s account.\nFormulated a proposal for the team\u0026rsquo;s upcoming workshop with teammates.\nCompleted the translation of three blogs.\nAttended the AI-Driven Development Life Cycle: Reimagining Software Engineering event.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.7-dashboard-setup/5.7.4-setup-cloudfront/","title":"Cloudfront Setup","tags":[],"description":"","content":"In this guide, you will setup a Cloudfront for cache, routing and web accessing.\nCreate Cloudfront Distribution Open the Cloudfront Console\nNavigate to https://console.aws.amazon.com/cloudfront/ Or: AWS Management Console ‚Üí Services ‚Üí Cloudfront Create Distribution:\nClick the Create distribution button In distribution creation, use this setting: Choose a plan: Free plan Name: Static Dashboard Website CloudFront Origin type: Amazom S3 S3 Origin: Choose the static-dashboard-bucket Keep the rest like default Enable security: Use this if you choose free plan Review and click Create distribution General setting:\nAfter creation complete, on your Cloudfront General tab click on Edit At the Default root object enter index.html Description: Static Dashboard Distribution Click Save change Create API Gateway origin:\nClick Origins on the menu tabs Then click Create origin In orogin creation, use this setting: Origin domain: choose dashboard-api Protocol: HTTPS only HTTPS port: 443 Minimum Origin SSL protocol: TLSv1.2 Origin path: /prod Click Create origin Create behaviors for API Gateway:\nClick Behaviors on the menu tabs Then click Create behavior In behavior creation, use this setting: Path pattern: /logs/* Origin and origin groups: choose dashboard-api Leave the rest setting like default Click Create behavior Update S3 policy to work with Cloudfront:\nClick Origins on the menu tabs, choose the s3-static-dashboard origin name Click Edit At Origin access controll section press Go to S3 bucket permissions Check if your S3 permission look like this, if don\u0026rsquo;t then copy and paste it to your S3 permission (Change the ACCOUNT_ID, ACCOUNT_REGION and CLOUDFRONT_ID to your): { \u0026#34;Version\u0026#34;: \u0026#34;2008-10-17\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;PolicyForCloudFrontPrivateContent\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudFrontServicePrincipal\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudfront.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::s3-static-dashboard-[ACCOUNT_ID]-[ACCOUNT_REGION]/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;ArnLike\u0026#34;: { \u0026#34;AWS:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudfront::[ACCOUNT_ID]:distribution/[CLOUDFRONT_ID]\u0026#34; } } } ] } Click Save change Create error pages:\nClick Error pages on the menu tabs Click Create custom error page In custom error page creation, use this setting: HTTP error code: 403: Forbident Error caching minimum TTL: 300 Customize error response: Yes Response page path: /index.html HTTP Response code: 200: OK Repeat this for 404 code "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.4-cloudwatch-eni-etl/","title":"CloudWatch ENI ETL Code","tags":[],"description":"","content":" import json import boto3 import gzip import os from datetime import datetime s3 = boto3.client(\u0026#34;s3\u0026#34;) firehose = boto3.client(\u0026#34;firehose\u0026#34;) # -------------------------------------------------- # CONFIGURATION # -------------------------------------------------- FIREHOSE_STREAM_NAME = os.environ.get(\u0026#34;FIREHOSE_STREAM_NAME\u0026#34;) # ----------------------------- UTILS ----------------------------- def read_gz(bucket, key): obj = s3.get_object(Bucket=bucket, Key=key) with gzip.GzipFile(fileobj=obj[\u0026#34;Body\u0026#34;]) as f: return f.read().decode(\u0026#34;utf-8\u0026#34;, errors=\u0026#34;replace\u0026#34;) def safe_int(x): try: return int(x) except: return None def parse_flow_log_line(line): parts = line.strip().split(\u0026#39; \u0026#39;) if len(parts) \u0026lt; 14: return None try: start_timestamp = safe_int(parts[10]) time_str = None if start_timestamp: dt_object = datetime.fromtimestamp(start_timestamp) time_str = dt_object.strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) record = { \u0026#34;version\u0026#34;: safe_int(parts[0]), # C·ªôt 1: version (int) \u0026#34;account_id\u0026#34;: parts[1], # C·ªôt 2: account_id (STRING) \u0026#34;interface_id\u0026#34;: parts[2], # C·ªôt 3: eni-... \u0026#34;srcaddr\u0026#34;: parts[3], \u0026#34;dstaddr\u0026#34;: parts[4], \u0026#34;srcport\u0026#34;: safe_int(parts[5]), \u0026#34;dstport\u0026#34;: safe_int(parts[6]), \u0026#34;protocol\u0026#34;: safe_int(parts[7]), \u0026#34;packets\u0026#34;: safe_int(parts[8]), \u0026#34;bytes\u0026#34;: safe_int(parts[9]), \u0026#34;start_time\u0026#34;: start_timestamp, # C·ªôt 11 \u0026#34;end_time\u0026#34;: safe_int(parts[11]), \u0026#34;action\u0026#34;: parts[12], \u0026#34;log_status\u0026#34;: parts[13], \u0026#34;timestamp_str\u0026#34;: time_str } return record except Exception as e: print(f\u0026#34;Error parsing line: {e}\u0026#34;) return None def lambda_handler(event, context): print(f\u0026#34;Received S3 Event. Records: {len(event.get(\u0026#39;Records\u0026#39;, []))}\u0026#34;) firehose_records = [] # Duy·ªát qua c√°c file S3 g·ª≠i v·ªÅ for record in event.get(\u0026#34;Records\u0026#34;, []): if \u0026#34;s3\u0026#34; not in record: continue bucket = record[\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] key = record[\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;] # Ch·ªâ x·ª≠ l√Ω file .gz if not key.endswith(\u0026#34;.gz\u0026#34;): print(f\u0026#34;Skipping non-gz: {key}\u0026#34;) continue print(f\u0026#34;Processing: {key}\u0026#34;) # ƒê·ªçc n·ªôi dung content = read_gz(bucket, key) if not content: continue # Parse t·ª´ng d√≤ng log for line in content.splitlines(): rec = parse_flow_log_line(line) if not rec: continue # Chuy·ªÉn th√†nh JSON string v√† th√™m xu·ªëng d√≤ng (\\n) json_row = json.dumps(rec) + \u0026#34;\\n\u0026#34; firehose_records.append({\u0026#39;Data\u0026#39;: json_row}) # ƒê·∫©y sang Firehose (Batching 500 d√≤ng) if firehose_records: total = len(firehose_records) print(f\u0026#34;Flushing {total} records to Firehose...\u0026#34;) batch_size = 500 for i in range(0, total, batch_size): batch = firehose_records[i:i + batch_size] try: response = firehose.put_record_batch( DeliveryStreamName=FIREHOSE_STREAM_NAME, Records=batch ) if response[\u0026#39;FailedPutCount\u0026#39;] \u0026gt; 0: print(f\u0026#34;Warning: {response[\u0026#39;FailedPutCount\u0026#39;]} records failed.\u0026#34;) except Exception as e: print(f\u0026#34;Firehose API Error: {e}\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;count\u0026#34;: len(firehose_records)} "},{"uri":"https://veljg.github.io/AWS-Worklog/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in seven events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders: Gen AI and Data track\nDate \u0026amp; Time: 08:30, September 18th, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 09:00, October 3rd, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS\nDate \u0026amp; Time: 08:30, November 15th, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: AWS Cloud Mastery Series #2 - DevOps on AWS\nDate \u0026amp; Time: 08:30, November 17th, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: Secure Your Applications: AWS Perimeter Protection Workshop\nDate \u0026amp; Time: 08:30, November 19th, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 6 Event Name: AWS Well-Architected ‚Äì Security Pillar Workshop\nDate \u0026amp; Time: 08:30, November 29th, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 7 Event Name: BUILDING AGENTIC AI - Context Optimization with Amazon Bedrock\nDate \u0026amp; Time: 08:30, December 5th, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.4-monitoring-setup/","title":"Monitoring Setup","tags":[],"description":"","content":"This Monitoring Setup phase activates and configures the three core log sources for threat detection. It involves enabling CloudTrail for comprehensive management and data events, activating GuardDuty to export security findings to the primary S3 bucket, and setting up VPC Flow Logs on your network to send all traffic metadata to the dedicated CloudWatch Log Group. This ensures a constant, centralized stream of log data is available for processing and automated response.\nCreate CloudWatch Log Group Open CloudWatch Console ‚Üí Log Management ‚Üí Create log group Configure:\nLog group name: /aws/incident-response/centralized-logs Retention: 90 days KMS key: None Click \u0026ldquo;Create\u0026rdquo;\nEnable AWS CloudTrail Open CloudTrail Console ‚Üí Trail ‚Üí Create trail Trail attributes:\nTrail name: incident-responses-cloudtrail-ACCOUNT_ID-REGION Storage location: Use existing S3 bucket S3 bucket: Choose your incident-response-log-list-bucket-ACCOUNT_ID-REGION Log file SSE-KMS encryption: Disable Log file validation: Enabled Click next Choose log events:\nEvents Choose Management events, Data events Management events: All (Read + Write) Data events: S3 - Log all events Click next till step 4 and Create Trail Advanced event selectors: Exlcude log buckets:\nClick the Trail then scroll down to Data Event and click Edit Setup like picture with the under format: -arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/\n-arn:aws:s3:::processed-guardduty-findings-ACCOUNT_ID-REGION/\n-arn:aws:s3:::processed-cloudtrail-logs-ACCOUNT_ID-REGION\n-arn:aws:s3:::athena-query-results-ACCOUNT_ID-REGION/\n-arn:aws:s3:::processed-cloudwatch-logs-ACCOUNT_ID-REGION/\nSave change Enable Amazon GuardDuty Open GuardDuty Console ‚Üí Get Started ‚Üí Enable GuardDuty\nConfigure settings:\nFinding export frequency: Update CWE and S3 every 15 minutes S3 export: incident-response-log-list-bucket-ACCOUNT_ID-REGION KMS encryption: Choose or create KMS key Enable VPC Flow Logs Open VPC Console ‚Üí Your VPCs ‚Üí Select your VPC\nActions ‚Üí Create flow log\nConfigure:\nFilter: All Aggregation interval: 10 minutes Destination: CloudWatch Logs Log group: /aws/incident-response/centralized-logs IAM role: FlowLogsIAMRole Log format: Default Create flow log\nEnable VPC DNS Query Logging Configure Resolver Query Logging Open the Amazon Route 53 Console.\nIn the left navigation pane, select VPC Resolver -\u0026gt; Query logging.\nClick \u0026ldquo;Configure query logging\u0026rdquo;.\nConfigure:\nName: Enter a descriptive name, e.g., IR-DNS-Query-Log-Config. Destination for query logs: CloudWatch Logs log group Log group: Select \u0026ldquo;Existing log group\u0026rdquo; and choose: /aws/incident-response/centralized-logs Click \u0026ldquo;Configure query logging\u0026rdquo;.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Summary Report: ‚ÄúSecure Your Applications: AWS Perimeter Protection Workshop‚Äù Event Objectives Introduce CloudFront as foundation for perimeter protection Introduce WAF and Application Protection Hands On Workshop: Optimize web application with CloudFront Hands On Workshop: Secure Internet Web Application Speakers Nguyen Gia Hung - Head of Solution Architect Julian Ju - Senior Edge Services Specialist Solution Architect Kevin Lim - Senior Edge Services Specialist GTM Key Highlights CloudFront What Form Of Security Do Customers Need And The Solution Customer Personas:\nSmall website owner: Basic security protection Business users: Comprehensive security against DDoS and bots Scaling businesses: Advanced configurability like origin failover and origin load reduction Solution: CloudFront\nSecurity with predictable monthly cost: Pay flat prices for CDN, WAF, DDoS Protection, DNS and Storage Single plan selection New CloudFront Pricing: CloudFront Flat-Rate Flat pricing for CDN, WAF, DDoS Protection, DNS and Storage, pay the same price for unlimited usage\nAWS provides 4 flat pricing plans for CloudFront:\nFree: $0/Month Pro: $15/Month Business: $200/Month Premium: $1000/Month Each plan have its own usage limit per month:\nFree: 1 million requests + 100 GB Data Pro: 10 millions requests + 50 TB Data Business: 125 millions requests + 50 TB Data Premium: 500 millions requests + 50 TB Data Exceeding the usage limit =\u0026gt; No extra cost but CloudFront will reduce performance and send alerts\nHow CloudFront helps in perimeter protection Fight distributed attacks with distributed defense: Rather than defending attacks from all over the world at once, CloudFront helps by defending at edge locations nearest to the source of attacks Shield Advanced: Gains visibility into infrastructure-layer attacks, access to Shield Response Team 24/7 Volumetric attack: CloudFront offer inline mitigation, STN Proxy protect from SYN flood attack and automatic network routing Cost optimization with Amazon CloudFront AWS Data transfer out to CloudFront: Free\nHTTP compression: ~80% compression of object file\nHTTPS Encryption in transit:\nAWS proprietary TLS Automatic TLS certificate issuance and validation for no additional cost Automatic redirection from HTTP to HTTPS TLS version and cipher suite managed by security policy TLS 1.3, ECDSA cert, post quantum Upcoming mutual TLS support\nOrigin cloaking with Origin Access Control: signs request with short term credential, applies to S3 Bucket, Lambda Function URL, Elemental Media Package\nOrigin cloaking for custom origins: Only allow CloudFront IP with managed prefixes list and add custom header with a predefined secret\nContent protection with Signed URL: Prevent URL Copy Paste theft\nCaching benefits for improved availability: Cache content based on TTL, respond without request to origin and respond stale content in case of origin timeout\nBuilt-in failover and origin failover: Automatically route traffic to optimal and healthy edge location, same goes with origin when the primary fails=\u0026gt; failover to secondary origin\nGraceful failure: Support for custom error pages, stale content and cached error result\nEnhanced performance with Amazon CloudFront Multi-layer caching architecture: Aggregates traffic from CloudFront PoPs in Regional Edge Caches and Origin Shield, enable request collapsing and improve cache hit ratio\nMultiplexing: Download in parallel\nPersistent connections to origins: Avoid extra TCP handshake and maintain full TCP capacity\nAWS Global backbone: Lower Latency and no public internet event impact\nRun logic at the edge:\nRedirect at edge (CloudFront Function, Lambda at Edge) URL Rewrite / AB Testing / Geographic Redirect Device-based Redirect Device based content delivery Respond faster without origin: Rate limiting / API Mocking / Health check / Error Handling CloudFront use cases Static web resources: High cache hit ratio =\u0026gt; Performance gains + Minimal origin load\nWhole website delivery: Global performance + Security + High availability + Cost optimization\nAPI acceleration: Reuse connection + AWS backbone =\u0026gt; Reduce latency + Selective caching\nMedia Streaming: Unlimited scale + Low latency + Cost-effective + Millions of concurrent user\nLarge file download: Range requests + Edge caching + 80-90% cost savings\nCloudFront best practices End to end visibility: Real user/Internet/Infrastructure monitoring\nMaximize caching: Normalize cache key, cache dynamic/private content and error caching\nBlock unwanted requests: Malicious pattern, rate limiting, DDoS mitigation and filter API requests\nOffload business logic: CORS response, redirection at edge and set cookies\nAutomatic failover: Route 53 failover routing + CloudFront Origin Group for request level failover\nAWS WAF \u0026amp; Application Protection Threats and impact to business Denial of Service\nApp Vulnerabilities\nBot traffic\n=\u0026gt; Stolen data, compromised credentials, spam, downtime, manual intervention, raise infrastructure cost, losing credibility\nSurge in bot activities Diverse AI bots from a wide range of source significantly increased in customer environment: 155% increase YoY\nRoute 53 in perimeter protection Globally dispersed DNS servers =\u0026gt; Automatic scaling and DDoS mitigation + Availability SLA 100%\nInfrastructure Protection with AWS Shield At edge:\nSYN Proxy Inspection Packet validation Distributed scrubbing capacity Automatic Routing policy At border:\nTraffic filtering Resource-level detection and mitigation Health-based detection Inspecting HTTP with AWS WAF WAF works together with CloudFront\nDetect HTTP flood, malicious patterns, bad IP\nBot control\nFraud control\nShield Advanced Incident Response Shield/Shield Advanced has metric to trigger alarm to start incident response\nShield Advanced provides Shield Response Team 24/7\nFind attack vector and contributor\nWAF Configuration Add rules and rules group\nUse managed rules and custom rules\nStart with COUNT mode: Monitor to avoid false positives\nUse labels to customize logic and responses\nScope-down to optimize cost\nWeb ACL/Protection Pack Set of rules, rule groups and default action\nAssociated to resources like CloudFront Distribution\nLogging and sampling configurations\nWAF Rules Inspection criteria (IP Address/Header value/Request Body) =\u0026gt; Action (Allow/Block/Count/Custom)\nRate-based rule: Based on number of HTTP request from one IP\nWAF Anti-DDoS Application Layer Protection Automatic application layer DDoS mitigation\nProtection with pre-configured rules\nConfigurable DDoS Protection based on application needs\nWAF Labels Added by managed rules (Always) and custom rules (Optional): Indicate matched rule, session status, Geographic and IP-based data and Bot/Fraud activities\nCommon bots What: Self-identifying or Search Engines, Social media, HTTP library Common bot control: Looks for request, IP, TLS fingerprint and verifies the bot with labels\nEvasive bots What: Scrapers, credential stuffers, vulnerability scanners, use existing browser headers and values, not well known and mimic real common bots Solution: Client interrogation, identify unique client session and monitor its behavior\nCloudFront hands-on: Perimeter Protection Workshop Comparison between normal static S3 hosting Origin and S3 hosting with CloudFront CloudFront delivers an object faster when its served from cache\nCloudFront delivers an object faster by using the AWS global network instead of the public internet\nIn CloudFront, compression is applied, the object size is significantly reduced\nAWS WAF hands-on: Strengthen Your Web Application Defenses with AWS WAF Created WAF rules to defend against: Cross site Scripting (XSS) SQL Injection Path traversal attack Server-side asset Common/Evasive Bot API misuse Mystery Test: Configured rule to block an access with encrypted header value Event Experience The event was very informative and right on time as our team was setting up CloudFront the night before. The new pricing system is so much better for us, with additional WAF for security\nThe workshop was very interactive and fun, with Mr.Julian being as supportive as possible too We got our questions answered by Mr.Julian after the event too:\nQ: WAF can be used control and block bot activities, but lately but the improvement of AI: Especially Gemini\u0026rsquo;s new Agent, which can directly interact on our browser, not headless, can WAF help protect against those?\nA: Well with Gemini, the interractions are still called by an API so WAF can detect that, but there is another that we cant detect, Claude Agent, but its only an accessibilty tool and doesnt do many harmful things normal bots do so in the near future we wont need to concern it yet\nSome event photos Event Attendee Group Picture\nPlaced Top 24 on end of event Kahoot Quiz\nBlocked all workshop\u0026rsquo;s threats\n"},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Continue building and planning proposal Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Family matters 06/10/2025 06/10/2025 3 - Family matters 07/10/2025 07/10/2025 4 - Learnt how to create basic AWS Architecture Diagrams - Created team\u0026rsquo;s workshop architecture diagram 08/10/2025 08/10/2025 5 - Lab 35: + Successfully set up data stream using Kinesis + Successfully sent sample data to S3 using Kinesis Data Generator with Amazon Cognito + Learnt how to use AWS Glue Crawler to map data to S3 Bucket + Used Athena to query data + Used AWS Glue Notebook to build dataset based on sample data + Used Athena to analyze data and visualized with QuickSight - Updated the architecture diagram based on changes in the workshop proposal - Started researching GuardDuty to use as a component of the workshop 09/10/2025 09/10/2025 Lab 35 6 - Lab 40: + Practiced more with AWS Glue and Athena, used it to analyze AWS Monthly Cost data - School subject: + KS57: Completed Gi√°o d·ª•c v√† Ph√°t tri·ªÉn ngu·ªìn nh√¢n l·ª±c s·ªë 10/10/2025 10/10/2025 Lab 40 Gi√°o d·ª•c v√† Ph√°t tri·ªÉn ngu·ªìn nh√¢n l·ª±c s·ªë Week 5 Achievements: Proposal Development: Successfully created and updated the team\u0026rsquo;s workshop architecture diagram, learning the best practices for diagramming AWS architecture.\nData Streaming and Analytics: Completed a complex lab focused on data pipelines:\nSuccessfully set up a real-time data stream using Amazon Kinesis.\nUsed Kinesis Data Generator with Amazon Cognito to send sample data to S3.\nLearned to use AWS Glue Crawler to map data and AWS Glue Notebook to build datasets.\nUsed Amazon Athena for querying data and Amazon QuickSight for data visualization.\nPracticed advanced analytics skills by using AWS Glue and Athena to analyze AWS Monthly Cost data.\nWorkshop Research: Initiated research on Amazon GuardDuty as a component for the team\u0026rsquo;s workshop proposal.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.5-cloudwatch-autoexport/","title":"CloudWatch Autoexport Code","tags":[],"description":"","content":" import json import base64 import gzip from io import BytesIO import boto3 import os import time s3 = boto3.client(\u0026#39;s3\u0026#39;) # --- CONFIGURATION --- RAW_S3_BUCKET = os.environ.get(\u0026#34;DESTINATION_BUCKET\u0026#34;) # The log group pattern constant is no longer used for filtering, but is kept for reference. # VPC_DNS_LOG_PATTERN = \u0026#39;/aws/route53/query/\u0026#39; def is_vpc_dns_log(log_message): try: json_body = json.loads(log_message.strip()) if \u0026#39;query_name\u0026#39; in json_body and \u0026#39;query_type\u0026#39; in json_body: return True return False except Exception: return False def lambda_handler(event, context): try: compressed_payload = base64.b64decode(event[\u0026#39;awslogs\u0026#39;][\u0026#39;data\u0026#39;]) f = BytesIO(compressed_payload) decompressed_data = gzip.GzipFile(fileobj=f).read() log_data = json.loads(decompressed_data.decode(\u0026#39;utf-8\u0026#39;)) log_lines = [] for log_event in log_data.get(\u0026#39;logEvents\u0026#39;, []): log_lines.append(log_event.get(\u0026#39;message\u0026#39;, \u0026#39;\u0026#39;)) if not log_lines: print(f\u0026#34;Batch skipped: No log events found in payload. Log Group: {log_data.get(\u0026#39;logGroup\u0026#39;)}\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: \u0026#39;Log batch ignored (No events).\u0026#39;} is_dns_log = is_vpc_dns_log(log_lines[0]) if is_dns_log: key_prefix = \u0026#39;vpc-dns-logs\u0026#39; filename_prefix = \u0026#39;vpc-\u0026#39; # Add vpc- to the filename else: key_prefix = \u0026#39;vpc-flow-logs\u0026#39; filename_prefix = \u0026#39;eni-\u0026#39; # Keep filename blank for other logs output_content = \u0026#39;\\n\u0026#39;.join(log_lines) full_log_group_name = log_data.get(\u0026#39;logGroup\u0026#39;, \u0026#39;unknown-group\u0026#39;) log_group_name_safe = full_log_group_name.strip(\u0026#39;/\u0026#39;).replace(\u0026#39;/\u0026#39;, \u0026#39;_\u0026#39;) final_filename = f\u0026#34;{filename_prefix}{context.aws_request_id}.gz\u0026#34; s3_key = f\u0026#39;exportedlogs/{key_prefix}/{log_group_name_safe}/{final_filename}\u0026#39; buffer = BytesIO() with gzip.GzipFile(fileobj=buffer, mode=\u0026#39;w\u0026#39;) as gz: gz.write(output_content.encode(\u0026#39;utf-8\u0026#39;)) gzipped_data = buffer.getvalue() s3.put_object( Bucket=RAW_S3_BUCKET, Key=s3_key, Body=gzipped_data, ContentType=\u0026#39;application/x-gzip\u0026#39; ) num_logs = len(log_lines) print(f\u0026#34;Exported {num_logs} raw log lines to s3://{RAW_S3_BUCKET}/{s3_key}\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: f\u0026#39;Logs exported. {num_logs} events processed. Key Prefix: {key_prefix}\u0026#39;} except Exception as e: print(f\u0026#34;Error in CW Export Lambda: {e}\u0026#34;) raise e "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.7-dashboard-setup/5.7.5-setup-cognito/","title":"Cognito Setup","tags":[],"description":"","content":"In this guide, you will create a Cognito user pool for dashboard login.\nCreate Cognito User Pool Open the Amazon Cognito Console\nNavigate to https://console.aws.amazon.com/cognito/ Or: AWS Management Console ‚Üí Services ‚Üí Cognito Create user pool:\nClick Create user pool In user pool creation, use this setting: Application type: Single-page application (SPA) Application name: dashboard-user-pool-client Options for sign-in identifiers: Email and Username Self-registration: Enable self-registration Required attributes for sign-up: email Add a return URL: Go to Cloudfront, choose the one that you just created and copy the Distribution domain name and paste it here (Example: https://d2bvvvpr6s4eyd.cloudfront.net) Click Create user directory After create, scroll down and click Go to overview User pool App clients configuration:\nSelect App clients on the left menu panel Choose dashboard-user-pool-client In App client information section, click Edit Change the setting like the image below: Click Save change Managed login pages configuration:\nIn Managed login pages configuration section, click Edit Click Add sign-out URL at Allowed sign-out URLs section Copy the URL on the callbacks URL and paste to Allowed sign-out URLs Scroll down to OpenID Connect scopes add Profile to the scopes Click Save change Create a user:\nOn the left menu panel, select User option Click Create user Enter your user information Click Create user "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.5-processing-setup/","title":"Processing Setup","tags":[],"description":"","content":"This Processing Setup phase establishes the core data pipeline for structuring raw logs and preparing them for queryable analysis. It mandates the deployment of three Kinesis Data Firehose streams for buffering and delivering CloudTrail and VPC logs to target S3 buckets. Concurrently, you will configure the AWS Glue Database and four Athena tables via DDL to make the structured data queryable. This pipeline relies on five ETL Lambda functions triggered by S3 Event Notifications to perform the necessary data transformation upon log arrival.\nContent Create Kinesis Data Firehose Delivery Streams Create AWS Glue Database and Tables Create Lambda Functions - ETL Processing "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/","title":"Workshop","tags":[],"description":"","content":"AWS Automated Incident Response and Forensics System Setup Overview This guide provides a complete, step-by-step procedure for deploying our automated incident response and forensic system in AWS. This system leverages CloudTrail, GuardDuty, VPC Flow Logs, Kinesis Firehose, Glue, Athena, and Lambda functions orchestrated by AWS Step Functions to automatically detect, analyze, and quarantine compromised resources like EC2 instances and IAM users. Futher log forensics capacity is added by setting up a Security Dashboard hosted on S3 and accessed via CloudFront and Cognito, query log using API Gateway and Lambda.\nContent Overview Prerequisites Phase 1: Foundation Setup Phase 2: Monitoring Setup Phase 3: Processing Setup Phase 4: Automation Setup Phase 5: Dashboard Setup Verify Use CDK Cleanup Appendices "},{"uri":"https://veljg.github.io/AWS-Worklog/4-eventparticipated/4.6-event6/","title":"Event 6","tags":[],"description":"","content":"Summary Report: ‚ÄúAWS Cloud Mastery Series #3: AWS Well-Architected ‚Äì Security Pillar Workshop‚Äù Event Objectives AWS Cloud Club Introduction Pillar 1: Identity a\u0026amp; Access Management (IAM) Pillar 2: Detection \u0026amp; Continuous Monitoring Pillar 3: Infrastructure Protection Pillar 4: Data Protection Pillar 5: Incident Response Speakers Le Vu Xuan An - AWS Cloud Club Captain HCMUTE\nTran Duc Anh - AWS Cloud Club Captain SGU\nTran Doan Cong Ly - AWS Cloud Club Captain PTIT\nDanh Hoang Hieu Nghi - AWS Cloud Club Captain HUFLIT\nHuynh Hoang Long - AWS Community Builders\nDinh Le Hoang Anh - AWS Community Builders\nNguyen Tuan Thinh - Cloud Engineer Trainee\nNguyen Do Thanh Dat - Cloud Engineer Trainee\nVan Hoang Kha - Cloud Security Engineer, AWS Community Builder\nThinh Lam - FCJ Member\nViet Nguyen - FCJ Member\nMendel Grabski (Long) - Ex-Head of Security \u0026amp; DevOps, Cloud Security Solution Architect\nTinh Truong - Platform Engineer at TymeX, AWS Community Builder\nKey Highlights AWS Cloud Club An introduction to AWS Cloud Club:\nHelps explore and grow cloud computing skills Develop technical leadership Build meaningful connections globally Provides hands-on AWS Experiences, mentorship with AWS Professional and long-term career support AWS Cloud Clubs that are part of FCJA:\nAWS Cloud Club HCMUTE AWS Cloud Club SGU AWS Cloud Club PTIT AWS Cloud Club HUFLIT Benefits: Build Skills, Community and Opportunities\nIdentity \u0026amp; Access Management (IAM) IAM is an essential AWS service, responsible for controlling secure access. IAM manages Users, Groups, Roles, and Permissions, and ensures both authentication and authorization.\nBest Practices include:\nLeast Privilege Principle.\nDelete root access keys post-creation.\nAvoid \u0026ldquo;*\u0026rdquo; in Actions/Resources\nUse Single Sign-On (SSO) for multi-account integration and centralized access management.\nService Control Policies (SCPs): Organization-level policies that set the maximum available permissions for all accounts within an Organization. SCPs only filter permissions; they never grant permissions.\nPermission Boundaries: Sets the maximum permissions that an identity-based policy can grant to a specific User/Role within an account.\nMFA :\nTOTP (Time-based One-Time Password) FIDO2 (Fast Identity Online 2) Shared secret Public-key cryptography Requires manually typing a 6-digit code Requires a simple touch or biometric scan Free Variable Flexible backups and recovery Strict backups and zero recovery Credential Rotation with AWS Secrets Manager:\nCredential Updater uses Secrets Manager functions in a cycle: Create Secret, Set Secret (e.g., every 7 days) Test Secret, and Finish Secret Rotation events can be sent to an EventBridge Schedule for timing control. Finally deprecate previous Secret Detection and Continous Monitoring Multi-Layer Security Visibility:\nManagement Events:** API calls and console actions across all organization accounts. Data Events:** S3 object access and Lambda executions at scale. Network Activity Events:** VPC Flow Logs integration for network-level monitoring. Organization Coverage:** Unified logging across all member accounts and regions. Alerting \u0026amp; Automation with EventBridge\nReal-time Events: CloudTrail events flow to EventBridge for immediate processing. This is the foundation of Event-Driven Architecture (EDA), allowing systems to react to changes as they occur.\nAutomated Alerting: Detect suspicious activities across all organization accounts.\nCross-account Event Routing: Centralized event processing and automated response. EventBridge makes this seamless, routing events based on rules to targets across accounts or regions.\nIntegration \u0026amp; Workflows: Lambda, SNS, SQS integration for automated security workflows.\nDetection-as-Code:\nCloudTrail Lake Queries: Creating and using SQL-based detection rules for advanced threat hunting.\nVersion-Controlled Logic: Detection rules and logic are tracked and managed through code repositories.\nAutomated Deployment: The trails and detection rules are deployed automatically across all relevant organization accounts, ensuring uniform security coverage.\nInfrastructure-as-Code (IaC): Uses IaC tools for the automated setup and configuration of the organization\u0026rsquo;s logging and event trails\nGuard Duty GuardDuty is an Always-On, Intelligent Threat Detection solution\nHow GuardDuty Works ‚Äì Rely on continuous analysis of Three Pillars of Detection:\nData Source What It Monitors Real-World Example CloudTrail Events IAM actions, permission changes, API calls Attacker disables logging to cover tracks. VPC Flow Logs Network traffic to/from your resources EC2 sending data to a botnet C2 server. DNS Logs DNS queries from your infrastructure Malware-infected queries cryptomining sites. Advanced Protection Plans: GuardDuty offers specialized detection add-ons for complete coverage:\nS3 Protection: Detects abnormal S3 access patterns and scans malware in S3 objects at upload time.\nEKS Protection: Monitors Kubernetes audit logs for unauthorized access and chains findings with S3 to map the full attack path.\nMalware Protection: Automatically scans EBS volumes of EC2 instances when compromise is suspected.\nRDS Protection: Analyzes login activity logs to databases (Aurora/RDS) and detects brute-force attacks (multiple failed login attempts from a single IP).\nLambda Protection: Monitors network logs flowing from Lambda function invocations and detects if a compromised function sends data to malicious IPs.\nRuntime Monitoring ‚Äì Deep Inside Your OS: Achieved using a GuardDuty Agent installed on EC2/EKS/ECS Fargate. It monitors running processes, file access patterns, system calls, and attempts at privilege escalation or reverse shells.\nCompliance Standards:\nAWS Foundational Security Best Practices: Developed by AWS and covers a range of AWS services.\nCIS AWS Foundations Benchmark: developed by: AWS and industry professionals focusing on Identity (IAM), Logging \u0026amp; Monitoring, and Networking.\nCompliance Enforcement with Detection-as-Code\nIaC Tool: AWS CloudFormation is used to deploy configurations.\nCompliance Engine: AWS CloudFormation pushes configuration checks to AWS Security Hub CSPM.\nCompliance Standards Applied: Security Hub performs checks against the listed standards (AWS Foundational Security Best Practices, CIS AWS Foundations Benchmark, PCI DSS, NIST).\nResources Covered: Amazon S3, Amazon EC2, and Amazon RDS.\nNetwork Security Controls Attack Vectors: Threats are categorized into Ingress Attacks (DDoS, SQL injection), Egress Attacks (data exfiltration, DNS tunneling), and Inside Attacks (lateral movement).\nSecurity Groups (SG): Act as a Stateful firewall at the instance/interface level. They only support allow rules and have an implicit deny all.\nNetwork ACLs (NACLs): Operate at the subnet level. They are stateless and use numbered rules to explicitly ALLOW or DENY traffic.\nAWS TGW Security Group Referencing: Allows Transit Gateway (TGW) VPCs to define Inbound rules using only SG references.\nRoute 53 Resolver: Routes DNS queries to Private DNS (private hosted zones), VPC DNS, or Public DNS.\nAWS Network Firewall:\nUse Cases: Egress filtering (blocking bad domains, malicious protocols), Environment segmentation (VPC to VPC), and Intrusion prevention (IDS/IPS rules).\nActive Defense: Can automatically block malicious traffic using Amazon Threat Intelligence, where GuardDuty findings are marked for automated blocking.\nData Protection \u0026amp; Governance Encryption (KMS): Data is encrypted using a Data Key, which is protected by a Master Key (CMK). KMS policies enforce the second secure layer with Condition keys to define When encryption/decryption is allowed.\nCertificate Management (ACM): Provides free public certificates and automatically renews certificates 60 days before expiration. DNS Validation is the recommended validation method.\nSecrets Manager: Solves the problem of hardcoded credentials. It uses a 4-step Lambda logic (createSecret, setSecret, testSecret, finishSecret) for automatic credential rotation without downtime.\nAPI Service Security (S3 \u0026amp; DynamoDB): S3 requires TLS 1.2+ and bucket policies with aws:SecureTransport for enforcement. DynamoDB is secure by default with mandatory HTTPS.\nDatabase Service Security (RDS): Requires client-side trust in the AWS Root CA Bundle to verify server identity, and server-side enforcement (e.g., rds.force_ssl=1 for PostgreSQL).\nIncident Response \u0026amp; Prevention Prevention Best Practices: Key steps include using temporary credentials, never exposing S3 buckets directly, placing sensitive services behind private subnets, managing everything through Infrastructure as Code, and using double-gate high-risk changes (PR approval, pipeline deployment).\nIncident Response Process: A structured 5-step approach: Preparation, Detection \u0026amp; Analysis, Containment (isolate, revoke credentials), Eradication \u0026amp; Recovery, and Post-Incident (lessons learned).\nEvent Experience The event is extremely useful for our team, aligns directly with our project of Automated Incident Response and Forensics\nQ: Our team\u0026rsquo;s project is an Automated Incident Response and Forensics tool with Guard Duty being the main focus of our incident response, but from our testing we can see that Guard Duty can take up to 5 minutes to generate a finding when an incident occured, we want to ask are there any solutions to reduce this latency?\nA: Guard Duty takes 5 minutes to generate findings is just something you have to accept, as its the way Guard Duty is configured to work: Guard Duty have to go through a large amount of security data set to determine the exact threat and then generate a finding. If you want to reduce latency however, one of the ways you solve it is with 3rd party security services integration such as: Open Clarity Free for almost realtime findings and also you can detech anomalies and unsual user behavior with CloudTrail\nMr.Mendel Grabski was very keen to offer his support when we asked about our project after the event\nSome event photos Picture of all Attendee\nGroup Picture With Speaker Mendel Grabski and Speaker Van Hoang Kha\n"},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Complete and submit proposal Assign tasks for team member to get started on the workshop Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Reformatted and refined the worklog, adding information and summaries - Successfully deployed worklog to GitHub Pages 13/10/2025 13/10/2025 3 - Team meeting - Revised workshop proposal: Focused on using GuardDuty for intrusion detection instead of a custom Lambda function due to the need for a large dataset and extensive development time. - Redrew AWS Architecture: Added GuardDuty replacing CloudWatch Alarm - Wrote a draft of the proposal outlining basic function and providing a rough cost estimate. 14/10/2025 14/10/2025 4 - Team meeting - Revised workshop proposal: + Incorporated the use of EventBridge + Recalculated costs by reducing the EC2 instance type and active hours - Updated AWS Architecture: Included the EventBridge icon and connections 15/10/2025 15/10/2025 5 - Updated AWS Architecture: + Rearranged icons for clearer connections. + Moved SSM inside of region group + Added public subnet group for EC2 Instance - Installed AmazonQ for enhanced proposal analytics - Revised workshop proposal: Recalculated cost using AWS Pricing Calculator - Translated proposal draft into markdown code and successfully deployed it to GitHub Pages - Joined the online seminar ùóóùó´\u0026lt;ùó∂ùóªùóîùó∞ùòÅùó∂ùóºùóª\u0026gt; ùóßùóÆùóπùó∏#ùü≥: Reinventing DevSecOps with AWS Generative AI 16/10/2025 16/10/2025 6 - Compiled study materials for midterm exam - School subject: + ENW493c: Completed Being a researcher (in Information Science and Technology) 17/10/2025 17/10/2025 Being a researcher (in Information Science and Technology) Week 6 Achievements: Proposal Refinement:\nCompleted multiple revisions of the workshop proposal, shifting from a custom Lambda function to using GuardDuty for intrusion detection.\nSuccessfully recalculated and reduced estimated costs by optimizing the EC2 instance type and active hours.\nTranslated to markdown, and deployed the proposal draft to GitHub Pages.\nArchitecture and System Updates:\nRevised the AWS Architecture diagram, incorporating GuardDuty, EventBridge, and refining icon arrangements and subnet groups for clarity and accuracy.\nUpdated the worklog and successfully deployed the refined worklog to GitHub Pages.\nInstalled AmazonQ.\nAttended the online seminar \u0026lsquo;DX Talk#7: Reinventing DevSecOps with AWS Generative AI\u0026rsquo;.\nCompiled study materials for the midterm exam.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.6-automation-setup/","title":"Automation Setup","tags":[],"description":"","content":"Phase 4: Automation Setup Create Isolation Security Group EC2 Console ‚Üí Security Groups ‚Üí Create security group Name: IR-Isolation-SG Description: Denies all inbound and outbound traffic for compromised instances VPC: Select your VPC Inbound rules: None (deny all) Outbound rules: Remove default (deny all) Create and note Security Group ID (e.g., sg-0078026b70389e7b3) Create SNS Topic SNS Console ‚Üí Create topic Type: Standard, Name: IncidentResponseAlerts Access policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;events.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sns:Publish\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sns:ap-southeast-1:831981618496:IncidentResponseAlerts\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AWSEvents_IncidentResponseAlert_Target0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;events.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;SNS:Publish\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sns:ap-southeast-1:831981618496:IncidentResponseAlerts\u0026#34; } ] } Create Lambda Functions - Incident Response ir-parse-findings-lambda Handler: parse_findings.lambda_handler Role: ParseFindingsLambdaServiceRole Code: parse-findings ir-isolate-ec2-lambda Handler: isolate_ec2.lambda_handler Role: IsolateEC2LambdaServiceRole Env: ISOLATION_SG_ID=sg-XXXXXXX (from step 12) Code: isolate-ec2 ir-quarantine-iam-lambda Handler: quarantine_iam.lambda_handler Role: QuarantineIAMLambdaServiceRole Env: QUARANTINE_POLICY_ARN=arn:aws:iam::ACCOUNT_ID:policy/IrQuarantineIAMPolicy Code: quarantine-iam ir-alert-dispatch Handler: alert_dispatch.lambda_handler Role: AlertDispatchLambdaServiceRole Env: SENDER_EMAIL, RECIPIENT_EMAIL, SLACK_WEBHOOK_URL Add SNS trigger: Topic IncidentResponseAlerts Code: alert-dispatch Update SNS Topic Subscription SNS Console ‚Üí IncidentResponseAlerts ‚Üí Subscriptions Verify: Protocol=AWS Lambda, Endpoint=ir-alert-dispatch, Status=Confirmed Create Step Functions State Machine Step Functions Console ‚Üí Create state machine Type: Standard, Name: IncidentResponseStepFunctions Definition: Step Functions Definition Role: StepFunctionsRole Create Create EventBridge Rule EventBridge Console ‚Üí Rules ‚Üí Create rule Name: IncidentResponseAlert Event pattern: { \u0026#34;source\u0026#34;: [\u0026#34;aws.guardduty\u0026#34;], \u0026#34;detail-type\u0026#34;: [\u0026#34;GuardDuty Finding\u0026#34;] } Targets (2): SNS topic: IncidentResponseAlerts Step Functions: IncidentResponseStepFunctions with role IncidentResponseStepFunctionsEventRole Configure Athena Workgroup Athena Console ‚Üí Workgroups ‚Üí primary ‚Üí Edit Query result location: s3://athena-query-results-ACCOUNT_ID-REGION/ Save "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.6-parse-findings/","title":"Parse Findings Code","tags":[],"description":"","content":" import json import logging logger = logging.getLogger() logger.setLevel(logging.INFO) def lambda_handler(event, context): instance_ids = [] detail = event.get(\u0026#39;detail\u0026#39;, {}) region = event.get(\u0026#39;region\u0026#39;) or detail.get(\u0026#39;region\u0026#39;) or \u0026#39;ap-southeast-1\u0026#39; instance_id_primary = detail.get(\u0026#39;resource\u0026#39;, {}).get(\u0026#39;instanceDetails\u0026#39;, {}).get(\u0026#39;instanceId\u0026#39;) if instance_id_primary: instance_ids.append(instance_id_primary) # --- 2. Extract from the older/secondary \u0026#39;resources\u0026#39; array structure --- for r in detail.get(\u0026#34;resources\u0026#34;, []): if r.get(\u0026#34;type\u0026#34;) == \u0026#34;AwsEc2Instance\u0026#34;: id_from_details = r.get(\u0026#39;details\u0026#39;, {}).get(\u0026#39;instanceId\u0026#39;) if id_from_details: instance_ids.append(id_from_details) else: arn_id = r.get(\u0026#39;id\u0026#39;) if arn_id and arn_id.startswith(\u0026#39;arn:aws:ec2:\u0026#39;): instance_ids.append(arn_id.split(\u0026#39;/\u0026#39;)[-1]) unique_instance_ids = list(set([id for id in instance_ids if id])) return { \u0026#34;InstanceIds\u0026#34;: unique_instance_ids, \u0026#34;Region\u0026#34;: region } "},{"uri":"https://veljg.github.io/AWS-Worklog/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam from 08/09/2025 to 12/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI lead my team and participated in create the Automated Incident Response and Forensics System, through which I improved my skills in Remote leadership and working, AWS and Cloud Services as a whole, learning and coding with Python for Lambdas.\nIn terms of work ethic, I always strived to complete all my available tasks and helps others if capable, complied with workplace regulations, even not on office, I always continue to work remotely and communicate frequently.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚òê ‚úÖ ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚úÖ ‚òê ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚úÖ ‚òê ‚òê 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚úÖ ‚òê ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚úÖ ‚òê ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement While remote work was manageable, my overall productivity level was not as high as it would be in an office environment Havent communicate enough with mentors and others FCJ Members as i had wanted Need to learn how to divide workload better and not keep too much work to myself causing burnt-out, luckily my team members are very proactive and urged me to assign more tasks to them "},{"uri":"https://veljg.github.io/AWS-Worklog/4-eventparticipated/4.7-event7/","title":"Event 7","tags":[],"description":"","content":"Summary Report: ‚ÄúBUILDING AGENTIC AI - Context Optimization with Amazon Bedrock‚Äù Event Objectives Technical deep-dive into AWS Bedrock Agent Core Building Agentic Workflow on AWS Introduction to Diaflow Introduction to CloudThinker CloudThinker Agentic Orchestration, Context Optimization on Amazon Bedrock CloudThinker Hack: Hands-on Workshop Speakers Nguyen Gia Hung - Head of Solutions Architect AWS\nKien Nguyen - AWS Startup Solutions Architect\nViet Pham - CEO \u0026amp; Founder of Diaflow\nThang Ton - Co-Founder \u0026amp; COO CloudThinker\nHenry Bui - Head of Engineering CloudThinker\nVan Hoang Kha - AWS Comumnity Leaders\nNhat Tran - CTO CloudThinker\nKey Highlights Amazon Bedrock AgentCore The evolution to Agentic AI: The number of enterprise ultilizing Agentic AI systems is on the rise =\u0026gt; AWS provide the best place to build and scale AI Agents What does AWS provides?\nFrameworks for building agents: Strands, Langgraph, OpenAI, etc.. Applications: Kiro, AWS QuickSuite Amazon Sagemaker Amazon Bedrock Models and AgentCore What are Amazon Bedrock AgentCores?\nRuntime: Secure, serverless agent deployment =\u0026gt; Scale workloads and accelerate time to market Identity: Enterprise agent identity and access management =\u0026gt; Accelerate development and secure access for AI Agents and build streamlined AI Agent experiences Gateway: Unified, secure tool access =\u0026gt; Simplified development and secure access of tools, with intelligent tool discovery Memory: Intelligent context-aware memory retention =\u0026gt; Simplify memory management of enterprise-grade data with deep customization Browser: Scalable cloud browser runtime: Provides serverless browser infastructure with enterprise-grade security and observabililty Code Interpreter: Secure sandboxed code execution =\u0026gt; Execute code securely, provide large-scale data processing and ease of use Observability: Complete agent performance visibility =\u0026gt; Maintain quality of trust and accelerate time to market, allow intergration with other observability tools Diaflow: Amplify team\u0026rsquo;s potential with AI Automation Diaflow is an AI Automation Platform for Businesses whose mission is to unlock the full potential of teams by providing powerful, flexible tools to automate what matters most‚Äîfast and without code\nTransforms manual processes into intelligent workflows in minutes, not months =\u0026gt; solving 20+ hours/week/person wasted on repetitive tasks in 90% of businesses, and concerns over data privacy and security when deploying AI internally\nCustomers: 6000+ users from global customers across various industries, including Retail, IT Services, Finance, Marketing, and Healthcare.\nBacked By: Developed by experts from major tech and academic institutions and sponsored/backed by key partners like NVIDIA, Microsoft, AWS, and Google.\nLocations: The USA, Switzerland, France, South Korea, Singapore, and Vietnam.\nDiaflow Capabilities AI-Powered Automation: Reduces manual tasks by 80% by connecting Databases, Apps, Knowledge bases, APIs, and Legacy systems\nAI Agents: Deploys 24/7 intelligent assistance\nInternal AI Tools \u0026amp; Apps: Deploy AI-powered apps tailored for specific business needs.\nDiaflow Features Autonomous Task Execution: Describe your goal, Diaflow will plan and execute it\nMulti-Model Processing: Choose from Claude, Gemini, Deepseek, etc..\nIntergration with usable tools: Built-in Drive, Pages, Table and Vector\nEnterprise-Grade Security: HIPAA, SOC2 and GDPR\nCloudThinker Some problems for customers using cloud platforms: Exploding Cloud Cost, Cloud Complexity \u0026gt; Cloud Capability and Reactive Incident Response\nCloudThinker Capabilities Agentic AI Cloud Operations: Insights -\u0026gt; Reasoning -\u0026gt; Execution\nMulti-agent Collaboration Autonomous Operations Continuous Optimization Continuous Learning Multi-cloud Capability CloudThinker Product Feature Code review Incident Response Operation Automation Infastructure Observability Kubernetes Management (Coming soon) Cloud Keeper (Coming soon) CloudThinker Solutions Cloud Migration \u0026amp; Modernization Cloud Operation \u0026amp; Optimization Compliance Readiness Security Assessment CloudThinker AI Agent Team Alex: Cloud Engineer Kai: Kubenetes Engineer Anna: General Manager Oliver: Security Engineer Tony: Database Engineer CloudThinker Agentic Orchestration and Context Optimization on Amazon Bedrock AI Agent Evolution and Architecture Chatbots: Reactive, use rule-based decisions for simple tasks, and have limited adaptability.\nAgents: Proactive, capable of initiating actions, making dynamic and complex decisions, and adapting by learning from experience to handle complex, multi-step tasks.\nCore Agent Core Architecture Components:\nPlanning: Breaks down tasks and creates steps.\nMemory: Recalls context and interactions.\nTools: Uses external APIs, search, and code.\nAction: Executes the plan and provides the final response.\nGetting started with Agents ReAct Agent Architecture: Interleaving thought and action for complex tasks =\u0026gt; User Input ‚Üí Reasoning (Thought) ‚Üí Action (Tool Use) ‚Üí Observation ‚Üí Final Answer.\nEvals \u0026amp; Observability from Day 1\nUtilize tools: Effective tools have Agent Context, diverse workflows, intuitive problem solving and have real-world tests\nQuick win techniques: Prompt Caching -\u0026gt; Context Compaction -\u0026gt; Tool Consolidation -\u0026gt; Parallel tool calling\nMulti-Agent vs Multi Session: Solving Single-Agent Bottlenecks Single-Agent systems: which handle all tasks monolithically, face issues like context isolation (100k+ tokens), sequential processing, and multi-batch problems. This leads to two patterns for distributed orchestration: Multi-Agent and Multi-Session.\nAgent Coordination Models Feature Network (Peer-to-Peer) Supervisor Structure Agents communicate directly with each other. A central Supervisor delegates tasks to specialized Worker agents. Pros * Flexible communication. * Good fault tolerance. * Easier to switch from single-agent to multi-agent modes. * Clear task delegation. * Central coordination. * Modular and scalable. Cons * Complex debugging. * Ownership ambiguity. * Bottleneck risk. * Cannot switch to single-agent mode. Supervisor Variants: Three Approaches Approach Group Chat-based Supervisor Supervisor as Tool (Subagents) Hierarchical Notes Collaborative context, simple coordination, Best for small teams. CloudThinker uses this. Fine-grained control, less worker autonomy, no direct user access. Scales to large organizations, multiple supervision layers, more complex setup. Single-Agent and Multi-Agent in a Unified Architecture: Unified architecture that dynamically routes tasks to the appropriate agents: Uses a Supervisor for multi-agent coordination and retry.\nTask routing is determined by complexity and agent capability:\nSimple requests are sent directly to specialists Complex tasks requiring coordination are transferred to the Supervisor. Simple tasks that an agent can handle alone are managed by that agent. Simple tasks that require collaboration are handled via a Group Chat. A default specialist agent handles requests when no specific agent is mentioned Solving the 100:1 Agentic Cost Problem Agentic Architectures face a \u0026ldquo;Context Explosion\u0026rdquo; where the Input/Output token ratio can be 100:1, compared to roughly 3:1 for chatbots. Input costs dominate due to 50‚Äì100 tool calls per task, compounding context degradation and latency.\n=\u0026gt; Four Quick-win Techniques: combined savings of 80‚Äì95% and a 3‚Äì5x latency reduction:\nPrompt Caching:\nGoal: Achieves 70‚Äì90% cost reduction and 95%+ cache hit rates.\nMethod: Uses a Three-Tier Architecture where the System Prompt (Tier 1) and Conversation History (Tier 2) are cached, while the current turn (Tier 3) is not, preserving turn-by-turn adaptability.\nContext Compaction:\nGoal: Delivers 80% summarization cost reduction and maintains longer sessions.\nMethod: Cache-Preserving Summarization appends summarization instructions at the payload level to preserve cache keys, summarizing old messages to preserve context without hitting limits.\nTool Consolidation:\nGoal: 20% reduction in token count and hallucination, leading to a faster decision time.\nTool Pollution Problem: Too many tool definitions (e.g., 50) flood the context window.\nMethod: Consolidates CRUD operations into single interfaces with command-based parameters, using Just-In-Time Schemas to fetch documentation only when needed.\nParallel Tool Calling:\nGoal: 30‚Äì40% reduction in power round trips and a 3‚Äì5x latency reduction.\nMethod: Uses explicit instruction to parallelize tool calls and reasoning steps, which is necessary because modern LLMs can parallelize but won\u0026rsquo;t without explicit guidance.\nCross-Region Inference Problem: Single-region deployments can hit rate limits due to 50‚Äì100 tool calls per task, creating a single point of failure. =\u0026gt; Solution: Automatically routes workloads across regions (US, EU, APAC) to eliminate rate limit bottlenecks and provide a global profile for near-zero latency and improved throughput.\nThe best multi-agent system self-realizes when NOT to use multiple agents. CloudThinker Hack: Hands-On Workshop Organizers provide attendees with CloudThinker instructions and Free Standard Plan Code to use on analyzing AWS Accounts Event Experience The event was very informative and strengthened our understading of Amazon Bedrocks and introduced us to Diaflow and CloudThinker, two incredibly useful AI Agents Tools\nParticipation in CloudThinker Hack: Hands-On Workshop: Used CloudThinker to analyze our AWS Account\u0026rsquo;s infastructure =\u0026gt; Analyzed our abnormally high amount of S3 Bucket GET requests (3 millions in 2 weeks) and resolve it by recommending us to use Data Firehose to consolidate logs before saving\n=\u0026gt; Our team got a prize for using CloudThinker to analyze our problem: CloudThinker T-Shirt and CloudThinker Keychain\nSome event photos Picture of all attendees\nRecieving Prize From CloudThinker\nRecieving Prize From CloudThinker\n"},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learnt the basics of GuardDuty with \u0026ldquo;Getting Hands on with Amazon GuardDuty - AWS Virtual Workshop\u0026rdquo; - Got familiarized with GuardDuty by using Amazon Q to generate a basic lab: + Created sample finding via setting + Learnt the finding interface + Tested EC2 by port scanning scanme.nmap.org + Simulated DNS exfiltration on EC2 + GuardDuty did not alert findings from VPC Flow Logs as expected + Triggered GuardDuty findings through CloudTrail by accessing API ListPolicies with root credentials - Learnt more by doing GuardDuty workshops - Online team meeting: Assigned members to research the services to be used in the workshop 20/10/2025 20/10/2025 Getting Hands on with Amazon GuardDuty - AWS Virtual Workshop GuardDuty Workshop 3 - Successfully triggered GuardDuty sample alerts with various severities and types via CloudShell CLI =\u0026gt; Easier testing environment - Created a custom threat list of IPs and domain names for GuardDuty via CloudShell commands although it did not work 21/10/2025 21/10/2025 4 - Team meeting: + Quick AWS Services knowledge revision + Conversed about changes in the proposal - Updated AWS Architecture: Added AWS Detective - Revised proposal: + Added the usage of AWS Detective + Added plan for CDK after finishing the workshop - Mentor recommendations: + Visualize data but without using QuickSight, instead make a custom-coded dashboard (Researching) + Save GuardDuty findings in S3 bucket for analyzing (Researching) - Successfully configured EventBridge to trigger upon specific GuardDuty findings and: + Sent SNS emails to all team members + Triggered a simple Lambda script - Formulated an idea to add to workshop: Make a simple data graphing page hosted in S3 and use API Gateway and Lambda to pull forensics data from Amazon Athena (Researching) 22/10/2025 22/10/2025 5 - Tried out AWS Card Clash with team members: Surprisingly good for learning services and their functions, their placement in Architectures - Reviewed AWS Services Knowledge for Mid-Term: Using Google Gemini to generate quizzes based on the given requirements 23/10/2025 23/10/2025 AWS Card Clash 6 - Successfully configured GuardDuty threat list to trigger findings from EC2 Instance activities 24/10/2025 26/10/2025 - School subject: + KS57: Completed Ph√°p lu·∫≠t v√† ƒë·∫°o ƒë·ª©c trong c√¥ng ngh·ªá s·ªë Ph√°p lu·∫≠t v√† ƒë·∫°o ƒë·ª©c trong c√¥ng ngh·ªá s·ªë Week 7 Achievements: GuardDuty Hands-on Practice:\nCompleted the \u0026ldquo;Getting Hands on with Amazon GuardDuty - AWS Virtual Workshop\u0026rdquo; and an in-depth lab generated with Amazon Q.\nSuccessfully created, tested, and triggered various GuardDuty findings through console settings, EC2 activity, and CloudTrail API access.\nEstablished an easier testing environment by successfully triggering sample alerts with different severities and types via CloudShell CLI.\nSuccessfully configured a GuardDuty threat list to trigger findings from EC2 Instance activities.\nWorkshop Proposal and Architecture Advancement:\nUpdated the proposal and AWS Architecture to incorporate AWS Detective for further investigation capabilities.\nAdded a plan for CDK implementation following the completion of the workshop.\nInitiated research into mentor recommendations, including custom-coded data visualization and saving GuardDuty findings to S3 for analysis.\nFormulated a new workshop idea for a simple data graphing page hosted in S3 using API Gateway and Lambda.\nService Integration and Automation:\nSuccessfully configured EventBridge to act upon specific GuardDuty findings.\nAutomated notifications by sending SNS emails to team members and triggering a Lambda script based on GuardDuty alerts.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.7-dashboard-setup/","title":"Dashboard Setup","tags":[],"description":"","content":"This guide will show you how to setup the security dashboard. The security dashboard will be using S3 to contain the web files and folder, Lambda to query data using Athena, API Gateway to routing api to Lambda and Cloudfront to caching and access to the web using it\u0026rsquo;s URL.\nContent Setup S3 Setup Lambda Setup API Gateway Setup Cloudfront Setup Cognito "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.7-isolate-ec2/","title":"Isolate EC2 Code","tags":[],"description":"","content":" import json import boto3 import os from botocore.exceptions import ClientError ISOLATION_SG_ID = os.getenv(\u0026#39;ISOLATION_SG_ID\u0026#39;) def lambda_handler(event, context): print(\u0026#34;=== ISOLATE EVENT RECEIVED ===\u0026#34;) print(json.dumps(event, indent=2)) instance_id = event.get(\u0026#39;InstanceId\u0026#39;) region = event.get(\u0026#39;Region\u0026#39;, \u0026#39;ap-southeast-1\u0026#39;) if not instance_id or not ISOLATION_SG_ID: print(\u0026#34;[ERROR] Missing InstanceId or IsolationSGId in input. Cannot isolate.\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;isolation_failed\u0026#34;, \u0026#34;InstanceId\u0026#34;: instance_id, \u0026#34;error\u0026#34;: \u0026#34;Missing input data\u0026#34;} try: ec2 = boto3.client(\u0026#39;ec2\u0026#39;, region_name=region) response = ec2.describe_instances(InstanceIds=[instance_id]) instance = response[\u0026#39;Reservations\u0026#39;][0][\u0026#39;Instances\u0026#39;][0] current_sgs = [sg[\u0026#39;GroupId\u0026#39;] for sg in instance.get(\u0026#39;SecurityGroups\u0026#39;, [])] if ISOLATION_SG_ID in current_sgs: print(f\u0026#34;[INFO] {instance_id} already has isolation SG {ISOLATION_SG_ID}\u0026#34;) return { **event, \u0026#34;status\u0026#34;: \u0026#34;already_isolated\u0026#34;, \u0026#34;InstanceId\u0026#34;: instance_id, \u0026#34;Region\u0026#34;: region, \u0026#34;IsolationSG\u0026#34;: None } print(f\u0026#34;[ACTION] Isolating {instance_id} in {region} with SG {ISOLATION_SG_ID}\u0026#34;) ec2.modify_instance_attribute( InstanceId=instance_id, Groups=[ISOLATION_SG_ID] ) print(f\u0026#34;[SUCCESS] {instance_id} isolated with SG {ISOLATION_SG_ID}\u0026#34;) return { **event, \u0026#34;status\u0026#34;: \u0026#34;isolation_complete\u0026#34;, \u0026#34;InstanceId\u0026#34;: instance_id, \u0026#34;Region\u0026#34;: region, \u0026#34;IsolationSG\u0026#34;: ISOLATION_SG_ID } except ClientError as e: error_code = e.response.get(\u0026#39;Error\u0026#39;, {}).get(\u0026#39;Code\u0026#39;) print(f\u0026#34;[ERROR] Isolation FAILED for {instance_id} ({error_code}): {str(e)}\u0026#34;) return { \u0026#34;status\u0026#34;: \u0026#34;isolation_failed\u0026#34;, \u0026#34;InstanceId\u0026#34;: instance_id, \u0026#34;error\u0026#34;: str(e) } except Exception as e: print(f\u0026#34;[ERROR] Isolation FAILED (General) for {instance_id}: {str(e)}\u0026#34;) raise e "},{"uri":"https://veljg.github.io/AWS-Worklog/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is comfortable, helping me focus better. However due to the large amount of other students, chances to work on the office are small, so we mostly did our work remotely\n2. Support from Mentor / Team Admin\nThe mentors are very keen to help us with our project and questions, and organized Workshops and Events to provide important knowledge with us\n3. Relevance of Work to Academic Major\nAWS Services and other Cloud Services are almost unknown to me before applying for OJT at AWS FCJ, but the internship help me greatly learn about Cloud and how crucial it is to my academic major. My project: Automated AWS Incident Response and Forensics System was more relevant to the Information Assurance field-different from the Software Engineering field- but still it was a great learning exeperience about overall security knowledge and improve our hands-on with AWS Services\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learnt many new skills such as: Working with AWS and other Cloud Services, the Python coding language for our project, improved my leadership skill with leading the project and team member while mostly remote.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable, many of the mentors are around our age and very friendly. I appriciate my team very much, everyone is capable and willing to learn new thing, and they are proactive and do all the tasks as best they can. We also care for eachother, and check up eachother not only on tasks but overall health as around the final phase of the project, we all try our best to finish in time of the deadline.\n6. Internship Policies / Benefits\nThe company offers flexible working hours when needed, mostly they enables us to work remotely. In addition, the company introduced us to AWS Study Group and organized events about important informations in our field.\nAdditional Questions What did you find most satisfying during your internship?\nAnswer: Figuring how to work with AWS Services and configured their policies correctly in the process of building our project\nWhat do you think the company should improve for future interns?\nAnswer: More spaces on the office would be nice but it would be hard to accommodate this condition. Another thing is the midterm test where most of us underestimated the scope of the exam, not knowing it would be around the same format as a SAA Certificate Exam. If we had known such big scope, we would know how to prepare ourselves better\nIf recommending to a friend, would you suggest they intern here? Why or why not?\nAnswer: Yes but only if the friend have already found a good team and disciplined enough to do their work remotely, I have seen other OJT interns have a very hard time with the internship because they randomly choose a team with little support and low correspondence with eachother\nSuggestions \u0026amp; Expectations Would you like to continue this program in the future? Answer: Yes, my team is onboard with continuing with the program if possible "},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Review AWS knowledge. Complete FCJ Mid-Term exam. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Rewatched FCJ Bootcamp study videos - Completed the AWS Cloud Essentials Quiz - Deep dived into AWS Services previously learnt and compared similar services to each other - Checked out some AWS Well-Architected Labs to better understand each of the main pillars - Successfully exported log streams to S3 - Successfully created trail on CloudTrail to track S3 and Lambda activities - AWS Architecture: + Researched how to incorporate AWS Step Functions into the workshop\u0026rsquo;s architecture, rather than using only one Lambda for all IR actions + Considered using AWS Kinesis Data Firehose for continuous log stream to S3 27/10/2025 27/10/2025 AWS Cloud Essentials Quiz AWS Well Architected Lab 3 - Created 500 AWS Flashcards together with team members for learning 28/10/2025 28/10/2025 4 - Studied for Midterm Exam - School subject: + ENW493c: Completed Introduction to Research for Essay Writing 29/10/2025 29/10/2025 Introduction to Research for Essay Writing 5 - Practiced using AWS Certified Cloud Practitioner notes by other learners online: Did 5 practice tests - Practiced using AWS Certified Solutions Architect Associate practice questions: Practiced 40 questions 30/10/2025 30/10/2025 AWS Certified Cloud Practitioner notes AWS Certified Solutions Architect Associate practice 6 - Participated in FCJ Midterm Exam: Got in top 10 of the second exam session, got 320/650 score 31/10/2025 31/10/2025 Week 8 Achievements: FCJ Midterm Exam:\nCompleted extensive practice by taking five AWS Certified Cloud Practitioner practice tests and answering 40 AWS Certified Solutions Architect Associate practice questions.\nCollaborated with team members to create 500 AWS Flashcards for concentrated learning.\nCompleted the AWS Cloud Essentials Quiz and rewatched FCJ Bootcamp study videos.\nSuccessfully participated in the FCJ Midterm Exam and achieved a score of 320/650.\nReviewed key AWS services and the AWS Well-Architected Labs to understand the main pillars.\nWorkshop Architecture Research:\nResearched the architectural integration of AWS Step Functions to orchestrate Incident Response actions, replacing a single Lambda function.\nSuccessfully performed foundational logging tasks by exporting log streams to S3 and creating a CloudTrail to track S3 and Lambda activities.\nExplored the use of AWS Kinesis Data Firehose for continuous, reliable log streaming to S3.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.8-quarantine-iam/","title":"Quarantine IAM Code","tags":[],"description":"","content":" import json import boto3 import os QUARANTINE_POLICY_ARN = os.environ.get(\u0026#34;QUARANTINE_POLICY_ARN\u0026#34;) def lambda_handler(event, context): print(\u0026#34;=== EVENT RECEIVED ===\u0026#34;) print(json.dumps(event, indent=2)) try: finding = event.get(\u0026#39;detail\u0026#39;, {}) user_name = ( finding.get(\u0026#39;resource\u0026#39;, {}) .get(\u0026#39;accessKeyDetails\u0026#39;, {}) .get(\u0026#39;userName\u0026#39;) ) if not user_name: print(\u0026#34;[WARNING] No IAM user found in this finding. Skipping.\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;no_user\u0026#34;} print(f\u0026#34;[ACTION] Quarantining IAM User \u0026#39;{user_name}\u0026#39;...\u0026#34;) iam = boto3.client(\u0026#39;iam\u0026#39;) # Ki·ªÉm tra n·∫øu policy ƒë√£ ƒë∆∞·ª£c g√°n attached_policies = iam.list_attached_user_policies(UserName=user_name)[\u0026#39;AttachedPolicies\u0026#39;] policy_arns = [p[\u0026#39;PolicyArn\u0026#39;] for p in attached_policies] if QUARANTINE_POLICY_ARN in policy_arns: print(f\u0026#34;[INFO] Policy {QUARANTINE_POLICY_ARN} is already attached to user {user_name}.\u0026#34;) else: iam.attach_user_policy( UserName=user_name, PolicyArn=QUARANTINE_POLICY_ARN ) print(f\u0026#34;[SUCCESS] Policy attached. User {user_name} is now quarantined.\u0026#34;) except Exception as e: print(f\u0026#34;[ERROR] Failed to quarantine user: {str(e)}\u0026#34;) raise e return {\u0026#34;status\u0026#34;: \u0026#34;processed\u0026#34;, \u0026#34;action\u0026#34;: \u0026#34;iam_quarantined\u0026#34;} "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.8-verify-setup/","title":"Verify Setup","tags":[],"description":"","content":"After all the setup phase, please refer to the checklist to ensure complete resources creation\nVerify Setup Complete Verification Checklist:\nIncident Response and Forensics:\n‚úÖ S3 Buckets: All 5 created with versioning/encryption ‚úÖ IAM Roles: All 17 roles with correct policies ‚úÖ CloudTrail: Logging enabled ‚úÖ GuardDuty: Enabled with S3 export ‚úÖ VPC Flow Logs: Active ‚úÖ Lambda Functions: All 9 deployed ‚úÖ Firehose Streams: All 3 active ‚úÖ Glue Tables: All 4 created ‚úÖ S3 Events: All 4 triggers configured ‚úÖ SNS Topic: Created with subscription ‚úÖ Step Functions: Active ‚úÖ EventBridge Rule: Enabled with 2 targets Security Dashboard:\n‚úÖ S3 Buckets: Bucket is created with dashboard file stored and enabled hosting ‚úÖ Query Lambda: Lambda is created with the appropriate roles ‚úÖ API Gateway: API Gateway is created with the correct API and resources ‚úÖ CloudFront: Distribution is created with API and S3 origins configured ‚úÖ Cognito: Linked to CloudFront distribution and created user in user pool End-to-End Test\nGenerate sample GuardDuty findings: 1.1 GuardDuty Console ‚Üí Settings ‚Üí Generate sample findings (200+ findings) or 1.2 Trigger single finding via CloudShell (Dectector Id is in GuardDuty Console ‚Üí Settings ) aws guardduty create-sample-findings --detector-id [$dectector-id] --finding-types \u0026#34;Recon:EC2/PortProbeUnprotectedPort\u0026#34; Monitor workflow: Check EventBridge, SNS, Step Functions, Lambda logs Verify alerts: Check email and Slack Query data in Athena: "},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Keep working on the workshop Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - AWS Architecture revised: + Removed AWS Detective + Updated with Step Function Workflow instead of a singular AWS Lambda Function + Added Custom Dashboard: A static custom dashboard website hosted with S3 and use Athena to query from data lake - School subject: + KS57: Completed Qu·∫£n tr·ªã d·ª± √°n v√† duy tr√¨ ƒë·ªïi m·ªõi trong chuy·ªÉn ƒë·ªïi s·ªë 03/11/2025 03/11/2025 Qu·∫£n tr·ªã d·ª± √°n v√† duy tr√¨ ƒë·ªïi m·ªõi trong chuy·ªÉn ƒë·ªïi s·ªë 3 - Successfully exported GuardDuty Findings to S3 Bucket - Experimented with AWS Glue Crawler: + Failed on CloudWatch and CloudTrail logs, schema too complicated for Crawler (Have to research an alternative way) + Ran successfully on exported GuardDuty Findings: Had to update KMS Policy to allow Crawler to decrypt data - Researching ETL Pipeline for logs 04/11/2025 04/11/2025 4 - Team meetings: Progress report: + IR Workflow: Halfway done, EC2 quarantine function is finished, not tested with findings yet + Assigned task for dashboard frontend design + Assigned task for Glue ETL Pipeline research + Signed up for VPBank Cloud Day 2025 with team members 05/11/2025 05/11/2025 5 - Team meetings - Researched ETL Pipeline approach: + Instead of using Glue ETL Jobs, we use a custom Lambda ETL pipeline for CloudTrail and CloudWatch logs + Store raw logs into a Raw Log S3 Bucket then use ETL Lambda to process the data and write it to a Processed Data S3 to then be Crawled - AWS Architecture revised: Added a new group: DATA PREP group which contains the Raw Log S3 Bucket and the ETL Lambda - School subject: + ENW493c: Completed Advanced Writing 06/11/2025 06/11/2025 Advanced Writing 6 - Researched Kinesis Data Firehose to collect logs: Good for future usage, not suitable for current project because real-time streaming data was not necessary, using batch processing is better - Successfully built an ETL Pipeline for CloudTrail logs: Triggered by object creation in CloudTrail Raw Log Bucket and reformatted the raw logs into JSONL and saved it into Processed S3 - Successfully crawled and queried the processed log to show CloudTrail Events 07/11/2025 07/11/2025 Week 9 Achievements: Architecture Refinement:\nUpdated the Incident Response (IR) mechanism to use a Step Functions Workflow instead of a single Lambda function.\nIntroduced a Custom Dashboard (static website hosted on S3) that uses Athena to query the data lake.\nCreated a new DATA PREP group in the architecture, including a Raw Log S3 Bucket and an ETL Lambda to manage log transformation.\nSuccessfully configured the pipeline to export GuardDuty Findings to an S3 Bucket.\nBuilt and deployed a custom ETL Lambda pipeline** to process CloudTrail logs, triggered by new objects in the Raw Log S3 Bucket.\nSuccessfully crawled and queried the processed CloudTrail logs using Glue/Athena to demonstrate CloudTrail Events.\nTeam members completed half of the IR Step Functions Workflow, with the EC2 quarantine function being finished.\nAssigned tasks for dashboard frontend design and Glue ETL Pipeline research.\nSigned up with team members for the VPBank Cloud Day 2025 event.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.9-alert-dispatch/","title":"Alert Dispatch Code","tags":[],"description":"","content":" import os import json import logging import urllib.request import boto3 from botocore.exceptions import ClientError import html # --- Telegram ENV --- # BOT_TOKEN = os.environ.get(\u0026#39;BOT_TOKEN\u0026#39;) # CHAT_ID = os.environ.get(\u0026#39;CHAT_ID\u0026#39;) # MESSAGE_THREAD_ID = os.environ.get(\u0026#39;MESSAGE_THREAD_ID\u0026#39;) # --- Slack ENV --- SLACK_WEBHOOK_URL = os.environ.get(\u0026#34;SLACK_WEBHOOK_URL\u0026#34;) # --- SES ENV --- SENDER_EMAIL = os.environ.get(\u0026#39;SENDER_EMAIL\u0026#39;) RECIPIENT_EMAIL = os.environ.get(\u0026#39;RECIPIENT_EMAIL\u0026#39;) # Can now be \u0026#34;a@b.com, c@d.com\u0026#34; AWS_REGION = os.environ.get(\u0026#39;AWS_REGION\u0026#39;, \u0026#39;ap-southeast-1\u0026#39;) # --- Setup --- # TELEGRAM_URL = f\u0026#34;https://api.telegram.org/bot{BOT_TOKEN}/sendMessage\u0026#34; if BOT_TOKEN else None logger = logging.getLogger() logger.setLevel(logging.INFO) # Initialize SES Client ses_client = boto3.client(\u0026#39;ses\u0026#39;, region_name=AWS_REGION) # ==================================================================== # SEND TO TELEGRAM # ==================================================================== # def send_to_telegram(finding, chat_id, thread_id): # logger.info(\u0026#34;Formatting message for Telegram...\u0026#34;) # severity_num = finding.get(\u0026#39;severity\u0026#39;, 0) # if severity_num \u0026gt;= 7.0: # severity = \u0026#34;üî¥ HIGH\u0026#34; # elif severity_num \u0026gt;= 4.0: # severity = \u0026#34;üü† MEDIUM\u0026#34; # else: # severity = \u0026#34;üîµ LOW\u0026#34; # title = finding.get(\u0026#39;title\u0026#39;, \u0026#39;N/A\u0026#39;) # description = finding.get(\u0026#39;description\u0026#39;, \u0026#39;N/A\u0026#39;) # account_id = finding.get(\u0026#39;accountId\u0026#39;, \u0026#39;N/A\u0026#39;) # region = finding.get(\u0026#39;region\u0026#39;, \u0026#39;N/A\u0026#39;) # finding_type = finding.get(\u0026#39;type\u0026#39;, \u0026#39;N/A\u0026#39;) # message_text = ( # f\u0026#34;üö® *GuardDuty Finding* üö®\\n\\n\u0026#34; # f\u0026#34;*Severity:* {severity}\\n\u0026#34; # f\u0026#34;*Account:* {account_id}\\n\u0026#34; # f\u0026#34;*Region:* {region}\\n\u0026#34; # f\u0026#34;*Title:* {title}\\n\u0026#34; # f\u0026#34;*Description:* {description}\\n\\n\u0026#34; # f\u0026#34;*Finding Type:* `{finding_type}`\u0026#34; # ) # payload = {\u0026#39;chat_id\u0026#39;: chat_id, \u0026#39;text\u0026#39;: message_text, \u0026#39;parse_mode\u0026#39;: \u0026#39;Markdown\u0026#39;} # if thread_id: # payload[\u0026#39;message_thread_id\u0026#39;] = thread_id # try: # req = urllib.request.Request( # TELEGRAM_URL, # data=json.dumps(payload).encode(\u0026#39;utf-8\u0026#39;), # headers={\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} # ) # with urllib.request.urlopen(req) as response: # logger.info(\u0026#34;Telegram response: \u0026#34; + response.read().decode(\u0026#39;utf-8\u0026#39;)) # except Exception as e: # logger.error(f\u0026#34;TELEGRAM FAILED: {e}\u0026#34;) # ==================================================================== # SEND TO SLACK # ==================================================================== def send_to_slack(finding): if not SLACK_WEBHOOK_URL: logger.warning(\u0026#34;Slack ENV missing. Skipping.\u0026#34;) return severity_num = finding.get(\u0026#34;severity\u0026#34;, 0) title = finding.get(\u0026#34;title\u0026#34;, \u0026#34;No Title\u0026#34;) description = finding.get(\u0026#34;description\u0026#34;, \u0026#34;No Description\u0026#34;) region = finding.get(\u0026#34;region\u0026#34;, \u0026#34;N/A\u0026#34;) account_id = finding.get(\u0026#34;accountId\u0026#34;, \u0026#34;N/A\u0026#34;) finding_type = finding.get(\u0026#34;type\u0026#34;, \u0026#34;N/A\u0026#34;) if severity_num \u0026gt;= 7: color = \u0026#34;#ff0000\u0026#34; sev = \u0026#34;üî¥ HIGH\u0026#34; elif severity_num \u0026gt;= 4: color = \u0026#34;#ffa500\u0026#34; sev = \u0026#34;üü† MEDIUM\u0026#34; else: color = \u0026#34;#007bff\u0026#34; sev = \u0026#34;üîµ LOW\u0026#34; payload = { \u0026#34;text\u0026#34;: f\u0026#34;üö® {sev} ‚Äì {title}\u0026#34;, \u0026#34;attachments\u0026#34;: [{ \u0026#34;color\u0026#34;: color, \u0026#34;blocks\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;header\u0026#34;, \u0026#34;text\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;plain_text\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;üö® GuardDuty Finding: {title}\u0026#34;}}, {\u0026#34;type\u0026#34;: \u0026#34;section\u0026#34;, \u0026#34;fields\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Severity:*\\n{sev}\u0026#34;}, {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Region:*\\n{region}\u0026#34;} ]}, {\u0026#34;type\u0026#34;: \u0026#34;section\u0026#34;, \u0026#34;text\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Description:*\\n{description}\u0026#34;}}, {\u0026#34;type\u0026#34;: \u0026#34;divider\u0026#34;}, {\u0026#34;type\u0026#34;: \u0026#34;context\u0026#34;, \u0026#34;elements\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Account:* `{account_id}`\u0026#34;}, {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Type:* `{finding_type}`\u0026#34;} ]} ] }] } try: req = urllib.request.Request( SLACK_WEBHOOK_URL, data=json.dumps(payload).encode(\u0026#34;utf-8\u0026#34;), headers={\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} ) with urllib.request.urlopen(req) as response: logger.info(\u0026#34;Slack response: \u0026#34; + response.read().decode(\u0026#34;utf-8\u0026#34;)) except Exception as e: logger.error(f\u0026#34;SLACK FAILED: {e}\u0026#34;) # ==================================================================== # SEND TO SES EMAIL (UPDATED FOR MULTIPLE RECIPIENTS) # ==================================================================== def send_to_ses(finding): if not SENDER_EMAIL or not RECIPIENT_EMAIL: logger.warning(\u0026#34;SES Env vars missing. Skipping Email.\u0026#34;) return logger.info(\u0026#34;Formatting message for SES Email...\u0026#34;) recipient_list = [email.strip() for email in RECIPIENT_EMAIL.split(\u0026#39;,\u0026#39;)] severity_num = finding.get(\u0026#34;severity\u0026#34;, 0) title = finding.get(\u0026#34;title\u0026#34;, \u0026#34;No Title\u0026#34;) description = finding.get(\u0026#34;description\u0026#34;, \u0026#34;No Description\u0026#34;) region = finding.get(\u0026#34;region\u0026#34;, \u0026#34;N/A\u0026#34;) account_id = finding.get(\u0026#34;accountId\u0026#34;, \u0026#34;N/A\u0026#34;) finding_type = finding.get(\u0026#34;type\u0026#34;, \u0026#34;N/A\u0026#34;) finding_id = finding.get(\u0026#34;id\u0026#34;, \u0026#34;N/A\u0026#34;) if severity_num \u0026gt;= 7: color = \u0026#34;#ff0000\u0026#34; sev = \u0026#34;HIGH\u0026#34; elif severity_num \u0026gt;= 4: color = \u0026#34;#ffa500\u0026#34; sev = \u0026#34;MEDIUM\u0026#34; else: color = \u0026#34;#007bff\u0026#34; sev = \u0026#34;LOW\u0026#34; html_body = f\u0026#34;\u0026#34;\u0026#34; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;style\u0026gt; body {{ font-family: Arial, sans-serif; line-height: 1.6; color: #333; }} .container {{ width: 100%; max-width: 600px; margin: 0 auto; border: 1px solid #ddd; border-radius: 8px; overflow: hidden; }} .header {{ background-color: {color}; color: white; padding: 15px; text-align: center; }} .content {{ padding: 20px; }} .footer {{ background-color: #f4f4f4; padding: 10px; text-align: center; font-size: 12px; color: #666; }} .label {{ font-weight: bold; color: #555; }} \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;header\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;üö® GuardDuty Alert: {sev}\u0026lt;/h2\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;content\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;{title}\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;{description}\u0026lt;/p\u0026gt; \u0026lt;hr\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Account ID:\u0026lt;/span\u0026gt; {account_id}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Region:\u0026lt;/span\u0026gt; {region}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Type:\u0026lt;/span\u0026gt; {finding_type}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Finding ID:\u0026lt;/span\u0026gt; {finding_id}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;footer\u0026#34;\u0026gt; Generated by AWS Lambda Alert Dispatch \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#34;\u0026#34;\u0026#34; try: response = ses_client.send_email( Source=SENDER_EMAIL, Destination={\u0026#39;ToAddresses\u0026#39;: recipient_list}, # Uses the list now Message={ \u0026#39;Subject\u0026#39;: {\u0026#39;Data\u0026#39;: f\u0026#34;GuardDuty Alert [{sev}]: {title}\u0026#34;, \u0026#39;Charset\u0026#39;: \u0026#39;UTF-8\u0026#39;}, \u0026#39;Body\u0026#39;: {\u0026#39;Html\u0026#39;: {\u0026#39;Data\u0026#39;: html_body, \u0026#39;Charset\u0026#39;: \u0026#39;UTF-8\u0026#39;}} } ) logger.info(f\u0026#34;SES Email sent to {len(recipient_list)} recipients! MessageId: {response[\u0026#39;MessageId\u0026#39;]}\u0026#34;) except ClientError as e: logger.error(f\u0026#34;SES FAILED: {e.response[\u0026#39;Error\u0026#39;][\u0026#39;Message\u0026#39;]}\u0026#34;) # ==================================================================== # MAIN HANDLER # ==================================================================== def lambda_handler(event, context): logger.info(f\u0026#34;Event received: {json.dumps(event)}\u0026#34;) try: sns_message_raw = event[\u0026#34;Records\u0026#34;][0][\u0026#34;Sns\u0026#34;][\u0026#34;Message\u0026#34;] message_data = json.loads(sns_message_raw) # Normalization Logic finding = {} if \u0026#34;detail-type\u0026#34; in message_data and message_data[\u0026#34;detail-type\u0026#34;] == \u0026#34;GuardDuty Finding\u0026#34;: detail = message_data[\u0026#34;detail\u0026#34;] finding = { \u0026#34;severity\u0026#34;: detail.get(\u0026#34;severity\u0026#34;, 0), \u0026#34;title\u0026#34;: detail.get(\u0026#34;title\u0026#34;, \u0026#34;GuardDuty Finding\u0026#34;), \u0026#34;description\u0026#34;: detail.get(\u0026#34;description\u0026#34;, \u0026#34;No description provided\u0026#34;), \u0026#34;accountId\u0026#34;: detail.get(\u0026#34;accountId\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;region\u0026#34;: detail.get(\u0026#34;region\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;type\u0026#34;: detail.get(\u0026#34;type\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;id\u0026#34;: detail.get(\u0026#34;id\u0026#34;, \u0026#34;N/A\u0026#34;) } elif \u0026#34;AlarmName\u0026#34; in message_data: state = message_data.get(\u0026#34;NewStateValue\u0026#34;) severity = 8 if state == \u0026#34;ALARM\u0026#34; else 0 finding = { \u0026#34;severity\u0026#34;: severity, \u0026#34;title\u0026#34;: f\u0026#34;CloudWatch Alarm: {message_data.get(\u0026#39;AlarmName\u0026#39;)}\u0026#34;, \u0026#34;description\u0026#34;: message_data.get(\u0026#34;NewStateReason\u0026#34;, \u0026#34;State change detected\u0026#34;), \u0026#34;accountId\u0026#34;: message_data.get(\u0026#34;AWSAccountId\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;region\u0026#34;: message_data.get(\u0026#34;Region\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;type\u0026#34;: \u0026#34;CloudWatch Alarm\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;N/A\u0026#34; } else: finding = { \u0026#34;severity\u0026#34;: 0, \u0026#34;title\u0026#34;: \u0026#34;Unknown Alert\u0026#34;, \u0026#34;description\u0026#34;: f\u0026#34;Raw Payload: {json.dumps(message_data)}\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Unknown\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;N/A\u0026#34; } except Exception as e: logger.error(f\u0026#34;FATAL: Could not parse incoming SNS event: {e}\u0026#34;) return {\u0026#34;statusCode\u0026#34;: 500} # --- Send Telegram --- # if BOT_TOKEN and CHAT_ID: # send_to_telegram(finding, CHAT_ID, MESSAGE_THREAD_ID) # --- Send Slack --- if SLACK_WEBHOOK_URL: send_to_slack(finding) # --- Send SES Email --- if SENDER_EMAIL and RECIPIENT_EMAIL: send_to_ses(finding) return {\u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: \u0026#34;Dispatch complete\u0026#34;} "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.9-use-cdk/","title":"Use CDK","tags":[],"description":"","content":"Overview We have provided CDK stack to create all of the infrastructure required for this workshop.\nTo get the files please go to this Github Link and clone or download all the files to a folder\nSetup Guide Before deploying the CDK stack, you must configure your local environment to authenticate with your AWS account using the AWS Command Line Interface (CLI).\nInstall the AWS CLI.\nObtain Credentials: You need an Access Key ID and a Secret Access Key from an IAM user with deployment permissions.\nRun the Configuration Command: Open your terminal and execute aws configure.\n$ aws configure When prompted, enter your credentials and desired settings. The Default region name should match the region where you plan to deploy the stack (e.g., ap-southeast-1):\nPrompt Example Value AWS Access Key ID AKIA... AWS Secret Access Key wJalr... Default region name ap-southeast-1 Default output format json Verify Configuration: Test your setup by fetching your user identity. A successful output confirms you are authenticated.\n$ aws sts get-caller-identity Prerequisites Ensure the following tools and services are installed and configured on your system:\nPython 3.8+ and pip: Required for executing the CDK application and building Lambda function assets. Node.js and npm: Required for running the AWS CDK CLI and building the React dashboard. AWS CDK Toolkit: Install the CDK CLI globally: $ npm install -g aws-cdk Set Up Python Environment The infrastructure definition is written in Python. A dedicated virtual environment is used to manage project dependencies.\nCreate the Virtual Environment:\n$ python -m venv .venv Activate the Virtual Environment:\nOperating System Command macOS / Linux source .venv/bin/activate Windows (Command Prompt) .venv\\Scripts\\activate.bat Windows (PowerShell) .venv\\Scripts\\Activate.ps1 Install Python Dependencies:\n$ pip install -r requirements.txt Step to build the dashboard In the project folder location, check inside the react folder. If the dist folder already exists, you do not need to build. Otherwise, please follow the steps below. If you are on cmd use this command to move to react folder:\n$ cd react And use this command to list all content in react:\n$ ls Prerequisites Ensure you have Node.js and npm installed. You can check the current version by running:\n$ npm --version If the command is not recognized, please download and install Node.js from nodejs.org\nInstall dependencies Run the following command to install all necessary libraries:\n$ npm install Build the Project After the installation is complete, run the build command:\n$ npm run build Upon completion, a dist folder will be generated containing index.html and the assets folder.\nConfigure Deployment Context The stack utilizes context variables. These variables are read from cdk.context.json or provided via command-line flags.\nVariable Name Description Required if functionality is desired Default Value (in cdk.context.json) vpc_ids A list of VPC IDs for Flow Logs and DNS Query Logging. Yes [] alert_email A list of email addresses for alert notifications (requires SES). Yes [] sender_email The verified SES sender email address. Yes (if alert_email is set) \u0026quot;\u0026quot; slack_webhook_url The Slack webhook URL for sending alerts. No \u0026quot;\u0026quot; Example\n{ \u0026#34;vpc_ids\u0026#34;: [ \u0026#34;vpc-a1b2c3d4e5f6g7h8i\u0026#34; ], \u0026#34;alert_email\u0026#34;: [ \u0026#34;admin@example.com\u0026#34; ], \u0026#34;sender_email\u0026#34;: \u0026#34;alerts@your-domain.com\u0026#34;, \u0026#34;slack_webhook_url\u0026#34;: \u0026#34;\u0026#34; } Deploy the Stacks Before processing further, if inside the /react folder, enter this command to go back to the main folder:\n$ cd.. CDK Bootstrapping: If you have not used the AWS CDK in your target AWS account and region previously, run the bootstrap command once to provision necessary resources (e.g., S3 deployment bucket).\n$ cdk bootstrap (Optional) Synthesize and Diff: Review the proposed CloudFormation changes before deployment:\n$ cdk synth --all $ cdk diff --all Execute Deployment: Run the deployment command and approve any requested IAM security changes when prompted.\n$ cdk deploy --all The deployment is complete when the CDK CLI reports success for the stack: AwsIncidentResponseAutomationCdkStack and DashboardCdkStack\nIMPORTANT NOTE: After the deployment is complete, you should verify the email in SES. Create a user in Cognito to be able to log in to the Dashboard. Access the Security Group and remove the default outbound rule from the QuarantineSecurityGroup "},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Fully research and test all of the components and ready to add together to build the workshop Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Assisted in building ETL Pipeline for CloudWatch logs with team member - Updated proposal:\n+ Included updated Architecture and Services + Recalculated prices 10/11/2025 10/11/2025 3 - Finished building ETL Pipeline for CloudWatch logs - Fixed ETL Pipeline for CloudTrail logs: Processed logs from different dates caused schema errors due to randomized field order in struct data type - Successfully used AWS SSM to get EC2 system logs after IR Responses - Successfully integrated threat notification chatbots in Slack and Telegram - Successfully showed formatted notifications based on live threat findings Got sent over 1000 emails because team member triggered all GuardDuty sample findings combined with multiple test SNS - Team member suggested adding SES (Simple Email Service) to format emails and send 11/11/2025 11/11/2025 4 - Did research into CloudTrail Lake: Good for future usage specifically for in-depth CloudTrail log analysis, deemed unnecessary for current project due to it being CloudTrail exclusive - Updated CloudTrail ETL Lambda: promoted fields in request parameters into columns for better query and fewer schema crawling errors =\u0026gt; Reliably crawled processed data between days - Team members started on designing dashboard site, suggested integrating Grafana - Team member finished Lambda IR Functions - Got started on updating proposal to the new format 12/11/2025 12/11/2025 5 - Successfully tested using Lambda to query with Athena to prepare for API Gateway for Dashboard - Family matters 13/11/2025 13/11/2025 Lambda Athena Query Guide 6 - Crawling raw GuardDuty exported logs proved to be a bad idea, a large number of schema errors - Built a Lambda ETL Pipeline for GuardDuty logs - Revised architecture: + Directed the log from GuardDuty to the Raw Log S3 Bucket to undergo ETL Pipeline Added SES as per team member\u0026rsquo;s suggestion - Researched alternative architectures: We might be able to remove Crawler altogether, due to the custom Lambda ETL pipeline we created, we already did most of the Crawler\u0026rsquo;s service. Crawler is mostly used for large amounts of logs with various data types, except for struct data type it seems, which CloudWatch, CloudTrail and GuardDuty logs have a lot of. After formatting the logs into Parquet with custom Lambda ETL, Crawler\u0026rsquo;s purpose now is to turn it into a Catalog Table, which alternatively can be done with Lambda. Will be testing this alternative approach. - Successfully updated CloudTrail ETL Pipeline to directly call Glue API to create table without the use of Crawler - Included the use of KMS in the project due to the sensitive nature of the security logs - Joined the AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS 14/11/2025 16/11/2025 Event Summary and Experience Week 10 Achievements: Advanced ETL Pipeline Development and Optimization** Successfully completed the build of the ETL pipeline for CloudWatch logs. Resolved critical schema errors in the CloudTrail ETL pipeline to ensure reliable processing of data across different dates. Built a custom Lambda ETL pipeline for GuardDuty logs to address schema issues encountered with raw log exports. Refined the CloudTrail ETL process to bypass the Glue Crawler by having the Lambda function directly call the Glue API to create the Catalog Table. Security Tooling and Notification Integration Successfully implemented AWS Systems Manager (SSM) for the retrieval of EC2 system logs for incident response. Integrated and tested threat notification chatbots in both Slack and Telegram, successfully showing formatted notifications based on live threat findings. Added Amazon Simple Email Service (SES) to the project architecture for professional email formatting and distribution. Project Architecture, Documentation, and Security Updated the project proposal, including the revised architecture, service list, and recalculated pricing. Integrated Key Management Service (KMS) into the project to secure the sensitive security logs. Dashboard Backend Development Successfully tested a Lambda function to query with Athena, preparing for the API Gateway integration for the dashboard. Contributed to the dashboard design, suggesting the integration of Grafana. Event Participated Joined the AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.10-cleanup/","title":"Cleanup","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you have created an Automated Incident Response and Forensics System and familiarized with Lambda, Step Functions, EventBridge, Glue, Athena, CloudFront, Cognito, S3 Buckets\nCleanup Guide: Cleanup Guide for Manual Infrastructure Setup Clean Guide for CDK Setup "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.11-appendices/5.11.10-step-functions-state-machine-definition/","title":"Steps Functions Definition ASL Code","tags":[],"description":"","content":" { \u0026#34;Comment\u0026#34;: \u0026#34;Guardduty Incident Response Automation\u0026#34;, \u0026#34;StartAt\u0026#34;: \u0026#34;CheckFindingType\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;CheckFindingType\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Comment\u0026#34;: \u0026#34;Check if EC2\u0026#34;, \u0026#34;Variable\u0026#34;: \u0026#34;$.detail.resource.resourceType\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;Instance\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;ParseFindings\u0026#34; }, { \u0026#34;Comment\u0026#34;: \u0026#34;Check if IAM\u0026#34;, \u0026#34;Variable\u0026#34;: \u0026#34;$.detail.resource.resourceType\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;AccessKey\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;Quarantine_IAM_User\u0026#34; } ], \u0026#34;Default\u0026#34;: \u0026#34;NoActionNeeded\u0026#34; }, \u0026#34;ParseFindings\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::lambda:invoke\u0026#34;, \u0026#34;OutputPath\u0026#34;: \u0026#34;$.Payload\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;Payload.$\u0026#34;: \u0026#34;$\u0026#34;, \u0026#34;FunctionName\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:831981618496:function:ir-parse-findings-lambda\u0026#34; }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;Lambda.ServiceException\u0026#34;, \u0026#34;Lambda.AWSLambdaException\u0026#34;, \u0026#34;Lambda.SdkClientException\u0026#34;, \u0026#34;Lambda.TooManyRequestsException\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 1, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2, \u0026#34;JitterStrategy\u0026#34;: \u0026#34;FULL\u0026#34; } ], \u0026#34;Next\u0026#34;: \u0026#34;Isolate_EC2_Instance\u0026#34; }, \u0026#34;Isolate_EC2_Instance\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::lambda:invoke\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;FunctionName\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:831981618496:function:ir-isolate-ec2-lambda\u0026#34;, \u0026#34;Payload\u0026#34;: { \u0026#34;InstanceId.$\u0026#34;: \u0026#34;$.InstanceIds[0]\u0026#34;, \u0026#34;Region.$\u0026#34;: \u0026#34;$.Region\u0026#34; } }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;Lambda.TooManyRequestsException\u0026#34;, \u0026#34;Lambda.ServiceException\u0026#34;, \u0026#34;Lambda.AWSLambdaException\u0026#34;, \u0026#34;Lambda.SdkClientException\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 2, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2 } ], \u0026#34;Next\u0026#34;: \u0026#34;CheckIsolationStatus\u0026#34;, \u0026#34;OutputPath\u0026#34;: \u0026#34;$.Payload\u0026#34; }, \u0026#34;CheckIsolationStatus\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Variable\u0026#34;: \u0026#34;$.IsolationSG\u0026#34;, \u0026#34;IsNull\u0026#34;: true, \u0026#34;Next\u0026#34;: \u0026#34;AlreadyIsolated\u0026#34; } ], \u0026#34;Default\u0026#34;: \u0026#34;EnableTerminationProtection\u0026#34; }, \u0026#34;AlreadyIsolated\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Succeed\u0026#34; }, \u0026#34;EnableTerminationProtection\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:ec2:modifyInstanceAttribute\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;InstanceId.$\u0026#34;: \u0026#34;$.InstanceId\u0026#34;, \u0026#34;DisableApiTermination\u0026#34;: { \u0026#34;Value\u0026#34;: true } }, \u0026#34;Next\u0026#34;: \u0026#34;CreateQuarantineTag\u0026#34;, \u0026#34;ResultPath\u0026#34;: null }, \u0026#34;CreateQuarantineTag\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:ec2:createTags\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;Resources.$\u0026#34;: \u0026#34;States.Array($.InstanceId)\u0026#34;, \u0026#34;Tags\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;Quarantine\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;True\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;Security Group\u0026#34;, \u0026#34;Value.$\u0026#34;: \u0026#34;$.IsolationSG\u0026#34; } ] }, \u0026#34;Next\u0026#34;: \u0026#34;DescribeInstanceASG\u0026#34;, \u0026#34;ResultPath\u0026#34;: null }, \u0026#34;DescribeInstanceASG\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:autoscaling:describeAutoScalingInstances\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;InstanceIds.$\u0026#34;: \u0026#34;States.Array($.InstanceId)\u0026#34; }, \u0026#34;ResultPath\u0026#34;: \u0026#34;$.ASGInfo\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;CheckIfASGExists\u0026#34; }, \u0026#34;CheckIfASGExists\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Variable\u0026#34;: \u0026#34;$.ASGInfo.AutoScalingInstances[0]\u0026#34;, \u0026#34;IsPresent\u0026#34;: true, \u0026#34;Next\u0026#34;: \u0026#34;UpdateASGConfiguration\u0026#34; } ], \u0026#34;Default\u0026#34;: \u0026#34;DescribeVolumes\u0026#34; }, \u0026#34;UpdateASGConfiguration\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:autoscaling:updateAutoScalingGroup\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;AutoScalingGroupName.$\u0026#34;: \u0026#34;$.ASGInfo.AutoScalingInstances[0].AutoScalingGroupName\u0026#34;, \u0026#34;MinSize\u0026#34;: 0 }, \u0026#34;ResultPath\u0026#34;: null, \u0026#34;Next\u0026#34;: \u0026#34;Wait for ASG\u0026#34; }, \u0026#34;Wait for ASG\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Wait\u0026#34;, \u0026#34;Seconds\u0026#34;: 10, \u0026#34;Next\u0026#34;: \u0026#34;DetachFromASG\u0026#34; }, \u0026#34;DetachFromASG\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:autoscaling:detachInstances\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;AutoScalingGroupName.$\u0026#34;: \u0026#34;$.ASGInfo.AutoScalingInstances[0].AutoScalingGroupName\u0026#34;, \u0026#34;InstanceIds.$\u0026#34;: \u0026#34;States.Array($.InstanceId)\u0026#34;, \u0026#34;ShouldDecrementDesiredCapacity\u0026#34;: false }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;AutoScaling.ValidationException\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 15, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2 } ], \u0026#34;ResultPath\u0026#34;: null, \u0026#34;Next\u0026#34;: \u0026#34;DescribeVolumes\u0026#34; }, \u0026#34;DescribeVolumes\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:ec2:describeVolumes\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;Filters\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;attachment.instance-id\u0026#34;, \u0026#34;Values.$\u0026#34;: \u0026#34;States.Array($.InstanceId)\u0026#34; } ] }, \u0026#34;ResultPath\u0026#34;: \u0026#34;$.VolumeInfo\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;CreateSnapshots\u0026#34; }, \u0026#34;CreateSnapshots\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Map\u0026#34;, \u0026#34;ItemsPath\u0026#34;: \u0026#34;$.VolumeInfo.Volumes\u0026#34;, \u0026#34;MaxConcurrency\u0026#34;: 1, \u0026#34;Iterator\u0026#34;: { \u0026#34;StartAt\u0026#34;: \u0026#34;Wait before calling CreateSnapshot API\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;Wait before calling CreateSnapshot API\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Wait\u0026#34;, \u0026#34;Seconds\u0026#34;: 15, \u0026#34;Next\u0026#34;: \u0026#34;CreateSnapshot\u0026#34; }, \u0026#34;CreateSnapshot\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:ec2:createSnapshot\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;VolumeId.$\u0026#34;: \u0026#34;$.VolumeId\u0026#34;, \u0026#34;Description.$\u0026#34;: \u0026#34;States.Format(\u0026#39;IR Snapshot for {} - {}\u0026#39;, $.Attachments[0].InstanceId, $.VolumeId)\u0026#34;, \u0026#34;TagSpecifications\u0026#34;: [ { \u0026#34;ResourceType\u0026#34;: \u0026#34;snapshot\u0026#34;, \u0026#34;Tags\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;Quarantine\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;True\u0026#34; } ] } ] }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;Ec2.RequestLimitExceeded\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 60, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2 } ], \u0026#34;End\u0026#34;: true } } }, \u0026#34;End\u0026#34;: true }, \u0026#34;Quarantine_IAM_User\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Variable\u0026#34;: \u0026#34;$.detail.resource.accessKeyDetails.userType\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;Root\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;RootUserDetected\u0026#34; } ], \u0026#34;Default\u0026#34;: \u0026#34;ExecuteIAMQuarantine\u0026#34; }, \u0026#34;RootUserDetected\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Succeed\u0026#34;, \u0026#34;Comment\u0026#34;: \u0026#34;Cannot quarantine root user\u0026#34; }, \u0026#34;ExecuteIAMQuarantine\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::lambda:invoke\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;FunctionName\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:831981618496:function:ir-quarantine-iam-lambda\u0026#34;, \u0026#34;Payload.$\u0026#34;: \u0026#34;$\u0026#34; }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;Lambda.TooManyRequestsException\u0026#34;, \u0026#34;Lambda.ServiceException\u0026#34;, \u0026#34;Lambda.AWSLambdaException\u0026#34;, \u0026#34;Lambda.SdkClientException\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 2, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2 } ], \u0026#34;End\u0026#34;: true }, \u0026#34;NoActionNeeded\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Succeed\u0026#34; } } } "},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Refine project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Joined the AWS Cloud Mastery Series #2 - DevOps on AWS: Got really good values in CDK and CloudFormation for the project, got some recommendation from mentors about demo strategy, wrote about event experience 17/11/2025 17/11/2025 Event Summary and Experience 3 - Dashboard overview: Updated for more upfront fields - Architecture revised: + Removed Crawler from the architecture + Added SQS between EventBridge and StepFunctions - Took note of the cost of S3 API calls, especially the S3 Bucket for CloudTrail logs. The Lambda is configured to process every CloudTrail log, resulting in a large number of S3 GET calls, so we have to update the pricing accordingly - Team member successfully isolated EC2 in testing environment - Successfully upgraded GuardDuty ETL Lambda to create table without Crawler and return more detailed fields - Team member successfully upgraded CloudWatch ETL Lambda to create table without Crawler. I helped in creating a trigger and updating Lambda code to process new exported file Backed up Lambda codes 18/11/2025 18/11/2025 4 - Joined the Secure Your Applications: AWS Perimeter Protection Workshop: Learnt more about CloudFront and WAF and got introduced to the brand new CloudFront pricing tier, did two workshops on CloudFront and WAF - Learnt how to set up API Gateway RestAPIs to prepare to integrate with dashboard 19/11/2025 19/11/2025 Event Summary and Experience 5 - Invited back to school for Teacher\u0026rsquo;s Day - Family matters 20/11/2025 20/11/2025 6 - Invited to FPT\u0026rsquo;s Convocation Day by graduating bachelors - Researched on CDK: How to install, how to use, how to configure stacks to prepare for next week\u0026rsquo;s plan 21/11/2025 23/11/2025 AWS CDK Github AWS CDK Document Week 11 Achievements: Architecture Refinement and Cost Optimization Successfully removed the Glue Crawler service from the architecture by upgrading ETL Lambdas to directly create Glue Catalog Tables. Upgraded the GuardDuty ETL Lambda and assisted in upgrading the CloudWatch ETL Lambda to create tables without the Crawler. Added SQS (Simple Queue Service) between EventBridge and Step Functions to improve the reliability and decoupling of the workflow. Identified potential high cost associated with S3 API GET calls due to the processing of all CloudTrail logs and took note to update pricing accordingly. Incident Response (IR) and Dashboard Development Team member successfully achieved EC2 isolation in the testing environment, validating a critical IR function. Updated the dashboard overview to include more upfront and detailed fields. Learnt how to set up API Gateway RestAPIs in preparation for integration with the custom dashboard. Event Participated: Joined the AWS Cloud Mastery Series #2 - DevOps on AWS. Joined the Secure Your Applications: AWS Perimeter Protection Workshop, completing two workshops on CloudFront and WAF. "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.11-appendices/","title":"Appendices","tags":[],"description":"","content":"Appendices Lambda Codes: CloudTrail ETL GuardDuty ETL CloudWatch ETL CloudWatch ENI ETL CloudWatch Auto Export Parse Findings Isolate EC2 Instance Quarantine IAM Alert Dispatch Step Functions ASL Code: Step Functions ASL Code "},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Successfully installed AWS CDK with AWS CLI Completed the tutorial for creating a first application with CDK: + Deployed stacks on AWS Accounts + Used diff to compare changes + Destroyed stack after finishing - Created a Github Organization for the team 24/11/2025 24/11/2025 CDK Tutorial 3 - Added to IR Step Functions: Added a Map State to iterate isolated Instances and trigger SSM Lambda for those Instances to collect logs from them for forensics - Successfully helped with creating auto export CloudWatch logs: Used Lambda to parse subscription filter from log stream to Raw Log S3 bucket, will have to modify CloudWatch ETL Lambda to work with the new auto export rather than the batch export job - Updated CloudTrail ETL Lambda: Noticed the usually high storage cost in the Processed CloudTrail Log bucket =\u0026gt; The current Lambda Function saves files as unzipped .jsonl =\u0026gt; Updated the function so that the files are compressed by gzip before saving - CloudTrail ETL Lambda had some spiking and error invocations when multiple people were interacting with the account =\u0026gt; Raised timeout limit CDK: Moved the CDK testing environment to a new account - CDK: Created a Stack that enables GuardDuty and CloudTrail and the Raw Log S3 Bucket 25/11/2025 25/11/2025 4 - CDK: Updated Bucket and CloudTrail Policy to replicate the current infrastructure: got into circular dependencies but it is resolved CDK: Successfully recreated CloudTrail ETL pipeline with Raw and Processed log buckets, ETL Lambda and Glue table to be queried with Athena and set the related policies 26/11/2025 26/11/2025 5 - CDK: Configured CloudWatch, Log Group, DNS Query logging, added cdk-context for user to enter VPC ids to add logging for analysis - Optimization: CloudTrail logs have gotten too much, quick check reveals that it also logs S3 Put events from the ETL Lambdas, causing a loop =\u0026gt; Created custom event exclusion in CloudTrail Events tab to exclude APIs called by the ETL Lambdas - Exclude event by Lambda\u0026rsquo;s ARN wasn\u0026rsquo;t reliable =\u0026gt; Exclude API from log buckets - CDK: It\u0026rsquo;s impossible to configure advanced event selectors in CDK so that will have to be removed - CDK: Successfully configured the CloudWatch Auto Export Lambda and Subscription Filter: Got a lot of permission errors from Subscription Filter permission to invoke Lambda =\u0026gt; Used L2 construct and explicit dependency for _create_subscription_filter 27/11/2025 27/11/2025 6 - CDK: Added CloudWatch ETL and the related Glue Table and Processed Bucket -CDK: Added KMS Key to allow GuardDuty to export findings to S3 Bucket and added the GuardDutyETL to process the findings for querying =\u0026gt; Fully completed the ETL Pipeline and Data Forensics - Team meetings: + Assigned CDK tasks for members + Got started on updating proposal and architecture diagram - Fixed and improved IR Step Functions: + Fixed EC2Isolate Lambda: Wrong parsing method + Improved state: Added Parsing Lambda and reordered functions + SSM Failed due to missing IAM: Add role will be added into the SSM Forensics Function - Joined the AWS Cloud Mastery Series #3: AWS Well-Architected ‚Äì Security Pillar Workshop 28/11/2025 30/11/2025 Event Summary and Experience Week 12 Achievements: AWS CDK Successfully installed and learned the basics of AWS CDK and completed the introductory tutorial, including stack deployment, change comparison (diff), and destruction. Created a GitHub Organization for the team. Developed and deployed the foundational ETL infrastructure using CDK, including: Stacks for enabling GuardDuty and CloudTrail, along with the necessary Raw Log S3 Bucket. Successful recreation of the complete CloudTrail ETL pipeline (Raw/Processed buckets, ETL Lambda, Glue Table, and related policies). Configuration of CloudWatch logging components (Log Group, DNS Query logging) and the CloudWatch Auto Export Lambda using L2 construct to resolve subscription filter permission errors. Full completion of the ETL pipeline and Data Forensics by adding the CloudWatch ETL, GuardDuty ETL, related Glue Tables, and configuring the KMS Key for GuardDuty export. ETL Pipeline Optimization and Cost Reduction Updated the CloudTrail ETL Lambda to compress files using gzip before saving them to the Processed S3 Bucket, significantly reducing storage costs. Addressed spiking and error invocations in the CloudTrail ETL Lambda by raising the timeout limit. Optimized CloudTrail logging by creating custom event exclusions to prevent logging S3 Put events triggered by the ETL Lambdas, resolving a potential logging loop. Incident Response (IR) Workflow Improvement Enhanced the IR Step Functions Workflow by adding a Map State to iterate over isolated instances and trigger the SSM Lambda to collect logs for forensics. Fixed the EC2 Isolate Lambda (incorrect parsing method) and improved the overall workflow state by adding a Parsing Lambda and reordering functions. Identified and noted the need to add the correct IAM role to the SSM Forensics Function to resolve SSM failures. Assisted in the creation of the CloudWatch log auto-export mechanism via a subscription filter and Lambda. Event participated Joined the AWS Cloud Mastery Series #3: AWS Well-Architected ‚Äì Security Pillar Workshop. "},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.13-week13/","title":"Week 13 Worklog","tags":[],"description":"","content":"Week 13 Objectives: Complete the project and submit\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Removed Map State for Isolating EC2 Instance in Steps Function - Created Lambdas to add policies to EC2 Instance for SSM automation in IR Step Functions - Reconfigured Quarantine SG: Added Outbound rule for HTTPS for SSM connection - Replaced Lambdas with Step Functions provided States: Used DescribeIamInstanceProfileAssociation, AttachRolePolicy, DetachRolePolicy and StartAutomationExecution - CDK: Created EventBridge and Topics with subscription emails stored in cdk-context - Team meetings: Planned and reassigned task to meet the new deadline 01/12/2025 01/12/2025 3 - CDK: Added SES alert for GuardDuty findings - CDK: Added ENI ETL into ETL Pipeline - Assisted in upgrading dashboard Updated Event Participated and overall Worklog update fix and improvement - Researched more on how to optimize pipeline, currently the S3 Get Request is higher than expected do to Athena query low size but many objects 02/12/2025 02/12/2025 4 - CDK: Upgraded Alert with Slack - Architecture: + Researched and failed in using SQS to pool logs before sending to Lambda: Lambda is still event based and still process log individually instead of pooling + Researched and added Data Firehose to consolidate logs before writing to the processed S3 =\u0026gt; Reduced the amount of objects written to S3 - IR Step Function revised: Removed SSM actions due to it requiring outbound connections after isolating EC2 =\u0026gt; Replaced it with tagging, removing it from ASG and taking a EBS Snapshot to for analyzing and preserving data - Team member updated CloudWatch ETL with Data Firehose succesfully - CDK: Updated all of ETL Pipeline with Kinesis Firehose + Overhauled CloudTrailELT 03/12/2025 03/12/2025 5 - Partly finished writing Workshop on creating ETL Pipeline CDK: Created and updated Step Functions: Removed SSM entirely replacing it with tagging, termination protection, detachment from ASG and snapshot creation - Removed SQS between EventBridge and StepFunctions - Updated Worklog: Events 04/12/2025 04/12/2025 6 - Joined the BUILDING AGENTIC AI - Context Optimization with Amazon Bedrock Workshop: Won a prize from CloudThinker for winning in the Workshop - Updated GuardDuty ETL and table for querying optimzation - Redrew and updated Architecture Diagram Updated Proposal to the newest format - Updated Step Functions States - Overall infrastructure optimization and bug fixes 05/12/2025 07/12/2025 Event Summary and Experience Week 13 Achievements: Final Architecture Refinement and Optimization Integrated Kinesis Data Firehose into the ETL pipelines (including CloudWatch and CloudTrail) to consolidate logs before writing to S3. This redesign successfully reduced the number of objects written to S3, optimizing the pipeline and lowering future Athena query costs. Overhauled the CloudTrail ETL to work with the new Firehose configuration. Added ENI (Elastic Network Interface) ETL into the core data processing pipeline. Finalized overall infrastructure optimization and bug fixes across the project. Incident Response Workflow Overhaul Completely revised the IR Step Function to remove reliance on outbound connections (SSM), which were blocked after isolation. The new isolation workflow now focuses on robust data preservation and asset removal by: Tagging the instance. Enabling termination protection. Detaching the instance from its Auto Scaling Group (ASG). Creating an EBS Snapshot for forensic analysis. AWS CDK Finalized the CDK Stack for the Incident Response workflow, implementing the new, fully-revised Step Functions logic. Completed the CDK deployment of all alerts, adding both SES and Slack notifications for GuardDuty findings. Documentation and Workshop Material Redrew and updated the Architecture Diagram to reflect the final Kinesis Firehose integration and IR changes. Updated the Proposal to its newest and final format. Partly finished writing the Workshop documentation. Event Participated Joined the BUILDING AGENTIC AI - Context Optimization with Amazon Bedrock Workshop. Won a prize from CloudThinker during the workshop. "},{"uri":"https://veljg.github.io/AWS-Worklog/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://veljg.github.io/AWS-Worklog/tags/","title":"Tags","tags":[],"description":"","content":""}]