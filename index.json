[{"uri":"https://veljg.github.io/AWS-Worklog/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Huynh An Khuong\nPhone Number: 0964440342\nEmail: huynhankhuong0511@gmail.com\nUniversity: FPTU Ho Chi Minh Campus\nMajor: Information Technology\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect with FCJ members and mentors. Find out what working in an office is like. Install Linux, learn how to properly use Linux. Learn the basics of AWS, console and CLI. Complete first and second module. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Read internship rules - Create AWS account - Learnt what AWS is - Module 1 Lab 1 Done (Learnt how to create AWS account and manage user groups) - Module 1 Lab 7 Done (Learnt how to create budgets of using the service) - Lab 7-3 (Usage Budget) cannot be done, error in usage type dropdown, showing nothing - Module 1 Lab 9 Done (Learnt about AWS Support Services, Its type, benefits and how to request supports) 08/09/2025 08/09/2025 Create new AWS Account MFA for AWS Accounts Create Admin Group and Admin User Account Authentication Support Explore and Configure AWS Management Console Creating Support Cases and Case Management in AWS Tue - Get started on Module 2 theory: + Learnt about VPC (Amazon Virtual Private Cloud)\n+ Learnt about Subnets and Routetable, Security Groups\n+ Learnt about ENI and EIP\n+ Learnt about VPC Peering and Transit Gateway + Learnt about Elastic Load Balancing\n+ Learnt about EC2\n- Setup site for workshop report - Installed Hugo - Successfully write worklog using markdown and Hugo 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ Wed - Complete Module 2\u0026rsquo;s labs - Lab 3: + Learnt about the resources necessary to create and run EC2 instances + Successfully configured and run EC2 instances + Successfully connect and ping to EC2 instances + Created NAT Gateway to allow private EC2 connections - Lab 10: + Learnt how to create and use keypair for security + Learnt how to configure security group to manage connections + Successfully connect and use RDP via EC2 + Set up hybrid DNS with Route 53 Resolver (In progress, Cloud Formation template didn\u0026rsquo;t create security group to proceed with the lab) - Lab 19: + Successfully created VPC Peering Connection + Learnt how to configure Network ACLs + Enabled Cross-Peer DNS to resolve private host names - Downloaded and used MobaXTerm to connect to EC2 instances - Downloaded and used Putty to configures keypairs 10/09/2025 11/09/2025 Lab 3 Lab 10 Lab 19 Thu - Lab 20: + Successfully created AWS Transit Gateway to allow for connection between VPCs via a common hub • The Cloud Formation template yaml file isnt up to date, creation failed • Fixed template file, changed EC2 instance type to t3.micro - Learnt the hard way why you need to clean up resources after a lab, got charged 12$ credits - Verified the cost and budget plans worked as intended, notified over email. 11/09/2025 11/09/2025 Lab 20 Fri - Get started on Module 3 theory + Learnt about EBS, Instance store feature and check User and Meta Data + Learnt about Amazon Lightsail + Learnt about Elastic File System(EFS) and FSx + Learnt about MGN + Learnt how to use S3 Bucket on AWS - Complete Module 3\u0026rsquo;s labs - Lab 13: Successuly created Backup Plan and Vaults for data in S3 Bucket + Successfully setted up notification for Backup events + Successfully restored backup - Lab 24: + Created storage gateway + Succesfully completed file sharing - Lab 57: + Successfully hosted static website using S3 Bucket + Successfully configured access modifiers + Accelerate Static Websites with Cloudfront configuration did not work, skipping this step + Successfully created bucket versions + Moved objects between buckets + Replicated bucket across regions. 12/09/2025 12/09/2025 Lab 13 Lab 24 Lab 57 Week 1 Achievements: Created and secured an AWS account, including setting up budgets and exploring support services.\nCompleted theory and practical labs for VPC, Subnets, Security Groups, and Routetables.\nSuccessfully deployed and connected to EC2 instances, configured a NAT Gateway, and managed key connections using VPC Peering and AWS Transit Gateway.\nGained hands-on experience with S3 Buckets (static website hosting, versioning, replication), AWS Backup, and Storage Gateway.\nDocumentation Setup: Successfully installed Hugo and configured the site for writing the worklog using markdown.\nTool Proficiency: Learned to use MobaXTerm and PuTTY for connecting to and managing EC2 instances.\nSuccessfully fixed an outdated CloudFormation template during the Transit Gateway lab and learned cost management through budget alerts.\nCompleted Module 1 and Module 2, and made a strong start on Module 3.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 9 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"},{"uri":"https://veljg.github.io/AWS-Worklog/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders: Gen AI and Data track” Event Objectives Vietnam\u0026rsquo;s premier technology event, bringing together businesses, builders, and leaders to harness cloud and AI innovation. Explore the latest in generative AI, cloud technologies, and digital solutions. Gain valuable insights through keynote sessions, learn from customer success stories, participate in hands-on workshops, and discover cutting-edge solutions from Amazon Web Services (AWS) experts and partners. Speakers H.E Pham Duc Long – Vice Minister of Science and Technology H.E Marc E. Knapper – US Ambassador to Vietnam Jaime Valles – Vice President, General Manager Asia Pacific and Japan, AWS Jeff Johnson – Managing Director ASEAN, AWS Dr Jens Lottner – CEO, Techcombank Dieter Botha – CEO, TymeX Trang Phung – CEO \u0026amp; Co-Founder, U2U Network Vu Van – CEO \u0026amp; Co-Founder, ELSA Corp Nguyen Hoa Binh – Chairman, Texttech Group Taiki Dang – Solutions Architect, AWS Jun Kai Loke – AI/ML Specialist SA, AWS Kien Nguyens – Solutions Architect, AWS Tamelly Lim – Storage Specialist SA, AWS Binh Tran – Senior Solutions Architect, AWS Michael Armentano - Principal WW GTM Specialist, AWS Key Highlights AWS Customer Keynotes Building a Unified Data Foundation on AWS for AI and Analytics Workloads Constructing a unified, scalable data foundation on AWS, specifically tailored to support AI and analytics workloads Cover key components such as: Data ingestion Storage Processing Governance Ensuring that organizations can effectively manage and utilize their data for advanced analytics and AI initiatives Building the Future: Gen AI Adoption and Roadmap on AWS Showcase comprehensive vision, emerging trends, and strategic roadmap for the adoption of Generative AI (GenAI) technologies Cover key AWS services and initiatives designed to empower organizations in leveraging GenAI to drive innovation and efficiency. AI-Driven Development Lifecycle (AI-DLC) Transformative, AI-centric approach reshaping the future of software implementation by fully embedding AI as a central collaborator in the entire software development lifecycle. Securing Generative AI Applications with AWS: Fundamentals and Best Practices Explore security challenges at each layer of the generative AI stack—infrastructure, models, and applications. Showcase built-in security measures such as encryption, zero-trust architecture, continuous monitoring, and fine-grained access controls to safeguard generative AI workloads Beyond Automation: AI Agents as Your Ultimate Productivity Multipliers Paradigm shift where AI agents aren\u0026rsquo;t just tools, but intelligent partners actively driving businesses forward. Showcases upcoming AWS AI Agent: AWS Quick Suite Key Takeaways Bridging Theory with Industry Application The Critical Role of Data Pipelines in AI/ML: The sessions confirmed that sophisticated AI/ML models are entirely dependent on a robust data foundation. This involves more than just storing data; it requires well-architected pipelines for ingestion, processing, governance, and storage, which is the true engineering challenge before any model training can begin. The Shift Towards AI-Augmented Development and Asynchronous Systems: The future of software development was presented as a combination of two powerful trends. First, the AI-Driven Development Lifecycle (AI-DLC), with tools like Amazon Q Developer, is set to become a standard, augmenting developer productivity. Second, there was a strong preference for event-driven, asynchronous communication over traditional synchronous APIs to build more resilient and scalable systems. Integrating Security Throughout the Development Lifecycle: Security is not a final step but an integral part of the entire development process. The discussions covered securing the full stack—from the cloud infrastructure and the AI models to the application layer itself. Event Experience The \u0026ldquo;Vietnam Cloud Day 2025\u0026rdquo; was incredibly valuable as it provided a clear, practical context for many of the theoretical concepts I\u0026rsquo;m learning in my Computer Technology program. It bridged the gap between academic knowledge and real-world industry application.\nThe showcase of the AI-Driven Development Lifecycle was a highlight. It suggested a paradigm shift where the developer\u0026rsquo;s role evolves to focus more on architecture and complex problem-solving, while AI assistants like Amazon Q Developer handle more of the routine code generation and debugging. Learning about upcoming technologies like the AWS Quick Suite for AI agents provided a compelling look at the next wave of automation.\nI left the event with a much clearer understanding of current industry trends and a better sense of which skills to focus on to prepare for future internships and my career.\nSome event photos "},{"uri":"https://veljg.github.io/AWS-Worklog/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"AWS Partner Network (APN) Blog Đạt Được Sự Xuất Sắc trong Dịch Vụ Hậu Mãi với Syncron và AWS bởi Ankit Gupta, Angelo Malatacca, Taj Abdulahi, và Marc Cervera Castro vào ngày 10 tháng 7 năm 2025 trong Amazon Athena, Amazon EMR, Analytics, AWS Glue, Industries, Manufacturing, Partner solutions | Permalink | Bình luận | Chia sẻ\nBởi: Taj Abdulahi, Sr. Manager Product – Syncron\nBởi: Ankit Gupta, Sr. Solutions Architect – AWS\nBởi: Marc Cervera Castro, Sr. Account Manager – AWS\nBởi: Angelo Malatacca, Partner Solutions Architect – AWS\nCác nhà sản xuất thiết bị khi quản lý dịch vụ hậu mãi của họ một cách hiệu quả có thể kiếm được nhiều doanh thu hơn từ việc bán phụ tùng, tăng lợi nhuận, cải thiện kết quả dịch vụ và củng cố lòng trung thành từ cả khách hàng và các đại lý. Các nhà sản xuất chưa phát triển dịch vụ hậu mãi của mình có nguy cơ gặp phải các kết quả khách hàng kém, cạnh tranh với các đại lý về thị phần dịch vụ và tăng nguy cơ bị các nhà cung cấp chợ đen thay thế.\nCác nhà sản xuất phải đối mặt với các rào cản vận hành trong việc điều phối dịch vụ hậu mãi của họ. Sau khi bán hàng, họ phải huy động nhiều đội ngũ để hỗ trợ chức năng của thiết bị trong suốt vòng đời của nó, và đảm bảo có sẵn phụ tùng thay thế để hoàn thành bất kỳ công việc sửa chữa nào. Sự phức tạp của các hoạt động này bộc lộ những thách thức trong môi trường sản xuất ngày nay. Hoạt động cô lập xuất hiện khi các phòng ban khác nhau, đội ngũ phụ tùng, dịch vụ và bảo hành, làm việc riêng lẻ. Sự tách biệt này tạo ra rào cản cho sự cộng tác hiệu quả và dẫn đến dữ liệu bị phân mảnh trên nhiều hệ thống, gây khó khăn trong việc duy trì cái nhìn toàn diện về hoạt động dịch vụ. Các hoạt động cô lập này trực tiếp góp phần làm tăng chi phí vận hành. Khi các đội ngũ hoạt động độc lập, các quy trình trở nên trùng lặp và việc phối hợp trở nên tốn thời gian. Sự kém hiệu quả này dẫn đến việc phân bổ nguồn lực không tối ưu và hạn chế khả năng hiển thị trên chuỗi dịch vụ, cuối cùng làm tăng chi phí vận hành và ảnh hưởng đến lợi nhuận của nhà sản xuất.\nĐể giải quyết những thách thức này, Syncron cung cấp một nền tảng Service Lifecycle Management (SLM) giúp chuyển đổi cách các nhà sản xuất quản lý các hoạt động hậu mãi và tổ chức dịch vụ của họ.\nTrong bài blog này, bạn sẽ tìm hiểu cách Service Lifecycle Management (SLM) của Syncron có thể hiện đại hóa các dịch vụ hậu mãi của bạn trên AWS.\nNền tảng SLM của Syncron củng cố các hoạt động hậu mãi Syncron được thành lập 35 năm trước với mục tiêu giúp việc lưu kho và bán phụ tùng trở nên dễ dàng hơn. Hiện tại, họ có hơn 200+ khách hàng doanh nghiệp hàng đầu trên toàn cầu. Các giải pháp hậu mãi của Syncron bao gồm Giá phụ tùng, Giá hợp đồng, Lập kế hoạch kho hàng và Bảo hành.\nGần đây, họ đã thêm SLM Data Platform vào dịch vụ này, được lưu trữ trên AWS. Công cụ tập trung mới này tích hợp dữ liệu trên các hoạt động dịch vụ, phụ tùng và bảo hành (từ cả các giải pháp của Syncron và các nguồn dữ liệu bên ngoài), tạo ra một hệ sinh thái kinh doanh được kết nối hoàn toàn.\nCách tiếp cận toàn diện này điều chỉnh việc ra quyết định giữa các phòng ban, thay thế các hệ thống bị phân mảnh bằng một giải pháp thống nhất, dựa trên dữ liệu và tăng cường sự linh hoạt, cho phép các doanh nghiệp nhanh chóng phản ứng với sự thay đổi của thị trường. Hình 1 minh họa cách SLM Data Platform tích hợp dữ liệu trên giải pháp của Syncron và các nguồn dữ liệu.\nHình 1 – Service Lifecycle Management (SLM) của Syncron\nCách tiếp cận mới này trao quyền cho các đội ngũ tại hiện trường để truy cập dữ liệu ngay lập tức nhằm đưa ra các quyết định tốt hơn tại chỗ với thời gian ngừng hoạt động và sự phối hợp thấp hơn. Ở cấp độ thứ hai, nó hỗ trợ các nhà khoa học dữ liệu khám phá dữ liệu, thu thập thông tin chi tiết và xây dựng các sản phẩm dữ liệu như bảng điều khiển phân tích và mô hình AI. Điều này mở ra các trường hợp sử dụng trước đây không thể thực hiện được. Cuối cùng, các nhà quản lý và giám đốc điều hành có quyền truy cập vào các công cụ phân tích và bảng điều khiển, đảm bảo các quyết định được căn chỉnh với các mục tiêu kinh doanh dài hạn.\nCác Trường hợp Sử dụng của Khách hàng Truy cập dữ liệu hậu mãi tức thì\nVới SLM Data Platform, khách hàng có thể nhận được gấp 10 lần dữ liệu từ các ứng dụng Service Lifecycle của họ. Các nhà sản xuất truy cập ngay lập tức vào dữ liệu sạch, đã được tinh chỉnh và sẵn sàng hành động từ tất cả các hoạt động hậu mãi của họ trong một trong một giao diện quản lý thống nhất. Điều này cho phép truy cập dữ liệu có thể mở rộng và an toàn, tuân theo các hướng dẫn quản trị được thiết lập sẵn, để nhiều đội ngũ có thể xây dựng các thông tin chi tiết và mô hình học máy (ML) của riêng họ. Việc bỏ qua các bước trích xuất và điều chỉnh dữ liệu giúp tăng tốc quá trình vận hành dữ liệu, mang lại thông tin chi tiết kinh doanh nhanh hơn.\nTrung tâm Phân tích cho nhà phân tích dữ liệu\nNhờ vào việc sử dụng SLM Data Platform, khách hàng có thể định vị, kết hợp và truy vấn các bộ dữ liệu để xây dựng các data stories có thể chia sẻ. Họ có thể cung cấp những dữ liệu này làm nguồn cho các công cụ trực quan hóa dữ liệu hiện có, để thông tin được phân phối đến các đội ngũ khác nhau.\nTạo ra các sản phẩm dữ liệu\nCuối cùng, các nhà sản xuất có thể xây dựng các sản phẩm dữ liệu của riêng họ (ví dụ: mô hình ML tùy chỉnh về định giá phụ tùng hoặc tối ưu hóa kho hàng) từ tất cả dữ liệu và cung cấp chúng từ SLM Data Platform của Syncron. Tận dụng kinh nghiệm của Syncron, họ đã xác định được hơn 30 trường hợp sử dụng được xác định trước (một số được khách hàng cuối định giá hơn 1 triệu đô la Mỹ) có thể được kích hoạt và tích hợp với quy trình sản xuất hiện có.\nSolution Architecture Cốt lõi của SLM Data Platform của Syncron là một hệ sinh thái dữ liệu mạnh mẽ, hợp nhất nhiều nguồn—dữ liệu định giá, hợp đồng, lập kế hoạch phụ tùng và bảo hành—thành một khung thông minh, duy nhất. Sự hợp nhất này cho phép các doanh nghiệp biến dữ liệu thô thành thông tin chi tiết có ý nghĩa, thúc đẩy hiệu suất và lợi nhuận. Nền tảng được xây dựng trên AWS bao gồm các thành phần chính sau:\nData Landing Zone\nMột nền tảng an toàn, có khả năng mở rộng để tiếp nhận dữ liệu đa đối tượng thuê từ nhiều nguồn, được xây dựng trên Amazon Simple Storage Service (Amazon S3).. Data Landing Zone chứa cả dữ liệu có cấu trúc và phi cấu trúc. Data Product Framework\nMột cách tiếp cận có cấu trúc để tinh chỉnh và triển khai các bộ dữ liệu được điều chỉnh theo mô hình vận hành OEM của Syncron, tận dụng AWS Glue. Multi-Tenant Data.all Setup\nĐảm bảo cô lập và quản trị dữ liệu khách hàng trong khi duy trì hiệu suất vận hành bằng cách áp dụng data.all, khung phát triển mã nguồn mở của AWS giúp xây dựng một thị trường dữ liệu trên AWS. Unified Data Access\nCung cấp quyền truy cập dữ liệu theo thời gian thực, cho phép khách hàng đưa ra các quyết định có thông tin đầy đủ. Nền tảng này có các tính năng: Hình ảnh hóa dữ liệu trực quan giúp đơn giản hóa dữ liệu phức tạp, xuất dữ liệu liền mạch để tích hợp với các hệ thống bên ngoài và khả năng sử dụng AI cùng phân tích nâng cao cho thông tin chi tiết dự đoán. Hình 2 làm nổi bật kiến trúc cấp cao của SLM Data Platform của Syncron.\nHình 2 – Kiến trúc SLM Data Platform của Syncron\nKết luận Các giải pháp của Syncron trên AWS cung cấp một nền tảng mạnh mẽ để khai thác dữ liệu, thúc đẩy việc ra quyết định thông minh hơn và phối hợp giữa các phòng ban. Dù là tối ưu hóa kho hàng, định giá, hay thực hiện dịch vụ, SLM Data Platform đều cung cấp một nền tảng dữ liệu mạnh mẽ cho các trường hợp sử dụng phân tích và AI.\nĐể tìm hiểu thêm về cách nền tảng SLM của Syncron có thể phù hợp với quy trình hậu mãi của bạn, hãy nói chuyện với đội ngũ quản lý tài khoản của chúng tôi. .\nSyncron – AWS Partner Spotlight Syncron là Đối tác Công nghệ Nâng cao của AWS, giúp các nhà sản xuất hàng đầu thế giới tối đa hóa thời gian hoạt động của sản phẩm và mang lại trải nghiệm dịch vụ hậu mãi vượt trội.\nLiên hệ với Syncron | Tổng quan về Đối tác | AWS Marketplace\nTAGS: AI for Data Analytics, Industries, Manufacturing, Syncron\n"},{"uri":"https://veljg.github.io/AWS-Worklog/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"AWS DevOps \u0026amp; Developer Productivity Blog Amazon Q Developer CLI hỗ trợ đầu vào hình ảnh trong terminal của bạn bởi Keerthi Sreenivas Konjety vào ngày 21 tháng 5 2025 trong Amazon Q Developer, Announcements | Permalink | Chia sẻ\nTrong bài đăng này, tôi sẽ khám phá cách tính năng hỗ trợ hình ảnh trong Amazon Q Developer Command Line Interface (CLI) thay đổi quy trình làm việc phát triển. Q Developer CLI gần đây đã bổ sung hỗ trợ hình ảnh, mở rộng khả năng xử lý thông tin trực quan và tăng cường năng suất của nhà phát triển. Tính năng mới này cho phép các nhà phát triển tương tác trực tiếp với sơ đồ, bản thiết kế kiến trúc và các tài sản trực quan khác thông qua dòng lệnh.\nPhát triển phần mềm hiện đại ngày càng dựa vào các biểu diễn trực quan để truyền đạt ý tưởng. Ví dụ, sơ đồ kiến trúc minh họa các thành phần hệ thống và sự tương tác của chúng, trong khi sơ đồ thực thể liên kết phác thảo cấu trúc cơ sở dữ liệu. Việc chuyển đổi tài sản trực quan thành mã hoạt động thường là một quy trình giải thích và triển khai thủ công, dễ xảy ra lỗi.\nTính năng hỗ trợ hình ảnh mới trong Q Developer CLI thu hẹp khoảng cách này bằng cách cho phép các nhà phát triển cung cấp hình ảnh trực tiếp cho tác nhân Q Developer CLI để phân tích. Tôi rất hào hứng khi sử dụng tính năng này để chuyển đổi các sơ đồ kiến trúc của mình từ các ý tưởng phác thảo, vẽ tay thành các tài liệu thiết kế trau chuốt, và sau đó thành cơ sở hạ tầng dưới dạng mã. Tôi mong muốn áp dụng nó trong nhiều trường hợp sử dụng khác nhau, cho dù tôi đang bắt đầu một dự án mới hay tinh giản các quy trình làm việc hàng ngày của mình.\nTại thời điểm ra mắt, Q Developer CLI hỗ trợ các định dạng hình ảnh JPEG, PNG, WEBP và GIF, cùng với khả năng tải lên 10 hình ảnh cho mỗi yêu cầu. Bạn phải sử dụng phiên bản mới nhất (1.10.0 trở lên) của Q Developer CLI để tận hưởng tính năng hỗ trợ hình ảnh trong Q Developer CLI. Hãy sử dụng hướng dẫn này để nâng cấp hoặc cài đặt phiên bản mới nhất.\nTôi sẽ sử dụng bốn tình huống sau làm ví dụ để chứng minh lợi ích của hỗ trợ hình ảnh cho Q Developer CLI.\nTrường hợp sử dụng 1: Tạo cơ sở hạ tầng dưới dạng mã từ sơ đồ kiến trúc Sơ đồ sau mô tả một ứng dụng thay đổi kích thước hình ảnh. Nó bao gồm một bucket Amazon S3 nguồn mà người dùng tải hình ảnh lên, và một hàm AWS Lambda thay đổi kích thước hình ảnh và lưu trữ nó trong một S3 Bucket đích. Giờ đây tôi có thể chuyển đổi sơ đồ kiến trúc thành mã bằng Q Developer CLI.\nKiến trúc cho một ứng dụng thay đổi kích thước hình ảnh\nTrong ảnh chụp màn hình sau, tôi đã yêu cầu Q Developer CLI: “Vui lòng cung cấp cho tôi một mẫu terraform tham chiếu sử dụng các phương pháp hay nhất”. Lưu ý rằng việc kéo và thả hình ảnh vào CLI sẽ thêm đường dẫn vào lời nhắc của bạn. CLI với mã Terraform được tạo bởi Q Developer\nHình ảnh trên cho thấy một phần phản hồi mà Q Developer CLI đã tạo ra.\nQ Developer phản hồi bằng mẫu terraform cần thiết để bắt đầu xây dựng ứng dụng thay đổi kích thước hình ảnh. Q Developer CLI đã phân tích hình ảnh, xác định các thành phần và mối quan hệ của chúng, rồi tạo mã Terraform tương ứng. Mặc dù không hiển thị trong hình ảnh, phản hồi bao gồm mã của hàm Lambda bằng Python và quyền IAM cần thiết cho hàm Lambda.\nTrước đây, việc chuyển đổi sơ đồ này thành cơ sở hạ tầng dưới dạng mã sẽ yêu cầu tôi phải tự giải thích thủ công từng thành phần và viết cấu hình tương ứng. Với hỗ trợ hình ảnh, giờ đây tôi có thể tự động hóa phần lớn quy trình này và tinh chỉnh mã được tạo thông qua một cuộc trò chuyện với Q Developer. Sau đó, tôi có thể trò chuyện với Q Developer để tinh chỉnh mã đã tạo, đặt câu hỏi về các chi tiết triển khai cụ thể hoặc yêu cầu sửa đổi dựa trên các yêu cầu bổ sung và xuất mã sang tệp .tf.\nTrường hợp sử dụng 2: Chuyển đổi sơ đồ ER thành lược đồ cơ sở dữ liệu Đối với tình huống thứ hai, hãy xem xét một trường hợp sử dụng trong đó tôi là một phần của nhóm mô hình hóa dữ liệu đang phát triển phần mềm quản lý khóa học cho các trường đại học. Tôi đã tạo một sơ đồ thực thể liên kết (ER) cho các cấu trúc dữ liệu cốt lõi của họ. Giờ đây tôi có thể sử dụng Q Developer để giúp tôi chuyển đổi sơ đồ ER thành SQL.\nSơ đồ thực thể liên kết cho hệ thống Quản lý Khóa học\nTrong ảnh chụp màn hình sau, tôi đã yêu cầu Q Developer CLI sử dụng sơ đồ ER để tạo lược đồ cơ sở dữ liệu.\nCLI với câu lệnh của người dùng và mã SQL được tạo bởi Q Developer CLI với mã SQL được tạo bởi Q Developer\nHình ảnh trên cho thấy phản hồi mà Q Developer CLI đã tạo ra.\nQ Developer đã phân tích sơ đồ, xác định các thực thể, thuộc tính và mối quan hệ, sau đó tạo mã SQL thích hợp để tạo lược đồ cơ sở dữ liệu.\nSau khi Q Developer đưa ra kết quả, tôi có thể tinh chỉnh lược đồ này thông qua một cuộc trò chuyện với Q Developer bằng cách yêu cầu thay đổi độ dài chuỗi, chỉ mục, v.v., hoặc yêu cầu giải thích về các quyết định thiết kế.\nTrường hợp sử dụng 3: Chuyển đổi hình ảnh vẽ tay thành tài liệu thiết kế Hãy xem xét một tình huống trong đó tôi đã động não ra một ý tưởng trên giấy và tôi muốn chia sẻ ý tưởng này với nhóm của mình. Trong hình ảnh sau, tôi đã vẽ tay quy trình đặt hàng cho một trang web. Khi người dùng trang web đặt mua sách từ trang web, ứng dụng sẽ cập nhật kho hàng, sau đó gọi các hành động thanh toán và giao hàng. Giờ đây tôi có thể sử dụng Q Developer CLI để phác thảo tài liệu từ ý tưởng vẽ tay.\nSơ đồ vẽ tay của luồng quy trình đặt hàng cho một trang web\nTrong ví dụ sau, tôi đã yêu cầu Q Developer viết một tài liệu thiết kế bằng cách sử dụng hình ảnh này làm tham chiếu. CLI với câu lệnh của người dùng và phản hồi được tạo bởi Q Developer\nẢnh chụp màn hình trên cho thấy, Q Developer trước tiên đã đọc hình ảnh và hiểu nội dung từ sơ đồ vẽ tay. CLI với phản hồi được tạo bởi Q Developer\nẢnh chụp màn hình trên là một phần phản hồi mà Q Developer CLI đã tạo ra.\nQ Developer đã chuyển đổi ý tưởng thành một tài liệu thiết kế bao gồm kiến trúc hệ thống, luồng xử lý, mô hình dữ liệu, các yêu cầu chức năng, và các yêu cầu kỹ thuật. Tôi cũng có thể yêu cầu Q Developer xuất toàn bộ nội dung sang tệp .md. Điều này giảm lượng thời gian từ ý tưởng đến thực thi và tinh giản việc viết tài liệu.\nTrường hợp sử dụng 4: Xây dựng bản mô phỏng UI/wireframe từ ảnh chụp màn hình Giả sử tôi muốn bắt đầu xây dựng Giao diện Người dùng (UI) từ tài liệu thiết kế của mình trong trường hợp sử dụng 3. Tôi có thể cung cấp một hình ảnh tham chiếu cho Q Developer để tạo các wireframe ban đầu cho UI của mình.\nTrang chủ mẫu của trang web bán sách\nTrong ví dụ này, tôi đã yêu cầu Q Developer giúp tạo front-end cho một trang web mới bằng Vue.js CLI với câu lệnh của người dùng và phản hồi được tạo bởi Q Developer CLI với mã Vue.js được tạo bởi Q Developer\nHình ảnh trên cho thấy một phần mã Vue.js được tạo bởi Q Developer CLI để tái tạo front-end của trang web trong ảnh chụp màn hình. Sau khi tôi xác minh mã, tôi có thể yêu cầu Q Developer CLI tạo các tệp này cục bộ.\nCách tiếp cận này giảm các khía cạnh dễ xảy ra lỗi của việc tạo wireframe, cho phép tôi tập trung vào các quyết định thiết kế sáng tạo thay vì các tác vụ thiết lập lặp đi lặp lại. Bằng cách này, tôi có thể tăng tốc chu kỳ phát triển, đảm bảo tính nhất quán giữa các thành phần và cung cấp một nền tảng có thể dễ dàng tùy chỉnh để đáp ứng các yêu cầu dự án cụ thể.\nCác khả năng bổ sung: Ngoài các ví dụ trên, Q Developer CLI có thể phân tích nhiều loại hình ảnh, bao gồm:\nSơ đồ luồng (Flow charts) và sơ đồ quy trình Sơ đồ lớp (Class diagrams) cho thiết kế hướng đối tượng Sơ đồ cấu trúc liên kết mạng (Network topology diagrams) Ảnh chụp màn hình của thông báo lỗi hoặc trạng thái ứng dụng Tính linh hoạt này làm cho Q Developer CLI trở thành một công cụ mạnh mẽ cho các quy trình làm việc phát triển khác nhau.\nKết luận:\nViệc bổ sung hỗ trợ hình ảnh cho Amazon Q Developer CLI đại diện cho một bước tiến đáng kể trong việc thu hẹp khoảng cách giữa các biểu diễn trực quan và văn bản trong phát triển phần mềm. Bằng cách cho phép tôi làm việc với sơ đồ và các tài sản trực quan khác trực tiếp từ dòng lệnh, Amazon Q Developer cải thiện hiệu suất của tôi trong việc chuyển đổi thiết kế thành triển khai, giảm lỗi và tăng tốc các chu kỳ phát triển. Tôi khuyến khích bạn khám phá khả năng mới này và tìm hiểu cách nó có thể tăng cường quy trình làm việc phát triển của bạn.\nĐể tìm hiểu thêm về Q Developer và các khả năng của nó, hãy truy cập tài liệu.\nGiới thiệu về Tác giả: Keerthi Sreenivas Konjety\nKeerthi Sreenivas Konjety là Specialist Solutions Architect cho Amazon Q Developer, với hơn 3,5 năm kinh nghiệm về AI, ML và Kỹ thuật Dữ liệu. Chuyên môn của cô là tăng cường năng suất của nhà phát triển cho khách hàng AWS. Ngoài công việc, cô yêu thích nhiếp ảnh và sáng tạo nội dung AI.\nTAGS: Developer Tools, Development\n"},{"uri":"https://veljg.github.io/AWS-Worklog/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Blog AWS Cloud Operations Mô phỏng lỗi cục bộ với AWS Fault Injection Service bởi Ozgur Canibeyaz và Pablo Colazurdo vào ngày 30 THÁNG 6 2025 trong AWS Fault Injection Service (FIS), AWS Resilience Hub (ARH), AWS Systems Manager, Management Tools, Resilience, Technical How-to | Permalink | Chia sẻ\nCác hệ thống phân tán hiện đại phải có khả năng chống chịu lỗi trước các gián đoạn không mong muốn để duy trì tính khả dụng, hiệu suất và độ ổn định. Chaos engineering giúp các nhóm phát hiện những điểm yếu tiềm ẩn bằng cách thực hiện fault injection lên hệ thống và quan sát cách nó phục hồi. Trong khi thử nghiệm truyền thống xác thực hành vi mong đợi, chaos engineering kiểm tra khả năng phục hồi của hệ thống trong suốt thời gian xảy ra lỗi. AWS Fault Injection Service (AWS FIS) là một dịch vụ AWS được quản lý toàn diện giúp các nhóm chạy các thí nghiệm fault injection trên các workloads của AWS. Nó hỗ trợ các tình huống như chấm dứt Amazon EC2 instances, điều tiết các Amazon API Gateway requests, và tạo độ trễ mạng. Điều này cho phép bạn xác thực khả năng phục hồi trong các môi trường giống như môi trường sản xuất. Mặc dù những khả năng này rất mạnh mẽ, nhiều lỗi thực tế chỉ ảnh hưởng đến một phần lưu lượng truy cập.\nTrong bài đăng này, bạn sẽ tìm hiểu cách mô phỏng lỗi cục bộ. Một kiểu lỗi phổ biến nhưng ít được kiểm tra—bằng cách kết hợp AWS FIS với weighted routing trong Application Load Balancer (ALB) và một hàm AWS Lambda trả về các phản hồi lỗi tùy chỉnh. Phương pháp này cho phép bạn kiểm tra cách ứng dụng của bạn xử lý các điều kiện suy giảm mà không cần thay đổi mã hoặc làm gián đoạn luồng truy cập thông thường.\nTổng quan về giải pháp Giải pháp của chúng tôi kết hợp AWS FIS với định tuyến có trọng số của ALB để điều hướng một tỷ lệ phần trăm lưu lượng truy cập có thể định cấu hình đến một hàm Lambda có thể trả về các lỗi mô phỏng. Cách tiếp cận này không yêu cầu thay đổi mã nguồn ứng dụng và sẽ tự động khôi phục về hoạt động bình thường sau khi kiểm thử. Hình 1 Giải pháp này cho thấy cách sửa đổi Load Balancer của bạn một cách an toàn để mô phỏng lỗi trong quá trình thực hiện thử nghiệm và khôi phục an toàn sau khi kết thúc\nLợi ích chính Giải pháp này cung cấp các lợi ích chính sau cho các nhóm triển khai kỹ thuật hỗn loạn:\nMô phỏng lỗi có kiểm soát. Không cần sửa đổi ứng dụng. Thiết lập và khôi phục tự động. Tỷ lệ lỗi có thể định cấu hình. Hướng dẫn triển khai Điều kiện tiên quyết Trước khi bắt đầu, hãy xác minh bạn có:\nMột tài khoản AWS với quyền triển khai các stack AWS CloudFormation và quản lý các thí nghiệm AWS FIS. Một ALB hiện có được cấu hình với một target group định tuyến lưu lượng truy cập đến một microservice đang chạy. ALB phải đã hoạt động và có thể truy cập công khai để kiểm tra các lỗi mô phỏng. Truy cập vào AWS Command Line Interface(AWS CLI) hoặc AWS Management Console. Bước 1: Triển khai mẫu CloudFormation Mẫu CloudFormation thiết lập tất cả các tài nguyên cần thiết, bao gồm:\nMột hàm Lambda để mô phỏng các phản hồi lỗi. Một tài liệu AWS Systems Manager (SSM) automation. Một IAM Role cấp quyền cho AWS FIS để gọi tài liệu SSM Automation. Một AWS FIS experiment template được cấu hình sẵn. Hình 2 Cái nhìn cấp cao về các thành phần giải pháp và sự tương tác của chúng.\nCác tham số thí nghiệm có thể định cấu hình Mẫu CloudFormation yêu cầu ba tham số sau khi triển khai:\nTên Application Load Balancer. ARN của Listener ALB rule cần sửa đổi. Thời lượng kiểm tra bằng giây — lỗi cục bộ nên kéo dài bao lâu. Các cài đặt thí nghiệm khác, chẳng hạn như tỷ lệ phần trăm lưu lượng truy cập cần chuyển hướng và mã phản hồi Lambda, được cấu hình sẵn trong định nghĩa thí nghiệm. Nếu bạn muốn tùy chỉnh các giá trị này, bạn có hai tùy chọn:\nTùy chọn 1: Sửa đổi Mẫu CloudFormation và triển khai lại\nBạn có thể chỉnh sửa trường documentParameters trong phần định nghĩa thí nghiệm của mẫu để thay đổi:\nFailurePercentage (ví dụ: 10, 50, 100). Để thay đổi HTTP status code được hàm Lambda trả về (ví dụ: từ 500 thành 404), hãy sửa đổi giá trị statusCode trực tiếp trong khối mã nội tuyến bên trong mẫu.\nSau khi chỉnh sửa, hãy triển khai lại stack để áp dụng các thay đổi của bạn.\nTùy chọn 2: Tạo phiên bản mới của tài liệu SSM Automation\nNếu bạn không muốn triển khai lại stack:\nTruy cập vào AWS Systems Manager → Documents console. Định vị tài liệu SSM được tạo bởi mẫu. Chọn Create new version và điều chỉnh các giá trị mặc định như FailurePercentage. Sử dụng phiên bản đã cập nhật bằng cách tham chiếu nó trong một thí nghiệm AWS FIS mới (thông qua CLI hoặc console). IAM Permissions:\nBạn cần có quyền tạo các IAM role và policy khi triển khai CloudFormation template. Khi triển khai thông qua AWS Management Console, bạn sẽ cần xác nhận rằng template tạo ra các tài nguyên IAM. Nếu sử dụng AWS CLI, hãy thêm flag --capabilities CAPABILITY_NAMED_IAM.\nTải xuống template: Bạn có thể tải xuống CloudFormation template tại đây và lưu cục bộ dưới dạng fis_template.yaml trước khi triển khai nó thông qua AWS Console hoặc CLI.\naws cloudformation create-stack --stack-name alb-fis-experiment \\ --template-body file://fis_template.yaml \\ --parameters \\ ParameterKey=LoadBalancerName,ParameterValue=LoadBalancerName \\ ParameterKey=ListenerRuleArn,ParameterValue=RuleARN \\ ParameterKey=TestDurationInSeconds,ParameterValue=60 \\ --capabilities CAPABILITY_NAMED_IAM LoadBalancerName và RuleARN đề cập đến tên Load Balancer và ARN đầy đủ của Listener rule trước dịch vụ mà bạn muốn mô phỏng lỗi. 60 chỉ định thời lượng của lỗi mô phỏng bằng giây.\nLưu ý: FISExperimentRole IAM policy sử dụng \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; cho các hành động nhất định để cho phép AWS FIS sửa đổi các tài nguyên Load Balancer được tạo động. Bởi vì tên tài nguyên như ARN của target group không được biết tại thời điểm triển khai, nên việc giới hạn phạm vi các quyền này là không khả thi trong ngữ cảnh của bài đăng này. Mặc dù điều này mang lại sự linh hoạt, AWS security best practices khuyến nghị giới hạn phạm vi quyền đối với các tài nguyên cụ thể bất cứ khi nào có thể. Nếu bạn biết chính xác các tài nguyên sẽ được sử dụng, hãy cân nhắc cập nhật chính sách để hạn chế quyền truy cập cho phù hợp.\nBước 2: Xác minh Hàm Lambda Sau khi triển khai, hãy kiểm tra hàm Lambda trong AWS console để xác nhận nó trả về phản hồi lỗi mong đợi. Hàm phải trả về nội dung như sau:\n{ \u0026#34;statusCode\u0026#34;: 503, \u0026#34;body\u0026#34;: \u0026#34;Service Unavailable - Simulated Error Response\u0026#34; } Bước 3: Bắt đầu Thí nghiệm AWS FIS Mở AWS Fault Injection Service console. Định vị template được cấu hình sẵn trong Experiment Templates. Chọn Start experiment. Xác nhận và khởi chạy kiểm tra. Hình 3 AWS FIS console hiển thị experiment template tùy chỉnh được tạo bởi mẫu CloudFormation.\nKhi bạn bắt đầu thí nghiệm, AWS FIS sẽ gọi một AWS Systems Manager Automation Document được tạo trong quá trình triển khai. Automation này thực hiện các hành động sau:\nTạo một ALB target group mới trỏ đến một hàm Lambda được cấu hình để trả về các error responses mô phỏng. Sửa đổi một ALB Listener rule để chia một phần lưu lượng truy cập đến target group mới này, mô phỏng hiệu quả một lỗi cục bộ. Đợi trong một khoảng thời gian xác định (có thể định cấu hình thông qua CloudFormation template). Khôi phục ALB listener rule về trạng thái ban đầu và xóa target group tạm thời. Toàn bộ vòng đời này được tự động hóa — bạn không cần viết bất kỳ mã nào hoặc thực hiện cập nhật thủ công nào cho Load Balancer của mình. Tất cả những gì bạn làm là bắt đầu thí nghiệm từ FIS console và quan sát cách dịch vụ của bạn phản hồi với một trường hợp lỗi cục bộ được kiểm soát.\nTrong ảnh chụp màn hình sau, bạn sẽ thấy quy tắc Listener ALB ban đầu chỉ được cấu hình với target group mặc định.\nHình 4 ALB listener rule trước khi thí nghiệm bắt đầu, hiển thị một target group duy nhất nhận 100% lưu lượng truy cập.\nSau khi thí nghiệm bắt đầu, AWS FIS sửa đổi rule để chia lưu lượng truy cập — như được hiển thị trong ảnh chụp màn hình Sau.\nHình 5 ALB listener rule sau khi thí nghiệm bắt đầu, hiển thị một target group mới với Lambda được cấu hình để nhận 50% lưu lượng truy cập và phản hồi bằng một mã lỗi được xác định trước.\nBước 4: Quan sát và phân tích kết quả Bạn có thể xác thực thí nghiệm bằng cách làm mới ALB DNS trong trình duyệt của bạn hoặc chạy một curl loop:\nwhile true; do curl -s http://\u0026lt;your-alb-dns-name\u0026gt;; sleep 1; done Hình 6 Đầu ra CLI động hiển thị các yêu cầu lặp lại đến URL ALB trong một vòng lặp để chứng minh cách giải pháp tiêm lỗi.\nBạn sẽ thấy đầu ra xen kẽ như:\nBackend service is healthy (dịch vụ backend đang hoạt động tốt) Service Unavailable – Simulated Error Response (Lambda) Bạn có thể giám sát Amazon CloudWatch Logs để xem các chỉ số thực thi hàm Lambda và hành vi của ứng dụng (logic thử lại, cơ chế chuyển đổi dự phòng)\nLưu ý: Sau khi bắt đầu thí nghiệm, có thể mất đến một phút trước khi target group mới được gắn vào ALB và lưu lượng truy cập bắt đầu định tuyến đến hàm Lambda. Trong khoảng thời gian ngắn này, tất cả các yêu cầu có thể tiếp tục đến dịch vụ backend ban đầu.\nCơ chế khôi phục Thí nghiệm nhằm mục đích giúp ích cho các hoạt động khôi phục, mặc dù nên kiểm tra trong môi trường của bạn:\nALB rule sẽ tự động được khôi phục vào cuối thời gian kiểm tra. Target group tạm thời được gỡ bỏ và xóa để ngăn chặn bất kỳ cấu hình còn sót lại nào. Nếu thí nghiệm bị hủy, quy trình khôi phục sẽ đưa hệ thống trở lại trạng thái ban đầu. Các điểm cần cân nhắc Bài đăng này cung cấp thông tin kỹ thuật và cấu hình ví dụ. Việc triển khai trong môi trường của bạn có thể yêu cầu thêm các cân nhắc về bảo mật, tuân thủ và kỹ thuật. Luôn kiểm tra kỹ lưỡng trước trong các môi trường phi sản xuất.\nDọn dẹp Để tránh phát sinh chi phí trong tương lai, hãy xóa các tài nguyên đã triển khai:\naws cloudformation delete-stack --stack-name alb-fis-experiment Kết luận Trong bài đăng này, chúng tôi đã chứng minh cách mở rộng khả năng của AWS FIS bằng cách mô phỏng lỗi cục bộ cho các workload đằng sau ALB bằng cách sử dụng Lambda. Giải pháp này cho phép các nhóm kiểm tra khả năng phục hồi của ứng dụng đối với các lỗi không liên tục mà không gây ra sự cố ngừng hoạt động hoàn toàn. Bằng cách tận dụng AWS FIS, Lambda, và các ALB routing rules, bạn có thể tạo ra các kịch bản lỗi có kiểm soát và tăng cường độ vững chắc của hệ thống.\nĐể tìm hiểu thêm, hãy khám phá các tài nguyên sau:\nTài liệu AWS Fault Injection Service Tài liệu Amazon ALB Sử dụng SSM Systems Manager document với AWS FIS Bắt đầu với CloudFormation template và chia sẻ kinh nghiệm của bạn trong phần bình luận bên dưới.\nTAGS: aws fault injection simulator, chaos engineering\nOzgur Canibeyaz Ozgur là Senior Technical Account Manager tại Amazon Web Services với 8 năm kinh nghiệm. Ozgur giúp khách hàng tối ưu hóa việc sử dụng AWS của họ bằng cách xử lý các thách thức kỹ thuật, khám phá các cơ hội tiết kiệm chi phí, đạt được sự xuất sắc trong vận hành và xây dựng các dịch vụ sáng tạo bằng các sản phẩm AWS.\nPablo Colazurdo Pablo là Principal Solutions Architect tại AWS, nơi anh ấy thích giúp khách hàng ra mắt các dự án thành công trên Cloud. Anh ấy có nhiều năm kinh nghiệm làm việc với nhiều công nghệ đa dạng và đam mê học hỏi những điều mới. Pablo lớn lên ở Argentina nhưng hiện đang tận hưởng cơn mưa ở Ireland trong khi nghe nhạc, đọc sách hoặc chơi D\u0026amp;D với các con.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://veljg.github.io/AWS-Worklog/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://veljg.github.io/AWS-Worklog/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create a gateway endpoint","tags":[],"description":"","content":" Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://veljg.github.io/AWS-Worklog/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “AI-Driven Development Life Cycle: Reimagining Software Engineering” Event Objectives Explore the transformative shift in software development driven by generative AI. Introduce the AI-Driven Development Life Cycle (AI-DLC) and its core concepts. Kiro and Amazon Q Developer demonstration Speakers Toan Huynh – Specialist SA, PACE My Nguyen - Sr. Prototyping Architect, Amazon Web Services - ASEAN Key Highlights Focused on the concept of AI-DLC, a framework where AI orchestrates the development process, including planning, task decomposition, and architectural suggestions, while developers retain ultimate responsibility for validation, decision-making, and oversight - AI-DLC Core Concept: The approach is Human-Centric, with AI acting as a Collaborator to enhance developer capabilities, leading to Accelerated Delivery (cycles measured in hours/days instead of weeks/months) .\n- AI-DLC Workflow: It\u0026rsquo;s an iterative loop involving AI Tasks (Create plan, Implement Plan, Seek clarification) and Human Tasks (Provide clarification, Implement Plan), where the AI repeatedly asks clarifying questions and only implements solutions after human validation .\n- AI-DLC Stages: The lifecycle is broken down into Inception, Construction, and Operation. Each stage builds richer context for the next:\nInception: Includes building context, elaborating intent with User Stories, and planning with Units of Work.\nConstruction: Involves Domain Modeling, code generation and testing, adding architectural components, and deploying with IaC \u0026amp; tests.\nOperation: Focuses on deploying in production and managing incidents.\n- Challenges AI-DLC Aims to Solve:\nScaling AI development: AI coding tools can fail with complex projects.\nLimited control: Existing tools make it difficult to collaborate with and manage AI agents.\nCode quality: Maintaining quality control when moving from proof-of-concept to production becomes difficult.\nDeep Dive: Kiro - The AI IDE for Prototype to Production Kiro, an AI-first Integrated Development Environment (IDE) that supports the AI-DLC, focusing on Spec-driven development - Spec-driven Development: Kiro turns a high-level prompt (e.g., \u0026ldquo;I want to create a chat application like Slack\u0026rdquo;) into clear requirements (requirements.md), system design (design.md), and discrete tasks (tasks.md), fundamentally shifting development from \u0026ldquo;vibe coding\u0026rdquo; to a structured, traceable process. Developers collaborate with Kiro on these specs, which serve as the source of truth.\n- Agentic Workflows: Kiro\u0026rsquo;s AI agents implement the spec while keeping the human developer in control, with the key features being:\n+ Implementation Plan: Kiro generates a detailed Implementation Plan with start tasks, sub-tasks (e.g., \u0026ldquo;Implement user registration and login endpoints,\u0026rdquo; \u0026ldquo;Implement JWT middleware\u0026rdquo;), and links them back to specific requirements for validation .\n+ Agent Hooks: These delegate tasks to AI agents that trigger on events such as \u0026ldquo;file save.\u0026rdquo; They autonomously execute in the background based on pre-defined prompts, helping to scale work by generating documentation, unit tests, or optimizing code performance.\nKey Takeaways - AI Ensures Production Readiness: Kiro creating detailed design documents (like data flow diagrams and API contracts), and generating unit tests before the code is written, ensures that AI-generated code is production-ready and maintainable, not just a quick prototype.\n- Human Control via Artifacts: Developers maintain control not by writing the bulk of the code, but by validating and refining the artifacts—the requirements, the design, and the task plan—before the AI agents execute the implementation.\nApplying to Work - Integrate Amazon Q Developer/Similar Tools: Integrating AI coding assistants into my academic projects to automate boilerplate code and common tasks to boost productivity.\n- Focus on High-Value Tasks: By letting AI automate undifferentiated heavy lifting, I can focus my time on mastering higher-value, creative tasks like Domain Modeling and Architectural Design, which are crucial human-centric activities in the Construction phase.\nEvent Experience Attending the AI-Driven Development Life Cycle: Reimagining Software Engineering event provided a fascinating glimpse into the future of software development. It was clear that Generative AI isn\u0026rsquo;t just a coding assistant; it\u0026rsquo;s poised to become a core orchestrator of the entire development process. The session was well-structured, moving from the overarching concept of AI-DLC to specific demonstrations of Amazon Q Developer and Kiro. The demo of Kiro was particularly impactful, showing how a single text prompt can be transformed into a full, executable, and traceable development plan inside the IDE.\nLessons learned The three main challenges with current AI development (scaling, limited control, and code quality) made the structured, human-validated approach of AI-DLC seem highly necessary and well-thought-out. Some event photos "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Prepare the environment","tags":[],"description":"","content":"To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://veljg.github.io/AWS-Worklog/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Proposal: Automated AWS Incident Response and Forensics Workshop 1. Executive Summary The Automated Incident Response and Forensics Workshop is designed to provide security and operations teams with hands-on experience building an AWS-native, cost-optimized security automation pipeline. The architecture leverages Amazon GuardDuty\u0026rsquo;s machine learning for detection, instantly contains threats via AWS Lambda orchestrated by Amazon EventBridge, and utilizes AWS Glue and Athena for cost-efficient forensic analysis. The system is configured for minimal operational cost, making it ideal for an enterprise pilot.\n2. Problem Statement What’s the Problem? Traditional security operations rely on manual log review and human intervention, leading to high Mean Time To Respond (MTTR) and potentially devastating breaches. Manual incident investigation is slow and expensive due to unoptimized querying of massive log files.\nThe Solution The proposed solution implements a complete Detection-to-Containment-to-Forensics lifecycle using serverless AWS services:\nDetection: Managed threat intelligence via GuardDuty eliminates the need for complex custom rule writing. Orchestration: Amazon EventBridge ensures the GuardDuty finding instantly triggers multiple response actions. Containment: Lambda is triggered by EventBridge to instantly quarantine compromised EC2 instances and disable suspicious IAM keys. Forensics: S3, Glue, and Athena form a highly cost-optimized data lake, allowing analysts to run fast, targeted SQL queries on historical logs (VPC Flow Logs, CloudTrail) to generate comprehensive incident reports. Benefits and Return on Investment Reduced MTTR: Containment time drops from minutes/hours (manual) to seconds (automated Lambda). Cost Efficiency: AWS Glue optimizes S3 logs into partitioned, columnar data (Parquet), drastically cutting the data scanned and thus minimizing Athena query costs. Security Insight: Provides an immutable, centralized data foundation for all security investigations. Monthly Costs: Estimated minimal recurring cost of ~$8.21 USD, demonstrating responsible cloud financial management. 3. Solution Architecture The architecture implements a serverless, event-driven pipeline that covers the entire IR lifecycle.\nAWS Incident Response and Forensics Architecture\nAWS Services Used Service Purpose in IR Lifecycle AWS GuardDuty Detection: Analyzes CloudTrail \u0026amp; VPC Flow Logs for threats, generates high-fidelity findings. Amazon EventBridge Event Routing: Receives GuardDuty findings and routes them to multiple response targets (Lambda and SNS). Main Lambda (IR Orchestrator) Containment: Executes immediate response actions (Quarantine EC2, Disable IAM Key). AWS SSM (Forensic) Forensics: Provides secure remote execution to collect evidence inside the EC2 OS. Amazon SNS Alerting: Publishes GuardDuty findings for multi-channel delivery. Notification Lambda (Alert Dispatch) Custom Channel: Translates SNS alerts into messages for a 3rd party platform (e.g., Telegram). EC2 Instance Target: The asset being monitored and quarantined. Amazon S3 Storage: Immutable Data Lake for all historical log data. AWS Glue (Crawler) Optimization: Catalogs log data, enabling efficient, low-cost querying. Athena Analysis: Serverless SQL engine for querying S3 logs during forensics. 4. Technical Implementation Implementation Phases (6 Weeks) The workshop focuses on deploying the automation and optimization layers over six weeks:\nWeek 1-2 (Foundation): Enable core logging (CloudTrail, VPC Flow Logs, EC2 logs to CloudWatch). Activate GuardDuty and monitor initial findings. Week 3-4 (Automation): Develop and deploy the two Lambda functions (Main Logic and Notification Logic). Configure EventBridge rules to connect GuardDuty to the Lambdas and SNS. Finalize AWS SSM runbooks for forensic data collection. Week 5-6 (Optimization \u0026amp; Launch): Configure AWS Glue Crawler to run weekly on S3 log data. Test complex forensic queries using Athena to validate cost-efficiency. Conduct a full simulation of an intrusion and automated response. Technical Requirements Detection: GuardDuty configuration for high/medium severity findings. Response Logic: Python/Node.js SDK knowledge within Lambda to call the EC2 and IAM APIs. Configuration of EventBridge rules for event routing. Forensics: Proficiency in setting up S3 and Glue for a data lake (including partitioning) and using standard SQL queries in Athena. Best Practice: EC2 instance runs only as needed (e.g., 168 hours/month) to minimize operational costs. 5. Timeline \u0026amp; Milestones Timeline Key Milestones and Achievements Week 1-2: Logging \u0026amp; Detection All foundational logging (CloudTrail, VPC Flow Logs) enabled.Amazon GuardDuty fully enabled and generating sample findings.Initial S3 log archiving policies created. Week 3-4: Automation \u0026amp; Alerting Main Lambda deployed and tested (Quarantine EC2 action validated).Notification Lambda deployed and integrated with Telegram.AWS SSM configured for secure remote forensic data collection. Week 5-6: Forensics \u0026amp; Optimization AWS Glue Crawler configured and running weekly to catalog S3 logs.Athena queries validated against optimized log data for cost efficiency.Full simulation of intrusion → detection → containment → forensics completed. 6. Budget Estimation The budget assumes a lab environment operating under a paid tier model, demonstrating cost control.\nThe budget estimate was calculated on AWS Pricing Calculator.\nInfrastructure Costs Assumption Cost/Month (USD) EC2 Instance (t3.micro) 168 hours/month (7 days runtime) $2.88 Amazon GuardDuty Continuous monitoring, minimal log volume ~$2.08 AWS Glue (Crawler) 4 weekly runs @ 15 min each $0.44 Amazon CloudWatch 1000 GetMetric calls $2.34 Amazon S3 5gb free + 3gb extra $0.21 AWS Lambda 1 million free requests + compute charges $0.02 Amazon Athena 50 queries per month and 1gb of data scanned per query $0.24 TOTAL ESTIMATED MONTHLY COST ~$8.21 USD TOTAL ESTIMATED ANNUAL COST ~$98.52 USD 7. Risk Assessment Risk Impact Probability Mitigation Strategies GuardDuty Cost Spike (Due to high event volume) Medium Medium Implement strict budget alarms; utilize the 30-day Free Trial to establish a cost baseline. Logic Failure (Lambda fails to quarantine) High Low Robust error handling and logging in Lambda; Main Lambda will log failure to a Dead-Letter Queue (DLQ) for investigation and re-execution. Immediate manual intervention via the EC2 console. Unoptimized Athena Query Medium Medium Strict requirement to use AWS Glue partitioning and Parquet conversion to minimize data scanned. 8. Expected Outcomes Technical Improvements: Achieve an Automated MTTR (Mean Time To Respond) for EC2 quarantine measured in seconds. Establish an AWS-native, cost-optimized Security Data Lake. Gain hands-on expertise with key security and analysis tools: GuardDuty, Lambda, SSM, Glue, and Athena. Long-term Value: Provides a reusable, security-hardened reference architecture for future production cloud environments. Generates a foundational dataset of security logs suitable for AI/ML security research (anomaly detection model training). "},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Complete Module 3 \u0026amp; 4 Help team members get up to speed Discuss workshop ideas Do first optional research: AWS Well Architected Framework Check out AWS Advanced Networking - Specialty Study Guide Check out AWS Microsoft Workload Check out AWS Skill Builder Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Started on module 4 - Learnt about AWS Storage services - Learnt about S3 Bucket Accesspoint and Storage class. 15/09/2025 15/09/2025 3 - Lab 4 - Lab 6: RDS Database -Succesfully used Linux via EC2 instance to: + Install and use MySQL database + Install and run a web apllication, can be connected to from browers - Created Load Balancer and Target Groups -Paessler Webstress tool has been discontinued, cannot test using the given tool - Successfully install Siege on EC2 instance to load test: + Ran loadtest, simulated 50 users at the same time for 10 minutes + The EC2 instance is terminated and load balanced 10 times in succession + Seige automatically stopped after 5 minutes due to too much packet loss + The reason might be due to the EC2 instance and RDS Database were created using the only available free tier options, and could not handle the increased traffics. - Succesfully hosted database using RDS 16/09/2025 16/09/2025 Lab 6 4 Module 4 3, module 4 4 - Help teammate with Lab 5 - Instruction of lab 5 is missing some steps: 5.5.3: The given script didnt connect the RDS database to MySQL, 5.5.5: The instruction is missing the step: cd to the application folder 17/09/2025 17/09/2025 Lab 5 5 - Joined the AWS Cloud Day 2025 HCM Event: Gen AI and Data track 18/09/2025 18/09/2025 Vietnam Cloud Day Agenda 6 - Retry Lab 10: + Fix given template: Region changed to ap-southeast-1, instance changed to t3.micro + Successfully configured endpoints and rules in Route 53 for hybrid DNS + Succesfully deployed Microsoft AD\n- Lab 8:\n+ Viewed metrics and graph using Cloudwatch on selected EC2 Instances + Learnt the basics on monitoring logs + 8.4.2: cannot be done: Cant find the resource s3://workshop-template-bucket/logger.py . + Configured Cloudwatch Alarm and Dashboard - Lab 14: Installing Ubuntu - Reformatted worklog - Wrote about Cloud Day 2025 experiences - Additional research on AWS Well Architected Framework: + Documents a set of foundational questions that enable you to understand how a specific architecture aligns with cloud best practices + The pillars: • Operational Excellence: Focuses on running and monitoring systems to deliver business value, and continually improving supporting processes and procedures. • Security: Focuses on protecting information, systems, and assets, while delivering business value through risk assessments and mitigation strategies. • Reliability: Focuses on the ability of a workload to perform its intended function correctly and consistently when it\u0026rsquo;s expected to. • Performance Efficiency: Focuses on using computing resources efficiently to meet system requirements, and maintaining that efficiency as demand changes. • Cost Optimization: Focuses on avoiding unnecessary costs by managing and controlling where money is spent in the cloud. • ustainability: Focuses on minimizing the environmental impacts of running cloud workloads. + Purpose: • A cloud service for reviewing and measuring your workloads against AWS best practices to build more secure, resilient, high-performing, and cost-effective systems.\n• Core Function: Identifies High Risk Issues (HRIs) and Medium Risk Issues (MRIs) in your architecture and provides an improvement plan to mitigate them. + Usage: • Step 1: Define a Workload: Specify the name, environment, owner, and regions for the application or system you are reviewing.\n• Step 2: Document the State: Answer questions based on the pillars of the AWS Well-Architected Framework (Security, Reliability, etc.) and save a \u0026ldquo;milestone\u0026rdquo; to capture your progress.\n• Step 3: Review the Improvement Plan: The tool generates a prioritized list of risks (HRIs and MRIs) based on your answers.\n• Step 4: Make Improvements \u0026amp; Measure: Update your architecture based on the plan, then update your answers in the tool to track the reduction in risks over time + Key Features: • Workloads: The central component representing your application; can be viewed, edited, shared, and deleted. • Lenses: Provide focused questions for specific technologies (e.g., Serverless Lens) or industries. You can also create Custom Lenses for internal standards. • Review Templates \u0026amp; Profiles: Help standardize reviews by pre-filling common answers (Templates) and prioritizing questions based on business goals (Profiles). • Jira Integration: Allows you to sync improvement items directly from the Well-Architected Tool into your Jira projects as epics, tasks, and sub-tasks for streamlined tracking. + Security: • Shared Responsibility Model: AWS secures the cloud infrastructure, while you are responsible for securing your workloads in the cloud. • IAM Integration: Access is controlled through AWS IAM, with pre-built policies for full access and read-only access. • Data Protection: Recommends using IAM users (not root), enabling MFA, and avoiding placing sensitive data in free-form text fields. • Monitoring \u0026amp; Auditing: Integrates with AWS CloudTrail to log all API activity and with Amazon EventBridge to trigger automated notifications. 19/09/2025 20/09/2025 Lab 10 Lab 8 Lab 14 Vietnam Cloud Day Experience AWS Well Architected Framework Week 2 Achievements: Successfully completed core labs in Module 3 (focused on RDS, Load Balancing) and made significant progress in Module 4.\nUtilized Linux via EC2 to install and run a MySQL database and deploy a connected web application, successfully hosting the database using RDS (Relational Database Service).\nSuccessfully set up a Load Balancer and Target Groups, and adapted to a discontinued tool by installing and using Siege on an EC2 instance to execute a load test.\nConfigured Route 53 endpoints and rules for a hybrid DNS setup, including the deployment of Microsoft AD.\nGained foundational knowledge in cloud monitoring by viewing metrics and graphs in CloudWatch, configuring Alarms and Dashboards, and learning the basics of log management.\nTeam Support: Assisted team members with labs, identifying and helping to correct missing steps in the lab instructions.\nAttended the AWS Cloud Day 2025 HCM Event (Gen AI and Data track).\nOptional Research: Completed research on the AWS Well-Architected Framework, documenting its purpose, usage steps, key features (Lenses, Templates, Profiles), and security considerations.\nTechnical Setup: Successfully installed Ubuntu on a machine and reformatted the worklog for improved presentation.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create an S3 Interface endpoint","tags":[],"description":"","content":"In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Test the Gateway Endpoint","tags":[],"description":"","content":"Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Complete Module 5 Help teammates with previous labs Redo the labs that are unvailable with free tiers Do 2 additional research Discuss project idea Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Lab 25: Cannot be done for now, account is on free tier - Upgraded account to paid tier - Retry lab 25: + The given template used runtime nodejs12.x for Lambda functions, which is no longer supported, fixed it by changing it to nodejs20.x + Created Fsx file system + The S3 bucket endpoint for testing data is only reachable in the US region, so it must be changed to Read-S3Object -BucketName nasanex -KeyPrefix /AVHRR -Folder Z:/nasanex/AVHRR -Region us-west-2 + Created file shares + Created HDD and SSD Fsx + 25.4: The given tool version is outdated, downloaded latest version + Successfully tested drive performance with various parameters + Monitored performance using CloudWatch: Alarm got triggered, throughput was maxed at 400mb + Learnt how to deduplicate file:\n• Default dedup schedule is every Saturday • Initial dedup run optimized nothing due to the default fileAge -\u0026gt; changed to 0 =\u0026gt; Optimized half of the files + Created shadow copies for backup + Learnt how to manage open files and how to close them from the connection + Successfully created user quotas to manage storage space + Enabled Continuously Available (CA) file share on Amazon FSx to be used by mutiple user at the same time + Scaled throughput and storage on AWS Console - Learnt shared responsibility model: Both the provider and the customer have responsibility in security - Module 5-2: Best pratice is to create an admin IAM user rather than using root account - IAM Principal: Access resources in AWS Account - IAM Policy: Identity based and Resource based - IAM Role: A set of rules that control access to resources and services for IAM User - IAM Role can be used to enable cross account - School subject: + ENW493c: Completed Understanding Research Methods 22/09/2025 22/09/2025 Lab25 Understanding Research Methods 3 - Module 5: - Amazon Cognito: A authentication, permission and user management service, with two main feature: + User pool: A collection of user accounts and authentication informations, allowing for 3rd party authentication services + Identity pools: A mapping of permissions and credentials that can be applied to users - AWS Organization: Manage many AWS Accounts and resources + Organizes accounts by OU and use Service Control Policies to define permissions for user on the organization AWS Identity Center(SSO): Manage AWS Authorizations and Applications: + Utilize permission sets - AWS KMS: Create and manage encryption keys: + CMK (Customer Managed Key) is the main resource, used to create, encrypt and decrypt Data Key - AWS Security Hub: Scan and test security based and policies and best practices - Continue with lab 14: + Created role and S3 Bucket + The latest Ubuntu version (25.04) included unsupported kernel version, required reinstallation to proceed + Installing Ubutun 24.04: Failed, its kernel is still unsupported + Installing Ubuntu 22.04 + Successfully imported VM to AWS + Successfully connected to EC2 instance created from the AMI using VM\u0026rsquo;s username and password 23/09/2025 23/09/2025 Lab 14 4 Continue with lab 14: + Created export bucket, configured permission + Successfully exported instances into .OVA format for usage - Lab 18: Enabled Security Hub and configured AWS Config to record data for analyzing (It can take a long for a score to be calculated) 24/09/2025 24/09/2025 Lab 14 5 - Lab 22: + Created Lambda functions for running and stopping EC2 instance based on schedules and tags + Logged notification via Slack - Lab 28:\n+ Created IAM policies and role, only allowing access from Singapore Region(ap-southeast-1) + Restricted access to EC2 from regions outside of policy + Restricted creation of EC2 instances without valid tags Lab 30: Restricted IAM user to only use the specified region to access EC2 - Lab 18 (Update): Security finished scanning, got a security score of 85%, 1 critical exposure: IAM User have administrative access policy - Lab 33: + Created Key Management Service + Setted up Cloudtrail to log data in S3 Bucket + Created Athena to query logs + KMS successfully denied access to users without authorization 25/09/2025 25/09/2025 Lab 22 Lab 28 Lab 30 Lab 18 Lab 33 6 - Lab 44: Configured role conditions, restricting access by IP, Time and others - Lab 48:\n+ Used IAM access key to upload file to S3 via EC2 Instance + Uploaded file to S3 via EC2 Instance without access key by using IAM Roles - Lab 12: + Created AWS Organization + Created accounts and move them into units + Invited accounts to organization + Switched roles for accounts under the organization + Setted up policies for the accounts under the organization + Installed Python to continue with the lab + Created and configured users and groups using Identity Store APIs via AWS CLI Labs from AWS for Microsoft Workloads: + Managed user and group on Microsoft AD via AWS CLI + Learnt how to troubleshoot EC2 instances by detaching the volumes of the errorus instance and attach it to another running instance to configure and fix the problems + Learnt how to attach licenses to EC2 instances with Microsoft AD, demo with Libre Office 26/09/2025 26/09/2025 Lab 44 Lab 48 Lab 12 Microsoft Workloads Week 3 Achievements: Successfully upgraded the AWS account to complete labs previously unavailable on the Free Tier and learned to adapt resources, such as fixing outdated Lambda runtime versions. Advanced Storage (FSx): Completed Lab 25, creating and configuring an Amazon FSx file system. Gained practical skills in managing data deduplication, creating shadow copies for backup, setting user quotas, and scaling throughput while monitoring performance with CloudWatch.\nCompleted Module 5 theory and extensive security labs:\nLearnt concepts of IAM, Roles, Policies, Cognito, Organizations, Identity Center (SSO), and KMS.\nImplemented Region Restriction policies (Lab 28 \u0026amp; 30) for EC2 creation and access.\nConfigured Role Conditions to restrict access based on IP and time (Lab 44).\nPracticed securing file uploads to S3 using IAM Roles instead of access keys (Lab 48).\nEnabled Security Hub and AWS Config (Lab 18), achieving a security score of 85% and identifying a critical exposure (IAM User administrative access).\nCreated Lambda functions to schedule the start and stop of EC2 instances based on tags (Lab 22) and logged notifications via Slack.\nSuccessfully navigated kernel compatibility issues by selecting the correct Ubuntu version (22.04), imported a VM to AWS, created an AMI, and exported the instance back into the .OVA format (Lab 14).\nSet up an AWS Organization, created Organizational Units (OUs) and accounts, configured Service Control Policies, and practiced switching roles for accounts (Lab 12).\nAWS for Microsoft Workloads: Completed additional labs focused on:\nManaging users/groups in Microsoft AD via AWS CLI.\nAdvanced EC2 troubleshooting by detaching and re-attaching volumes.\nAttaching licenses to EC2 instances using Microsoft AD (demonstrated with Libre Office).\nSet up CloudTrail to log data to S3 and used Amazon Athena to query these logs for audit and security analysis (Lab 33).\n"},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.3-s3-vpc/","title":"Access S3 from VPC","tags":[],"description":"","content":"Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test the Interface Endpoint","tags":[],"description":"","content":"Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://veljg.github.io/AWS-Worklog/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Achieve Excellence in Aftermarket Service with Syncron and AWS This blog explains how manufacturers can enhance their aftermarket services by implementing the Syncron Service Lifecycle Management (SLM) platform on AWS. You will learn why traditional, isolated operations often lead to inefficiencies and increased costs , and how Syncron’s SLM platform creates a connected business ecosystem by unifying data from parts, service, and warranty management. The article also explores the solution\u0026rsquo;s architecture, which leverages services like Amazon S3 and AWS Glue, and walks through customer use cases such as gaining instant access to data and building custom AI/ML models for price optimization.\nBlog 2 - Amazon Q Developer CLI supports image inputs in your terminal This blog introduces the powerful new capability of the Amazon Q Developer CLI to accept and analyze image inputs directly in the terminal. You will learn how this feature bridges the gap between visual design assets and functional code, streamlining development by reducing the manual, error-prone work of translating diagrams into implementation. The article also provides a hands-on guide with several practical use cases, demonstrating how to generate Terraform code from an architecture diagram, create a SQL schema from an ER diagram, transform a hand-drawn sketch into a formal design document, and build UI code from a simple screenshot.\nBlog 3 - Simulating partial failures with AWS Fault Injection Service This blog details an advanced chaos engineering technique for simulating partial, or localized, system failures using AWS Fault Injection Service (FIS). You will learn why testing for these non-total failures is critical for building truly resilient applications and how traditional fault injection methods often overlook this important scenario. The article also provides a complete, step-by-step walkthrough of the solution, guiding you on how to combine FIS with an Application Load Balancer (ALB) and an AWS Lambda function to inject controlled faults that impact only a percentage of traffic, all without requiring any changes to your application\u0026rsquo;s code.\nBlog 4 - \u0026hellip; Blog 5 - \u0026hellip; Blog 6 - \u0026hellip; "},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Complete Module 6 Started on proposal Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Module 6: Database Concept review:\n+ Database + Session + Primary/Foreign Key + Index + Partitions + Execution/Query Plan + Log \u0026amp; Buffer + RDBMS (Relational Database Manangement System) + NOSQL + OLTP(Online Transaction Processing): For payments, transactions + OLAP (Online Analytical Processing): Analyze data, predict trends and patterns - AWS RDS (Relational Database Serive): Include Aurora, MySQL, Postgres SQL , MSSQL, Oracle , Maria + Automatic backup + Generate read replica +Read replica can be turned into primary code + Auto Fail Over/Multi AZ (Backups on mutiple AZs) + Commonly used for OLTP + Encrypt data while at rest/in transit + Protected my Security Group and NACL + Can change instance size + Storage Auto SCaling - Amazon Aurora: Optimized underlying storage infrastructure, uses MySQL and PostgreSQL + Back track: revert to previous state + Clone + Global Database (Multi Region) + Multi Master: Many Master Databases - Amazon Redshift: Data warehouse service: PostgreSQL core, optimized for OLAP + Uses MMP Database: data is partitioned and saved at computer nodes, a Leader node is used to coordinate and compile queries + Stores data in a columnar storage format, useful for OLAP applications + Uses SQL and drivers like JDBC and ODBC + Provide cost effective services (Transient Cluster/ Redshift spectrum) - Amazon ElastiCache: Creates Cluster Caching Engines (Redis/Memcached) + Detects and replaces failed nodes + Put before CSDL layer in order to cache data + Recommended to use Redis for new workloads + Using ElastiCache requires caching logic on applications, not recommended to use default system caching - Formulated a proposal for workshop with teammates - School subject: + KS57: Completed Quản trị dữ liệu và an toàn thông tin 29/09/2025 29/09/2025 Quản trị dữ liệu và an toàn thông tin 3 - Lab 43: Guide is broken, the link doesnt go anywhere, going by video + Downloaded Schema Conversion Tool + Downloaded MSSQL in EC2 Instance + No SQL script was given, trying with custom basic MSSQL Database + No CloudFormation Stack was given, skipping Oracle Database connection + Installed MySQL on EC2 Instance + Migrated custom MSSQL Database to MySQL Database using AWS Schema Conversion Tool + Created custom RDS to test migration task + Attempted to migrate from local machine to RDS + Tried to use AWS Replication Agent: Unsuccessful due to it being made for Window/Linux server only, not OS + Tried to portforward PC to be used as an endpoint + Failed portforwarding, not allowed by ISP 30/09/2025 30/09/2025 Lab 43 Application Mirgation Service Guide 4 - Found out AWS account\u0026rsquo;s credits are all expired from doing lab 12 - Wrote a support case - Stopping labs for now - Focus on researching about team\u0026rsquo;s proposal 01/10/2025 01/10/2025 - School subject: + ENW439c: Completed Research Methodologies Research Methodologies 5 - Continued doing labs by aquiring help from team member: Created an IAM User with admin privilege for me to log in and use their account - Translate first blog 02/10/2025 02/10/2025 Blog 1 6 - Joined the AI-Driven Development Life Cycle: Reimagining Software Engineering event - Translated second and third blog 03/10/2025 04/10/2025 Blog 2 Blog 3 Week 4 Achievements: Completed a comprehensive review of core database concepts including RDBMS, keys, indexes, partitioning, OLTP/OLAP, and AWS-specific database services.\nGained theoretical knowledge of the features and use cases for AWS RDS, Amazon Aurora (e.g., Backtrack, Global Database), Amazon Redshift (Data Warehouse for OLAP), and Amazon ElastiCache (caching with Redis/Memcached).\nDatabase Migration: Attempted a complex database migration lab, demonstrating resourcefulness by:\nSourcing a custom MSSQL Database and installing necessary services on an EC2 instance due to broken lab guides.\nSuccessfully migrating the custom MSSQL database to MySQL using the AWS Schema Conversion Tool (SCT).\nIdentified and addressed the issue of expired AWS credits by raising a support case.\nSecured continuation of lab work by setting up an IAM User with admin privileges on a team member\u0026rsquo;s account.\nFormulated a proposal for the team\u0026rsquo;s upcoming workshop with teammates.\nCompleted the translation of three blogs.\nAttended the AI-Driven Development Life Cycle: Reimagining Software Engineering event.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.4-s3-onprem/","title":"Access S3 from on-premises","tags":[],"description":"","content":"Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://veljg.github.io/AWS-Worklog/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders: Gen AI and Data track\nDate \u0026amp; Time: 08:30, September 18th, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 09:00, October 3rd, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"On-premises DNS Simulation","tags":[],"description":"","content":"AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Continue building and planning proposal Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Family matters 06/10/2025 06/10/2025 3 - Family matters 07/10/2025 07/10/2025 4 - Learnt how to create basic AWS Architecture Diagrams - Created team\u0026rsquo;s workshop architecture diagram 08/10/2025 08/10/2025 5 - Lab 35: + Succesfully setted up data stream using Kinesis + Successfully sent sample data to the S3 using Kinesis Data Generator with Amazon Cognito + Learnt how to use AWS Glue Crawler to map data to S3 Bucket + Used Athena to query data + USed AWS Glue Notebook to build dataset based on sample data + Used Athena to analyze data and visualized with QuickSight - Updated the architecure diagram based on changes in the workshop proposal - Started researching Guard Duty to use as a component of the workshop 09/10/2025 09/10/2025 Lab 35 6 - Lab 40: + Praticed more with AWS Glue and Athena, used it to analyze AWS Monthy Cost data - School subject: + KS57: Completed Giáo dục và Phát triển nguồn nhân lực số 10/10/2025 10/10/2025 Lab 40 Giáo dục và Phát triển nguồn nhân lực số Week 5 Achievements: Proposal Development: Successfully created and updated the team\u0026rsquo;s workshop architecture diagram, learning the best practices for diagramming AWS architecture.\nData Streaming and Analytics: Completed a complex lab focused on data pipelines:\nSuccessfully set up a real-time data stream using Amazon Kinesis.\nUsed Kinesis Data Generator with Amazon Cognito to send sample data to S3.\nLearned to use AWS Glue Crawler to map data and AWS Glue Notebook to build datasets.\nUsed Amazon Athena for querying data and Amazon QuickSight for data visualization.\nPracticed advanced analytics skills by using AWS Glue and Athena to analyze AWS Monthly Cost data.\nWorkshop Research: Initiated research on Amazon GuardDuty as a component for the team\u0026rsquo;s workshop proposal.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.5-policy/","title":"VPC Endpoint Policies","tags":[],"description":"","content":"When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Complete and submit proposal Assign tasks for team member to get started on the workshop Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Reformmatted and refined the worklog, adding information and summaries - Succesfully deployed worklog to Github Page 13/10/2025 13/10/2025 3 - Team meeting - Revised workshop proposal: Focused on using Guard Duty for intrusion dectection instead of a custom Lambda function due to the need for a large dataset and extensive development time. - Redrew AWS Architecture: Added Guard Duty replacing CloudWatch Alarm - Wrote a draft of the proposal with outlining basic function and providing a rough cost estimate. 14/10/2025 14/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Team meeting - Revised workshop proposal: + Incorporated the use of EventBridge + Recalculated costs by reducing the EC2 instance type and active hours - Updated AWS Architecture: Include the EventBridge icon and connections 15/10/2025 15/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Updated AWS Arhitecture: + Rearranged icons for clearer connections. + Moved SSM inside of region group + Added public subnet group for EC2 Instance - Installed AmazonQ for enhanced proposal analytics - Revised workshop proposal: Recalculated cost using AWS Pricing Calculator - Translated proposal draft into markdown code and successfully deployed it to Github Pages - Joined the online seminar 𝗗𝗫\u0026lt;𝗶𝗻𝗔𝗰𝘁𝗶𝗼𝗻\u0026gt; 𝗧𝗮𝗹𝗸#𝟳: Reinventing DevSecOps with AWS Generative AI 16/10/2025 16/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Compiled study materials for midterm exam - School subject: + ENW493c: Completed Being a researcher (in Information Science and Technology) 17/10/2025 17/10/2025 Being a researcher (in Information Science and Technology) Week 6 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://veljg.github.io/AWS-Worklog/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://veljg.github.io/AWS-Worklog/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learnt the basics of Guard Duty with \u0026ldquo;Getting Hands on with Amazon GuardDuty - AWS Virtual Workshop\u0026rdquo; - Got familiarized with Guard Duty by using Amazon Q to generate a basic lab: + Created sample finding via setting + Learnt the finding interface + Test EC2 by port scanning scanme.nmap.org + Simulated DNS exfiltration on EC2 + GuardDuty did not alert findings from VPC Flow Logs as expected + Triggered GuardDuty findings through CloudTrail by accessing API ListPolicies with root credentials - Learnt more by doing Guard Duty workshops - Online team meeting: Assign members to research the services to be used in the workshop 20/10/2025 20/10/2025 Getting Hands on with Amazon GuardDuty - AWS Virtual Workshop Guard Duty Workshop 3 - Succesfully triggered GuardDuty sample alerts with various severities and types via CloudShell CLI =\u0026gt; Easier testing environment - Created a custom threat list of IPs and domain names for GuardDuty via CloudShell commands although it did not work 21/10/2025 21/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Team meeting: + Quick AWS Services knowledge revision + Conversed about changes in the proposal - Updated AWS Architecture: Added AWS Detective - Revised proposal: + Added the usage of AWS Detective + Added plan for CDK after finishing the workshop - Mentor reccommendations: + Visualize data but without using Quicksight, instead make a custom-coded dashboard (Researching) + Save GuardDuty findings in S3 bucket for analyzing (Researching) - Succesfully configured EventBridge to trigger upon specific GuardDuty findings and: + Sent SNS emails to all of team members + Triggered a simple Lambda script - Fomulated an idea to add to workshop: Make a simple data graphing page hosted in S3 and use API Gateway and Lambda to pull forensics data from Amazon Athena (Researching) 22/10/2025 22/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Tried out AWS Card Clash with team members: Surprisingly good for learning services and their functions, their placement in Architectures - Reviewed AWS Services Knowledge for Mid-Term: Using Google Gemini to generate quizzes based on the given requirements 23/10/2025 23/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Successfully configured GuardDuty threat list to trigger findings from EC2 Instance activities 08/15/2025 08/15/2025 - School subject: + KS57: Completed Pháp luật và đạo đức trong công nghệ số Pháp luật và đạo đức trong công nghệ số Week 7 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://veljg.github.io/AWS-Worklog/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"https://veljg.github.io/AWS-Worklog/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Review AWS knowledge. Complete FCJ Mid-Term exam. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Rewatched FCJ Bootcamp study videos - Completed the AWS Cloud Essentials Quiz - Deep dived in AWS Services previously learnt and compared similar services to eachother - Checked out some AWS Architected Labs to better understand each of the main pillars - Successfully export log streams to S3 - Succesfully created trail on CloudTrail to track S3 and Lambda activities - AWS Architecture: + Researched how to incoporate AWS Step Functions into workshop\u0026rsquo;s architecture, rather than using only one Lambda for all IR actions + Considered using AWS Kinesis Data Firehose for continous log stream to S3 27/10/2025 27/10/2025 AWS Cloud Essentials Quiz AWS Well Architected Lab 3 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://veljg.github.io/AWS-Worklog/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://veljg.github.io/AWS-Worklog/tags/","title":"Tags","tags":[],"description":"","content":""}]